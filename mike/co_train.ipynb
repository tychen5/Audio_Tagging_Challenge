{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mike/Desktop/venv/lib/python3.5/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical ,Sequence\n",
    "from keras import losses, models, optimizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import Activation, LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from random_eraser import get_random_eraser\n",
    "from keras.optimizers import Adam\n",
    "from os.path import join\n",
    "import resnet\n",
    "\n",
    "map_dict = pk.load(open('data/map.pkl' , 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "semi = pd.read_csv('data/cotrain/leo_mfcc6_resnet_cotrain_Y.csv')\n",
    "semi_name = semi['fname'].values\n",
    "\n",
    "# unverified df\n",
    "unverified_df = pd.read_csv('data/train_label.csv')\n",
    "# test df \n",
    "test_df = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "#  append two df\n",
    "unverified_df = unverified_df[unverified_df['fname'].isin(semi_name)]\n",
    "unverified_df = unverified_df.drop(columns=['manually_verified'])\n",
    "\n",
    "test_df = test_df[test_df['fname'].isin(semi_name)]\n",
    "\n",
    "unverified_idx = unverified_df.index.values\n",
    "test_idx = test_df.index.values\n",
    "\n",
    "df = pd.concat([unverified_df , test_df])\n",
    "df = df.drop(columns=['label'])\n",
    "df['label_verified'] = semi['label_verified'].values\n",
    "df['onehot'] = df['label_verified'].apply(lambda x: to_categorical(x,num_classes=41))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>ffbc5fb8.wav</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>ffd137a4.wav</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>fff6d073.wav</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname  label_verified\n",
       "3606  ffbc5fb8.wav               4\n",
       "3607  ffd137a4.wav              38\n",
       "3608  fff6d073.wav              21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label_verified</th>\n",
       "      <th>onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386</th>\n",
       "      <td>ffbc5fb8.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9389</th>\n",
       "      <td>ffd137a4.wav</td>\n",
       "      <td>38</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9398</th>\n",
       "      <td>fff6d073.wav</td>\n",
       "      <td>21</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname  label_verified  \\\n",
       "9386  ffbc5fb8.wav               4   \n",
       "9389  ffd137a4.wav              38   \n",
       "9398  fff6d073.wav              21   \n",
       "\n",
       "                                                 onehot  \n",
       "9386  [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "9389  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "9398  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3609, 40, 345, 1)\n",
      "(3609, 41)\n"
     ]
    }
   ],
   "source": [
    "X_unverified = np.load('data/mfcc/X_train.npy')[unverified_idx]\n",
    "X_test = np.load('data/X_test.npy')[test_idx]\n",
    "\n",
    "X_semi = np.append(X_unverified,X_test , axis=0)\n",
    "Y_semi = np.array(df['onehot'].tolist()).reshape(-1,41)\n",
    "\n",
    "\n",
    "print(X_semi.shape)\n",
    "print(Y_semi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # data generator ====================================================================================\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training Semi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6948, 40, 345, 1)\n",
      "(6948, 41)\n",
      "(371, 40, 345, 1)\n",
      "(371, 41)\n",
      "Train on 6948 samples, validate on 371 samples\n",
      "Epoch 1/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9925Epoch 00000: val_acc improved from -inf to 0.82210, saving model to model_full_resnet152_refine/semi_1_0.82210.h5\n",
      "6948/6948 [==============================] - 395s - loss: 0.2690 - acc: 0.9925 - val_loss: 1.3928 - val_acc: 0.8221\n",
      "Epoch 2/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9978Epoch 00001: val_acc improved from 0.82210 to 0.82480, saving model to model_full_resnet152_refine/semi_1_0.82480.h5\n",
      "6948/6948 [==============================] - 160s - loss: 0.2464 - acc: 0.9978 - val_loss: 1.4326 - val_acc: 0.8248\n",
      "Epoch 3/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9984Epoch 00002: val_acc improved from 0.82480 to 0.83019, saving model to model_full_resnet152_refine/semi_1_0.83019.h5\n",
      "6948/6948 [==============================] - 160s - loss: 0.2394 - acc: 0.9983 - val_loss: 1.3918 - val_acc: 0.8302\n",
      "Epoch 4/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9991Epoch 00003: val_acc did not improve\n",
      "6948/6948 [==============================] - 150s - loss: 0.2351 - acc: 0.9991 - val_loss: 1.3535 - val_acc: 0.8221\n",
      "Epoch 5/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9994Epoch 00004: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2306 - acc: 0.9993 - val_loss: 1.3783 - val_acc: 0.8167\n",
      "Epoch 6/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9994Epoch 00005: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2286 - acc: 0.9993 - val_loss: 1.3398 - val_acc: 0.8275\n",
      "Epoch 7/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9990Epoch 00006: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2248 - acc: 0.9990 - val_loss: 1.3636 - val_acc: 0.8221\n",
      "Epoch 8/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9997Epoch 00007: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2203 - acc: 0.9997 - val_loss: 1.3915 - val_acc: 0.8221\n",
      "Epoch 9/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 1.0000Epoch 00008: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2171 - acc: 0.9999 - val_loss: 1.4031 - val_acc: 0.8086\n",
      "Epoch 10/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9991Epoch 00009: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2165 - acc: 0.9991 - val_loss: 1.4348 - val_acc: 0.8059\n",
      "Epoch 11/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9996Epoch 00010: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2131 - acc: 0.9993 - val_loss: 1.3963 - val_acc: 0.8140\n",
      "Epoch 12/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9993Epoch 00011: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2136 - acc: 0.9993 - val_loss: 1.3940 - val_acc: 0.8140\n",
      "Epoch 13/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9996Epoch 00012: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2081 - acc: 0.9996 - val_loss: 1.4166 - val_acc: 0.8140\n",
      "Epoch 14/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 1.0000Epoch 00013: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2051 - acc: 1.0000 - val_loss: 1.3747 - val_acc: 0.8167\n",
      "Epoch 15/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 1.0000Epoch 00014: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2025 - acc: 1.0000 - val_loss: 1.3630 - val_acc: 0.8194\n",
      "Epoch 16/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 1.0000Epoch 00015: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2000 - acc: 1.0000 - val_loss: 1.3695 - val_acc: 0.8194\n",
      "Epoch 17/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 1.0000Epoch 00016: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1978 - acc: 0.9999 - val_loss: 1.3759 - val_acc: 0.8221\n",
      "Epoch 18/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9987Epoch 00017: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.2014 - acc: 0.9987 - val_loss: 1.4030 - val_acc: 0.8086\n",
      "Epoch 19/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9999Epoch 00018: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1946 - acc: 0.9997 - val_loss: 1.4128 - val_acc: 0.8086\n",
      "Epoch 20/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9994Epoch 00019: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1936 - acc: 0.9994 - val_loss: 1.4055 - val_acc: 0.8086\n",
      "Epoch 21/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 1.0000Epoch 00020: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1903 - acc: 0.9999 - val_loss: 1.4051 - val_acc: 0.8113\n",
      "Epoch 22/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9993Epoch 00021: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1915 - acc: 0.9991 - val_loss: 1.4141 - val_acc: 0.8167\n",
      "Epoch 23/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9993Epoch 00022: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1900 - acc: 0.9993 - val_loss: 1.4650 - val_acc: 0.8059\n",
      "Epoch 24/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9996Epoch 00023: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1884 - acc: 0.9996 - val_loss: 1.5011 - val_acc: 0.8059\n",
      "Epoch 25/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 1.0000Epoch 00024: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1852 - acc: 1.0000 - val_loss: 1.4545 - val_acc: 0.8032\n",
      "Epoch 26/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9999Epoch 00025: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1833 - acc: 0.9999 - val_loss: 1.4509 - val_acc: 0.8059\n",
      "Epoch 27/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9997Epoch 00026: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1818 - acc: 0.9997 - val_loss: 1.4412 - val_acc: 0.8059\n",
      "Epoch 28/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9999Epoch 00027: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1798 - acc: 0.9999 - val_loss: 1.4333 - val_acc: 0.8059\n",
      "Epoch 29/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 1.0000Epoch 00028: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1778 - acc: 1.0000 - val_loss: 1.4392 - val_acc: 0.8032\n",
      "Epoch 30/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9996Epoch 00029: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1774 - acc: 0.9996 - val_loss: 1.4186 - val_acc: 0.7978\n",
      "Epoch 31/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 1.0000Epoch 00030: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1738 - acc: 1.0000 - val_loss: 1.3928 - val_acc: 0.8032\n",
      "Epoch 32/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9999Epoch 00031: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6948/6948 [==============================] - 140s - loss: 0.1719 - acc: 0.9999 - val_loss: 1.3978 - val_acc: 0.8113\n",
      "Epoch 33/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 1.0000Epoch 00032: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1695 - acc: 1.0000 - val_loss: 1.3900 - val_acc: 0.8032\n",
      "Epoch 34/10000\n",
      "6944/6948 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 1.0000Epoch 00033: val_acc did not improve\n",
      "6948/6948 [==============================] - 140s - loss: 0.1671 - acc: 1.0000 - val_loss: 1.3821 - val_acc: 0.8059\n",
      "(6948, 40, 345, 1)\n",
      "(6948, 41)\n",
      "(371, 40, 345, 1)\n",
      "(371, 41)\n",
      "Train on 6948 samples, validate on 371 samples\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,3,22,1024]\n\t [[Node: training_3/Adam/gradients/zeros_169 = Fill[T=DT_FLOAT, _class=[\"loc:@add_86/add\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_3/Adam/gradients/Shape_170, training_3/Adam/gradients/zeros_169/Const)]]\n\t [[Node: loss_3/add_154/_31335 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_73481_loss_3/add_154\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'training_3/Adam/gradients/zeros_169', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-afdc6bf6191b>\", line 50, in <module>\n    batch_size=32, epochs=10000)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\", line 1575, in fit\n    self._make_train_function()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\", line 960, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2310, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 572, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1345, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1442, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1771, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,3,22,1024]\n\t [[Node: training_3/Adam/gradients/zeros_169 = Fill[T=DT_FLOAT, _class=[\"loc:@add_86/add\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_3/Adam/gradients/Shape_170, training_3/Adam/gradients/zeros_169/Const)]]\n\t [[Node: loss_3/add_154/_31335 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_73481_loss_3/add_154\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,3,22,1024]\n\t [[Node: training_3/Adam/gradients/zeros_169 = Fill[T=DT_FLOAT, _class=[\"loc:@add_86/add\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_3/Adam/gradients/Shape_170, training_3/Adam/gradients/zeros_169/Const)]]\n\t [[Node: loss_3/add_154/_31335 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_73481_loss_3/add_154\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-afdc6bf6191b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), callbacks=callbacks_list,\n\u001b[0;32m---> 50\u001b[0;31m                         batch_size=32, epochs=10000)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,3,22,1024]\n\t [[Node: training_3/Adam/gradients/zeros_169 = Fill[T=DT_FLOAT, _class=[\"loc:@add_86/add\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_3/Adam/gradients/Shape_170, training_3/Adam/gradients/zeros_169/Const)]]\n\t [[Node: loss_3/add_154/_31335 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_73481_loss_3/add_154\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'training_3/Adam/gradients/zeros_169', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-afdc6bf6191b>\", line 50, in <module>\n    batch_size=32, epochs=10000)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\", line 1575, in fit\n    self._make_train_function()\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/engine/training.py\", line 960, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2310, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 572, in gradients\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1345, in ZerosLikeOutsideLoop\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1442, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1771, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/mike/Desktop/venv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,3,22,1024]\n\t [[Node: training_3/Adam/gradients/zeros_169 = Fill[T=DT_FLOAT, _class=[\"loc:@add_86/add\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_3/Adam/gradients/Shape_170, training_3/Adam/gradients/zeros_169/Const)]]\n\t [[Node: loss_3/add_154/_31335 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_73481_loss_3/add_154\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = 'model_full_resnet152'\n",
    "refine_path = 'model_full_resnet152_refine'\n",
    "\n",
    "all_x = np.concatenate( (np.load('data/mfcc/X_train.npy') , np.load('data/X_test.npy')))\n",
    "\n",
    "if not os.path.exists(refine_path):\n",
    "    os.mkdir(refine_path)\n",
    "\n",
    "for i in range(1,11):\n",
    "    X_train = np.load('data/ten_fold_data/X_train_{}.npy'.format(i)) \n",
    "    Y_train = np.load('data/ten_fold_data/Y_train_{}.npy'.format(i)) \n",
    "    X_test = np.load('data/ten_fold_data/X_valid_{}.npy'.format(i))\n",
    "    Y_test = np.load('data/ten_fold_data/Y_valid_{}.npy'.format(i))\n",
    "    \n",
    "    X_train = np.append(X_train,X_semi , axis=0)\n",
    "    Y_train = np.append(Y_train,Y_semi , axis=0)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    model = load_model(join(model_path,'best_{}.h5'.format(i)))\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(join(refine_path , 'semi_%d_{val_acc:.5f}.h5'%i), monitor='val_acc', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=30)\n",
    "    callbacks_list = [checkpoint, early]\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(all_x), v_h=np.max(all_x)) # Trainset's boundaries.\n",
    "    )\n",
    "    \n",
    "    mygenerator = MixupGenerator(X_train, Y_train, alpha=1.0, batch_size=128, datagen=datagen)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(lr=0.0001),\n",
    "             metrics=['accuracy'])\n",
    "    # mixup\n",
    "#     history = model.fit_generator(mygenerator(),\n",
    "#                     steps_per_epoch= X_train.shape[0] // 128,\n",
    "#                     epochs=10000,\n",
    "#                     validation_data=(X_test,Y_test), callbacks=callbacks_list)\n",
    "    # normalize\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), callbacks=callbacks_list,\n",
    "                        batch_size=32, epochs=10000)\n",
    "\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
