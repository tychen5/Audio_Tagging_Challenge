{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "# from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import pandas as pd # data frame\n",
    "import numpy as np # matrix math\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "import matplotlib.pyplot as plt # to view graphs\n",
    "import wave\n",
    "from math import log, floor\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import KMeans\n",
    "import sys, os\n",
    "import random,math\n",
    "from tqdm import tqdm ##\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "from keras.utils import *\n",
    "from sklearn.ensemble import *\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "from logHandler import Logger\n",
    "from utils import readCSV, getPath, writePickle,readPickle\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.load('feature/fbank4/mean.npy')\n",
    "std = np.load('feature/fbank4/std.npy')\n",
    "min_ = np.load('feature/fbank4/min.npy')\n",
    "range_ = np.load('feature/fbank4/range.npy')\n",
    "# autoencoder = load_model('model/lgd_dense_AE3.h5')\n",
    "# X_un_dense = np.load('feature/fbank2/semi/X_un_dense.npy')\n",
    "# Y_un_dense = np.load('feature/fbank2/semi/Y_un_dense.npy')\n",
    "# X_test_dense = np.load('feature/fbank2/semi/X_test_dense.npy')\n",
    "# Y_test_dense = np.load('feature/fbank2/semi/Y_test_dense.npy')\n",
    "\n",
    "base_path = 'feature/fbank4/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/fbank4/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def normalize(X_train, X_val):\n",
    "    X_train = (X_train - mean)/(std)\n",
    "#     X_train = (X_train - min_)/range_\n",
    "    X_val = (X_val - mean)/(std)\n",
    "#     X_val = (X_val - min_)/range_\n",
    "\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    first_size = 11\n",
    "    second_size = 8\n",
    "    input_img = Input(shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))\n",
    "    #block1\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(input_img)\n",
    "    act = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, \n",
    "                shared_axes=None)(conv)\n",
    "    # pad = ZeroPadding2D(padding=(2, 2), data_format='channels_last')(act)\n",
    "    bn = BatchNormalization()(act)\n",
    "    dp1 = Dropout(0.25)(bn)\n",
    "\n",
    "    # add = Add()([])\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp1)\n",
    "    act = LeakyReLU(alpha=0.05)(conv)\n",
    "    bn = BatchNormalization()(act)\n",
    "    # pad = ZeroPadding2D(padding=(1, 1), data_format='channels_last')(bn)\n",
    "    # maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "    dp2 = Dropout(0.3)(bn)\n",
    "\n",
    "    # block2\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(add)\n",
    "    act = LeakyReLU(alpha=0.33)(conv)\n",
    "    bn = BatchNormalization()(act)\n",
    "    # pad = ZeroPadding2D(padding=(1, 1), data_format='channels_last')(bn)\n",
    "    dp1 = Dropout(0.35)(bn)\n",
    "\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(add)\n",
    "    act = LeakyReLU(alpha=0.33)(conv)\n",
    "    # pad = ZeroPadding2D(padding=(2, 2), data_format='channels_last')(act)\n",
    "    bn = BatchNormalization()(act)\n",
    "    # maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "    dp2 = Dropout(0.4)(bn)\n",
    "\n",
    "    #block 3\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(add)\n",
    "    act = LeakyReLU(alpha=0.33)(conv)\n",
    "    bn = BatchNormalization()(act)\n",
    "    # pad = ZeroPadding2D(padding=(1, 1), data_format='channels_last')(bn)\n",
    "    dp1 = Dropout(0.45)(bn)\n",
    "\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(add)\n",
    "    act = LeakyReLU(alpha=0.33)(conv)\n",
    "    # pad = ZeroPadding2D(padding=(2, 2), data_format='channels_last')(act)\n",
    "    bn = BatchNormalization()(act)\n",
    "    # maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "    dp2 = Dropout(0.5)(bn)\n",
    "\n",
    "    #block4\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(int(first_size/2)+1,int(second_size/2)+1),padding='same',\n",
    "                  kernel_initializer='glorot_normal')(add)\n",
    "    bn = BatchNormalization()(conv)\n",
    "    act = LeakyReLU(alpha=0.33)(bn)\n",
    "    # pad = ZeroPadding2D(padding=(2, 2), data_format='channels_last')(act)\n",
    "    dp1 = Dropout(0.55)(act)\n",
    "\n",
    "    add = Add()([dp1,dp2])\n",
    "    conv = Conv2D(32,(int(first_size/2)+1,int(second_size/2)+1),padding='same',\n",
    "                  kernel_initializer='glorot_normal')(add)\n",
    "    bn = BatchNormalization()(conv)\n",
    "    act = LeakyReLU(alpha=0.33)(bn)\n",
    "    # pad = ZeroPadding2D(padding=(1, 1), data_format='channels_last')(bn)\n",
    "    # avgpool = AveragePooling2D(pool_size=(1,4))(act)\n",
    "    # maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "    cnn = Dropout(0.6)(act)\n",
    "    shape = K.int_shape(cnn)\n",
    "\n",
    "    # clf block\n",
    "    x = Reshape((shape[1],shape[2]*shape[3]))(cnn)\n",
    "    # x = TimeDistributed(Dense(128),input_shape=(11,2,256))(cnn)\n",
    "    x = LSTM(1024,return_sequences=True,dropout=0.75,recurrent_dropout=0.75,\n",
    "                             kernel_initializer='lecun_normal')(x)\n",
    "    res1 = GRU(512,return_sequences=False,dropout=0.55,recurrent_dropout=0.55,\n",
    "            kernel_initializer='lecun_normal')(x)\n",
    "    # x = Concatenate()([x,encoder])\n",
    "    x = BatchNormalization()(res1)\n",
    "    '''\n",
    "    for i in range(101):\n",
    "        x_ = Dense(512, activation='selu', kernel_initializer='lecun_normal',\n",
    "                     kernel_regularizer=l2(1e-4))(x)\n",
    "        x_ = Dropout(0.3)(x_)\n",
    "        x = Add()([x_, x])\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    '''\n",
    "    for i in range(2):\n",
    "        res2 = Dense(256, activation='selu', kernel_initializer='lecun_normal',\n",
    "                     kernel_regularizer=l2(3e-4))(x)\n",
    "        x = Dropout(0.45)(res2)\n",
    "        x = Concatenate()([x,res1])\n",
    "        x = BatchNormalization()(x)\n",
    "        res1 = Dense(256, activation='selu', kernel_initializer='lecun_normal',\n",
    "                     kernel_regularizer=l2(3e-4))(x)\n",
    "        x = Dropout(0.45)(res1)\n",
    "        x = Concatenate()([x,res2])\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    # res2 = Dense(256, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    # x = Dropout(0.25)(res2)\n",
    "    # x = Concatenate()([x,res1])\n",
    "    # x = BatchNormalization()(x)\n",
    "    # res1 = Dense(256, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    # x = Dropout(0.25)(res1)\n",
    "    # x = Concatenate()([x,res2])\n",
    "    # x = BatchNormalization()(x)\n",
    "    # res2 = Dense(128, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    # x = Dropout(0.25)(res2)\n",
    "    # x = Concatenate()([x,res1])\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    # x = Dropout(0.25)(x)\n",
    "    clf = Dense(41,activation='softmax',name='clf')(x)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=clf)\n",
    "    # model = Model(input_img,decoder)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 88, 64, 1) (3339, 41)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 88, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 88, 64, 32)   2848        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 88, 64, 32)   180224      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 88, 64, 32)   128         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 88, 64, 32)   90144       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 88, 64, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 88, 64, 32)   128         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 88, 64, 32)   0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 88, 64, 32)   90144       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 88, 64, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 88, 64, 32)   128         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 88, 64, 32)   0           dropout_3[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 88, 64, 32)   90144       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 88, 64, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 88, 64, 32)   128         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 88, 64, 32)   0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 88, 64, 32)   90144       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 88, 64, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 88, 64, 32)   128         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 88, 64, 32)   0           dropout_5[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 88, 64, 32)   90144       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 88, 64, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 88, 64, 32)   128         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 88, 64, 32)   0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 88, 64, 32)   30752       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 88, 64, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 88, 64, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 88, 64, 32)   0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 88, 64, 32)   0           dropout_7[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 88, 64, 32)   30752       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 88, 64, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 88, 64, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 88, 64, 32)   0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 88, 2048)     0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 88, 1024)     12587008    reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 512)          2360832     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 512)          2048        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           dropout_9[0][0]                  \n",
      "                                                                 gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 768)          3072        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           dropout_10[0][0]                 \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512)          2048        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           dropout_11[0][0]                 \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512)          2048        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          131328      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           dropout_12[0][0]                 \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 512)          2048        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           32832       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "clf (Dense)                     (None, 41)           2665        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,281,769\n",
      "Trainable params: 16,275,625\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3339 samples, validate on 371 samples\n",
      "Epoch 1/3000\n",
      "3339/3339 [==============================] - 22s 7ms/step - loss: 4.3700 - acc: 0.0662 - val_loss: 5.0125 - val_acc: 0.0809\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.08086, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 2/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.7753 - acc: 0.1168 - val_loss: 5.2678 - val_acc: 0.0836\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.08086 to 0.08356, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 3/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.5448 - acc: 0.1444 - val_loss: 3.9493 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.08356 to 0.14016, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 4/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3922 - acc: 0.1692 - val_loss: 3.6588 - val_acc: 0.1995\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.14016 to 0.19946, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 5/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.2878 - acc: 0.1869 - val_loss: 4.0334 - val_acc: 0.1456\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.19946\n",
      "Epoch 6/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.2326 - acc: 0.2037 - val_loss: 3.7727 - val_acc: 0.1860\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.19946\n",
      "Epoch 7/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.0592 - acc: 0.2381 - val_loss: 4.2814 - val_acc: 0.1348\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.19946\n",
      "Epoch 8/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.9692 - acc: 0.2576 - val_loss: 7.1477 - val_acc: 0.1267\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.19946\n",
      "Epoch 9/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.0148 - acc: 0.2387 - val_loss: 5.0006 - val_acc: 0.1159\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.19946\n",
      "Epoch 10/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.8900 - acc: 0.2609 - val_loss: 6.9398 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.19946\n",
      "Epoch 11/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.7686 - acc: 0.2908 - val_loss: 3.5185 - val_acc: 0.1995\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.19946 to 0.19946, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 12/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.7493 - acc: 0.2938 - val_loss: 4.0013 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.19946\n",
      "Epoch 13/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.6607 - acc: 0.3124 - val_loss: 2.8227 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.19946 to 0.26146, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 14/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.5431 - acc: 0.3342 - val_loss: 2.9235 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.26146 to 0.29919, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 15/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.6414 - acc: 0.3232 - val_loss: 3.9088 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.29919\n",
      "Epoch 16/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.5546 - acc: 0.3294 - val_loss: 5.2958 - val_acc: 0.1563\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.29919\n",
      "Epoch 17/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.5038 - acc: 0.3441 - val_loss: 2.7171 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.29919 to 0.32615, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 18/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.4705 - acc: 0.3540 - val_loss: 4.5364 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.32615\n",
      "Epoch 19/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.5571 - acc: 0.3297 - val_loss: 2.9307 - val_acc: 0.2507\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.32615\n",
      "Epoch 20/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.4540 - acc: 0.3525 - val_loss: 3.1912 - val_acc: 0.2453\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.32615\n",
      "Epoch 21/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.3973 - acc: 0.3579 - val_loss: 3.5533 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.32615\n",
      "Epoch 22/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.4127 - acc: 0.3702 - val_loss: 3.0668 - val_acc: 0.2507\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.32615\n",
      "Epoch 23/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.3063 - acc: 0.3836 - val_loss: 3.3008 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.32615\n",
      "Epoch 24/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3380 - acc: 0.3783 - val_loss: 2.9602 - val_acc: 0.2426\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.32615\n",
      "Epoch 25/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3016 - acc: 0.3971 - val_loss: 3.8930 - val_acc: 0.2022\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.32615\n",
      "Epoch 26/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2921 - acc: 0.3941 - val_loss: 2.8595 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.32615\n",
      "Epoch 27/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2670 - acc: 0.3935 - val_loss: 2.8662 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.32615\n",
      "Epoch 28/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.3354 - acc: 0.3896 - val_loss: 2.6974 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.32615 to 0.33154, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 29/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2609 - acc: 0.3935 - val_loss: 5.3831 - val_acc: 0.1456\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.33154\n",
      "Epoch 30/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.3401 - acc: 0.3759 - val_loss: 2.5984 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.33154\n",
      "Epoch 31/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.3470 - acc: 0.3768 - val_loss: 3.1493 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.33154\n",
      "Epoch 32/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2357 - acc: 0.3965 - val_loss: 3.1919 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.33154\n",
      "Epoch 33/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2702 - acc: 0.3920 - val_loss: 4.5335 - val_acc: 0.1806\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.33154\n",
      "Epoch 34/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3337 - acc: 0.3747 - val_loss: 3.6086 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.33154\n",
      "Epoch 35/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2388 - acc: 0.4010 - val_loss: 3.0997 - val_acc: 0.2453\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.33154\n",
      "Epoch 36/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3160 - acc: 0.3813 - val_loss: 2.9109 - val_acc: 0.2722\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.33154\n",
      "Epoch 37/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2224 - acc: 0.3992 - val_loss: 3.2712 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.33154\n",
      "Epoch 38/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2345 - acc: 0.4004 - val_loss: 2.8043 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.33154\n",
      "Epoch 39/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2449 - acc: 0.3914 - val_loss: 4.7784 - val_acc: 0.1429\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.33154\n",
      "Epoch 40/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1435 - acc: 0.4286 - val_loss: 3.5611 - val_acc: 0.2453\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.33154\n",
      "Epoch 41/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1261 - acc: 0.4364 - val_loss: 4.5467 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.33154\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2290 - acc: 0.4037 - val_loss: 2.5631 - val_acc: 0.3369\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.33154 to 0.33693, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 43/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1117 - acc: 0.4349 - val_loss: 3.2246 - val_acc: 0.2722\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.33693\n",
      "Epoch 44/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1411 - acc: 0.4355 - val_loss: 3.9910 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.33693\n",
      "Epoch 45/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2059 - acc: 0.4133 - val_loss: 2.5860 - val_acc: 0.3423\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.33693 to 0.34232, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 46/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1793 - acc: 0.4166 - val_loss: 2.7706 - val_acc: 0.2830\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.34232\n",
      "Epoch 47/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1552 - acc: 0.4235 - val_loss: 3.3848 - val_acc: 0.2507\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.34232\n",
      "Epoch 48/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1419 - acc: 0.4187 - val_loss: 5.3183 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.34232\n",
      "Epoch 49/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1958 - acc: 0.4109 - val_loss: 2.9443 - val_acc: 0.2803\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.34232\n",
      "Epoch 50/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2651 - acc: 0.3926 - val_loss: 2.9252 - val_acc: 0.3154\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.34232\n",
      "Epoch 51/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1273 - acc: 0.4277 - val_loss: 2.7133 - val_acc: 0.2938\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.34232\n",
      "Epoch 52/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1004 - acc: 0.4241 - val_loss: 4.3928 - val_acc: 0.2156\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.34232\n",
      "Epoch 53/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1717 - acc: 0.4121 - val_loss: 2.5417 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.34232\n",
      "Epoch 54/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.0436 - acc: 0.4489 - val_loss: 6.0925 - val_acc: 0.1806\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.34232\n",
      "Epoch 55/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1228 - acc: 0.4178 - val_loss: 4.9665 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.34232\n",
      "Epoch 56/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2342 - acc: 0.3959 - val_loss: 2.9636 - val_acc: 0.2722\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.34232\n",
      "Epoch 57/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1744 - acc: 0.4085 - val_loss: 2.5180 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.34232 to 0.35310, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 58/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1156 - acc: 0.4238 - val_loss: 2.9742 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.35310\n",
      "Epoch 59/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1389 - acc: 0.4250 - val_loss: 2.8017 - val_acc: 0.2938\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.35310\n",
      "Epoch 60/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1769 - acc: 0.4169 - val_loss: 2.2980 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.35310 to 0.41240, saving model to model/LGD_ConvLSTM_clf_0.h5\n",
      "Epoch 61/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.0849 - acc: 0.4549 - val_loss: 4.3058 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.41240\n",
      "Epoch 62/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1095 - acc: 0.4340 - val_loss: 3.0308 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.41240\n",
      "Epoch 63/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.0693 - acc: 0.4346 - val_loss: 3.2016 - val_acc: 0.2480\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.41240\n",
      "Epoch 64/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1394 - acc: 0.4235 - val_loss: 2.4953 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.41240\n",
      "Epoch 65/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1110 - acc: 0.4277 - val_loss: 2.6254 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.41240\n",
      "Epoch 66/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2547 - acc: 0.3947 - val_loss: 2.6624 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.41240\n",
      "Epoch 67/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1964 - acc: 0.3971 - val_loss: 5.2850 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.41240\n",
      "Epoch 68/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2675 - acc: 0.3983 - val_loss: 2.6443 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.41240\n",
      "Epoch 69/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1634 - acc: 0.4187 - val_loss: 2.2805 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.41240\n",
      "Epoch 70/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1111 - acc: 0.4367 - val_loss: 4.2983 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.41240\n",
      "Epoch 71/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.1718 - acc: 0.4172 - val_loss: 4.7529 - val_acc: 0.1671\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.41240\n",
      "Epoch 72/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2021 - acc: 0.4082 - val_loss: 4.9904 - val_acc: 0.1321\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.41240\n",
      "Epoch 73/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2247 - acc: 0.3944 - val_loss: 2.6570 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.41240\n",
      "Epoch 74/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.1777 - acc: 0.4160 - val_loss: 2.5335 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.41240\n",
      "Epoch 75/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2495 - acc: 0.3872 - val_loss: 2.6322 - val_acc: 0.3504\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.41240\n",
      "Epoch 76/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2179 - acc: 0.4064 - val_loss: 2.8344 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.41240\n",
      "Epoch 77/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.4870 - acc: 0.3492 - val_loss: 3.3603 - val_acc: 0.1779\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.41240\n",
      "Epoch 78/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.4732 - acc: 0.3531 - val_loss: 3.3046 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.41240\n",
      "Epoch 79/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3182 - acc: 0.3854 - val_loss: 2.7581 - val_acc: 0.2965\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.41240\n",
      "Epoch 80/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2996 - acc: 0.3854 - val_loss: 5.2860 - val_acc: 0.1240\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.41240\n",
      "Epoch 81/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.3043 - acc: 0.3860 - val_loss: 2.6639 - val_acc: 0.3154\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.41240\n",
      "Epoch 82/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2728 - acc: 0.3878 - val_loss: 3.0356 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.41240\n",
      "Epoch 83/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.2541 - acc: 0.3887 - val_loss: 2.9576 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.41240\n",
      "Epoch 84/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.2963 - acc: 0.3851 - val_loss: 6.8779 - val_acc: 0.0863\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.41240\n",
      "Epoch 85/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.6305 - acc: 0.3040 - val_loss: 3.6120 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.41240\n",
      "Epoch 86/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.6739 - acc: 0.3010 - val_loss: 3.3840 - val_acc: 0.1779\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.41240\n",
      "Epoch 87/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 2.6536 - acc: 0.2929 - val_loss: 3.0475 - val_acc: 0.2237\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.41240\n",
      "Epoch 88/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.6253 - acc: 0.3004 - val_loss: 3.1352 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.41240\n",
      "Epoch 89/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 2.6487 - acc: 0.2914 - val_loss: 4.7794 - val_acc: 0.0458\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.41240\n",
      "Epoch 90/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.5956 - acc: 0.1291 - val_loss: 3.6424 - val_acc: 0.1105\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.41240\n",
      "Epoch 91/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.6345 - acc: 0.1132 - val_loss: 3.6992 - val_acc: 0.0970\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.41240\n",
      "Epoch 92/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.8531 - acc: 0.0659 - val_loss: 3.9479 - val_acc: 0.0674\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.41240\n",
      "Epoch 93/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.8387 - acc: 0.0761 - val_loss: 3.9669 - val_acc: 0.0512\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.41240\n",
      "Epoch 94/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.8468 - acc: 0.0749 - val_loss: 3.8814 - val_acc: 0.0458\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.41240\n",
      "Epoch 95/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.7595 - acc: 0.0854 - val_loss: 3.7274 - val_acc: 0.0809\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.41240\n",
      "Epoch 96/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.6933 - acc: 0.0913 - val_loss: 3.6365 - val_acc: 0.0755\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.41240\n",
      "Epoch 97/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.6015 - acc: 0.0937 - val_loss: 3.5733 - val_acc: 0.0970\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.41240\n",
      "Epoch 98/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.5259 - acc: 0.1051 - val_loss: 3.5325 - val_acc: 0.1051\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.41240\n",
      "Epoch 99/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.4767 - acc: 0.1063 - val_loss: 4.2682 - val_acc: 0.0674\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.41240\n",
      "Epoch 100/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.4438 - acc: 0.1138 - val_loss: 4.3692 - val_acc: 0.0755\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.41240\n",
      "Epoch 101/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.4266 - acc: 0.1228 - val_loss: 4.5878 - val_acc: 0.0647\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.41240\n",
      "Epoch 102/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.4127 - acc: 0.1123 - val_loss: 4.7679 - val_acc: 0.0458\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.41240\n",
      "Epoch 103/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.4022 - acc: 0.1237 - val_loss: 4.7701 - val_acc: 0.0620\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.41240\n",
      "Epoch 104/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.3942 - acc: 0.1162 - val_loss: 4.9434 - val_acc: 0.0620\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.41240\n",
      "Epoch 105/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3947 - acc: 0.1261 - val_loss: 4.6788 - val_acc: 0.0512\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.41240\n",
      "Epoch 106/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3753 - acc: 0.1153 - val_loss: 4.3878 - val_acc: 0.0674\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.41240\n",
      "Epoch 107/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3745 - acc: 0.1144 - val_loss: 4.3806 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.41240\n",
      "Epoch 108/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3620 - acc: 0.1225 - val_loss: 4.5834 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.41240\n",
      "Epoch 109/3000\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 3.3456 - acc: 0.1297 - val_loss: 3.3877 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.41240\n",
      "Epoch 110/3000\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 3.3876 - acc: 0.1204 - val_loss: 3.3422 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.41240\n",
      "Epoch 00110: early stopping\n"
     ]
    }
   ],
   "source": [
    "# val_set_num = [x+1 for x in range(num_fold)]#str(sys.argv[1])\n",
    "val_set_num = [0]#str(sys.argv[1])\n",
    "min_ = np.swapaxes(min_,0,1)\n",
    "mean = np.swapaxes(mean,0,1)\n",
    "range_ = np.swapaxes(range_,0,1)\n",
    "std = np.swapaxes(std,0,1)\n",
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    model = get_model()\n",
    "    batchSize=[128,256]\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=100\n",
    "    epoch=3000\n",
    "    saveP = 'model/LGD_ConvLSTM_clf_'+str(fold)+'.h5'\n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=Adam(lr=3e-3,decay=1e-8),\n",
    "                  metrics=['acc']) \n",
    "\n",
    "    callback=[\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/2),\n",
    "                                      min_lr=1e-4,mode='min'),\n",
    "        EarlyStopping(patience=patien,monitor='val_acc',verbose=1,mode='max'),\n",
    "        ModelCheckpoint(saveP,monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,mode='max'),\n",
    "    ]\n",
    "\n",
    "    model.fit(X_train,Y_train,\n",
    "              shuffle=True,\n",
    "              callbacks=callback, \n",
    "              class_weight='auto',\n",
    "              validation_data=(X_valid,Y_valid),\n",
    "              batch_size=batchSize,\n",
    "              epochs=epoch)\n",
    "#1 =>0.41\n",
    "#2 =>0.51482\n",
    "#3 =>0.45822\n",
    "#4 =>0.49596\n",
    "#5 =>0.50674\n",
    "#6 =>0.33154\n",
    "#7 =>0.54717\n",
    "#8 => 0.37736\n",
    "#9 => 0.40431\n",
    "#0 => 0.41240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 88, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 88, 64, 32)   2080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 88, 64, 32)   180224      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 88, 64, 32)   128         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 88, 64, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 88, 64, 32)   65568       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 88, 64, 32)   180224      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 88, 64, 32)   128         p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 44, 32, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 44, 32, 32)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 44, 32, 64)   131136      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 44, 32, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 44, 32, 64)   256         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 44, 32, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 44, 32, 64)   262208      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 44, 32, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44, 32, 64)   256         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 22, 16, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 22, 16, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 22, 16, 128)  524416      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 22, 16, 128)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 22, 16, 128)  512         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 22, 16, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 22, 16, 128)  1048704     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 22, 16, 128)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 16, 128)  512         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 11, 8, 128)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 11, 8, 128)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 11, 8, 256)   524544      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 11, 8, 256)   1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 11, 8, 256)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 11, 8, 256)   0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 11, 8, 256)   1048832     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 8, 256)   1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 11, 8, 256)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 11, 2, 256)   0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 11, 2, 256)   0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 11, 512)      0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 11, 512)      2099200     reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 512)          1574400     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512)          2048        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           dropout_9[0][0]                  \n",
      "                                                                 gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1024)         4096        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          524800      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout_10[0][0]                 \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1024)         4096        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          524800      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           dropout_11[0][0]                 \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1024)         4096        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 512)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           dropout_12[0][0]                 \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1024)         4096        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          524800      batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           dropout_13[0][0]                 \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1024)         4096        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1024)         0           dropout_14[0][0]                 \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1024)         4096        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          524800      batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 512)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           dropout_15[0][0]                 \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1024)         4096        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          524800      batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1024)         0           dropout_16[0][0]                 \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1024)         4096        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512)          524800      batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dropout_17[0][0]                 \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 1024)         4096        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 512)          524800      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 512)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           dropout_18[0][0]                 \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 1024)         4096        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          524800      batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 512)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1024)         0           dropout_19[0][0]                 \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 1024)         4096        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 512)          524800      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 512)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1024)         0           dropout_20[0][0]                 \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 1024)         4096        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 512)          524800      batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 512)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1024)         0           dropout_21[0][0]                 \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1024)         4096        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 512)          524800      batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 512)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1024)         0           dropout_22[0][0]                 \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 1024)         4096        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 512)          524800      batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 512)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1024)         0           dropout_23[0][0]                 \n",
      "                                                                 dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 1024)         4096        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 512)          524800      batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 512)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1024)         0           dropout_24[0][0]                 \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 1024)         4096        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 512)          524800      batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 512)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1024)         0           dropout_25[0][0]                 \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 1024)         4096        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          524800      batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 512)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1024)         0           dropout_26[0][0]                 \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 1024)         4096        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          524800      batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 512)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 1024)         0           dropout_27[0][0]                 \n",
      "                                                                 dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 1024)         4096        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 512)          524800      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 512)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 1024)         0           dropout_28[0][0]                 \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1024)         4096        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 512)          524800      batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 512)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 1024)         0           dropout_29[0][0]                 \n",
      "                                                                 dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1024)         4096        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 512)          524800      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 512)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 1024)         0           dropout_30[0][0]                 \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1024)         4096        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 512)          524800      batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 512)          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 1024)         0           dropout_31[0][0]                 \n",
      "                                                                 dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 1024)         4096        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 512)          524800      batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 512)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1024)         0           dropout_32[0][0]                 \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 1024)         4096        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 512)          524800      batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 512)          0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 1024)         0           dropout_33[0][0]                 \n",
      "                                                                 dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 1024)         4096        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 512)          524800      batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 512)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 1024)         0           dropout_34[0][0]                 \n",
      "                                                                 dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 1024)         4096        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 512)          524800      batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 512)          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 1024)         0           dropout_35[0][0]                 \n",
      "                                                                 dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1024)         4096        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 512)          524800      batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 512)          0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 1024)         0           dropout_36[0][0]                 \n",
      "                                                                 dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 1024)         4096        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 512)          524800      batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 512)          0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 1024)         0           dropout_37[0][0]                 \n",
      "                                                                 dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1024)         4096        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 512)          524800      batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 512)          0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 1024)         0           dropout_38[0][0]                 \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1024)         4096        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 512)          524800      batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 512)          0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 1024)         0           dropout_39[0][0]                 \n",
      "                                                                 dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 1024)         4096        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 512)          524800      batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 512)          0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 1024)         0           dropout_40[0][0]                 \n",
      "                                                                 dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 1024)         4096        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "clf (Dense)                     (None, 41)           42025       batch_normalization_42[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 24,351,977\n",
      "Trainable params: 24,283,497\n",
      "Non-trainable params: 68,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_size = 8\n",
    "second_size = 8\n",
    "input_img = Input(shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))\n",
    "#block1\n",
    "conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(input_img)\n",
    "act = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, \n",
    "            shared_axes=None)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "dp = Dropout(0.25)(bn)\n",
    "\n",
    "conv = Conv2D(32,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp)\n",
    "act = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, \n",
    "            shared_axes=None)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "dp = Dropout(0.25)(maxpool)\n",
    "\n",
    "# block2\n",
    "conv = Conv2D(64,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp)\n",
    "act = LeakyReLU(alpha=0.33)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "dp = Dropout(0.25)(bn)\n",
    "\n",
    "conv = Conv2D(64,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp)\n",
    "act = LeakyReLU(alpha=0.33)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "dp = Dropout(0.25)(maxpool)\n",
    "\n",
    "#block 3\n",
    "conv = Conv2D(128,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp)\n",
    "act = LeakyReLU(alpha=0.33)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "dp = Dropout(0.25)(bn)\n",
    "\n",
    "conv = Conv2D(128,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(dp)\n",
    "act = LeakyReLU(alpha=0.33)(conv)\n",
    "bn = BatchNormalization()(act)\n",
    "maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "dp = Dropout(0.25)(maxpool)\n",
    "\n",
    "#block4\n",
    "conv = Conv2D(256,(int(first_size/2),int(second_size/2)),padding='same',\n",
    "              kernel_initializer='glorot_normal')(dp)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = LeakyReLU(alpha=0.33)(bn)\n",
    "dp = Dropout(0.25)(act)\n",
    "\n",
    "conv = Conv2D(256,(int(first_size/2),int(second_size/2)),padding='same',\n",
    "              kernel_initializer='glorot_normal')(dp)\n",
    "bn = BatchNormalization()(conv)\n",
    "act = LeakyReLU(alpha=0.33)(bn)\n",
    "avgpool = AveragePooling2D(pool_size=(1,4))(act)\n",
    "# maxpool = MaxPooling2D(pool_size=(2,2))(bn)\n",
    "cnn = Dropout(0.25)(avgpool)\n",
    "\n",
    "\n",
    "# AUTO ENCODER BLOCK\n",
    "shape = K.int_shape(cnn)\n",
    "flat = Flatten()(cnn)\n",
    "x = Dense(4096)(flat)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.33)(x)\n",
    "encoder = Dense(2048,name='encoder',activation='selu')(x)\n",
    "d = Dense(4096)(encoder)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = Dense(shape[1] * shape[2] * shape[3])(d)\n",
    "d = Reshape((shape[1], shape[2], shape[3]))(d)\n",
    "\n",
    "\n",
    "d = UpSampling2D((1,4))(d)\n",
    "d = Conv2DTranspose(256,(int(first_size/2),int(second_size/2)),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = Conv2DTranspose(256,(int(first_size/2),int(second_size/2)),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(128,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = Conv2DTranspose(128,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(64,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = Conv2DTranspose(64,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(32,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "d = Conv2DTranspose(32,(first_size,second_size),padding='same',\n",
    "                    kernel_initializer='glorot_normal')(d)\n",
    "d = LeakyReLU(alpha=0.33)(d)\n",
    "decoder = Conv2DTranspose(1,(first_size,second_size),padding='same',\n",
    "                          kernel_initializer='glorot_normal',name='AE')(d)\n",
    "#  = LeakyReLU(name='AE',alpha=0.33)(d)\n",
    "# d = Conv2DTranspose(1,(first_size,second_size),padding='same',kernel_initializer='glorot_normal')(d)\n",
    "# decoder = LeakyReLU(alpha=0.33)(d)\n",
    "\n",
    "\n",
    "# clf block\n",
    "x = Reshape((shape[1],shape[2]*shape[3]))(cnn)\n",
    "# x = TimeDistributed(Dense(128),input_shape=(11,2,256))(cnn)\n",
    "x = LSTM(512,return_sequences=True,dropout=0.55,recurrent_dropout=0.55,\n",
    "                         kernel_initializer='lecun_normal')(x)\n",
    "res1 = GRU(512,return_sequences=False,dropout=0.55,recurrent_dropout=0.55,\n",
    "        kernel_initializer='lecun_normal')(x)\n",
    "# x = Concatenate()([x,encoder])\n",
    "x = BatchNormalization()(res1)\n",
    "'''\n",
    "for i in range(101):\n",
    "    x_ = Dense(512, activation='selu', kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=l2(1e-4))(x)\n",
    "    x_ = Dropout(0.3)(x_)\n",
    "    x = Add()([x_, x])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "'''\n",
    "for i in range(16):\n",
    "    res2 = Dense(512, activation='selu', kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=l2(5e-5))(x)\n",
    "    x = Dropout(0.28)(res2)\n",
    "    x = Concatenate()([x,res1])\n",
    "    x = BatchNormalization()(x)\n",
    "    res1 = Dense(512, activation='selu', kernel_initializer='lecun_normal',\n",
    "                 kernel_regularizer=l2(5e-5))(x)\n",
    "    x = Dropout(0.28)(res1)\n",
    "    x = Concatenate()([x,res2])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "# res2 = Dense(256, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "# x = Dropout(0.25)(res2)\n",
    "# x = Concatenate()([x,res1])\n",
    "# x = BatchNormalization()(x)\n",
    "# res1 = Dense(256, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "# x = Dropout(0.25)(res1)\n",
    "# x = Concatenate()([x,res2])\n",
    "# x = BatchNormalization()(x)\n",
    "# res2 = Dense(128, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "# x = Dropout(0.25)(res2)\n",
    "# x = Concatenate()([x,res1])\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(128, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "clf = Dense(41,activation='softmax',name='clf')(x)\n",
    "\n",
    "model = Model(inputs=input_img, outputs=clf)\n",
    "# model = Model(input_img,decoder)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3339 samples, validate on 371 samples\n",
      "Epoch 1/500\n",
      "3339/3339 [==============================] - 25s 8ms/step - loss: 4.8642 - acc: 0.0407 - val_loss: 6.3721 - val_acc: 0.0674\n",
      "Epoch 2/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 4.2255 - acc: 0.0656 - val_loss: 5.6264 - val_acc: 0.0458\n",
      "Epoch 3/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.9139 - acc: 0.0848 - val_loss: 5.0452 - val_acc: 0.0997\n",
      "Epoch 4/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.7673 - acc: 0.0964 - val_loss: 4.5385 - val_acc: 0.0836\n",
      "Epoch 5/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.7071 - acc: 0.1045 - val_loss: 4.8070 - val_acc: 0.0970\n",
      "Epoch 6/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.6081 - acc: 0.1066 - val_loss: 3.9529 - val_acc: 0.1240\n",
      "Epoch 7/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.6355 - acc: 0.0982 - val_loss: 4.2757 - val_acc: 0.1024\n",
      "Epoch 8/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.5691 - acc: 0.1219 - val_loss: 4.6494 - val_acc: 0.0566\n",
      "Epoch 9/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.5352 - acc: 0.1198 - val_loss: 3.7559 - val_acc: 0.0970\n",
      "Epoch 10/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.4586 - acc: 0.1273 - val_loss: 4.4831 - val_acc: 0.0836\n",
      "Epoch 11/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.4671 - acc: 0.1390 - val_loss: 4.6518 - val_acc: 0.0889\n",
      "Epoch 12/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.4001 - acc: 0.1485 - val_loss: 5.9465 - val_acc: 0.0135\n",
      "Epoch 13/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.3622 - acc: 0.1447 - val_loss: 4.1979 - val_acc: 0.1105\n",
      "Epoch 14/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.3890 - acc: 0.1417 - val_loss: 4.4966 - val_acc: 0.0836\n",
      "Epoch 15/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.4362 - acc: 0.1387 - val_loss: 4.8378 - val_acc: 0.0512\n",
      "Epoch 16/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.4174 - acc: 0.1321 - val_loss: 4.5721 - val_acc: 0.0674\n",
      "Epoch 17/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.3747 - acc: 0.1381 - val_loss: 4.1370 - val_acc: 0.1024\n",
      "Epoch 18/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.3217 - acc: 0.1533 - val_loss: 4.4184 - val_acc: 0.0539\n",
      "Epoch 19/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.2593 - acc: 0.1701 - val_loss: 5.5444 - val_acc: 0.0485\n",
      "Epoch 20/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.3297 - acc: 0.1468 - val_loss: 4.1285 - val_acc: 0.1240\n",
      "Epoch 21/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.2212 - acc: 0.1689 - val_loss: 4.8608 - val_acc: 0.0701\n",
      "Epoch 22/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.4240 - acc: 0.1360 - val_loss: 4.0452 - val_acc: 0.1132\n",
      "Epoch 23/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.3745 - acc: 0.1465 - val_loss: 4.4898 - val_acc: 0.0782\n",
      "Epoch 24/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.2537 - acc: 0.1650 - val_loss: 3.8214 - val_acc: 0.1105\n",
      "Epoch 25/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.1836 - acc: 0.1797 - val_loss: 4.5026 - val_acc: 0.1321\n",
      "Epoch 26/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.1595 - acc: 0.1827 - val_loss: 4.7810 - val_acc: 0.1159\n",
      "Epoch 27/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.1463 - acc: 0.1854 - val_loss: 4.3287 - val_acc: 0.1105\n",
      "Epoch 28/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.1342 - acc: 0.1875 - val_loss: 4.3676 - val_acc: 0.1186\n",
      "Epoch 29/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.2669 - acc: 0.1530 - val_loss: 4.4878 - val_acc: 0.1267\n",
      "Epoch 30/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.1936 - acc: 0.1827 - val_loss: 5.4439 - val_acc: 0.0863\n",
      "Epoch 31/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.0339 - acc: 0.1962 - val_loss: 4.6139 - val_acc: 0.0916\n",
      "Epoch 32/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.2168 - acc: 0.1737 - val_loss: 5.0167 - val_acc: 0.0916\n",
      "Epoch 33/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.1591 - acc: 0.1845 - val_loss: 5.7806 - val_acc: 0.0755\n",
      "Epoch 34/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.1640 - acc: 0.1803 - val_loss: 6.9133 - val_acc: 0.1078\n",
      "Epoch 35/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 3.1686 - acc: 0.1815 - val_loss: 3.8595 - val_acc: 0.1482\n",
      "Epoch 36/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 3.1003 - acc: 0.1983 - val_loss: 3.6308 - val_acc: 0.1482\n",
      "Epoch 37/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.0264 - acc: 0.2210 - val_loss: 3.4210 - val_acc: 0.1402\n",
      "Epoch 38/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9670 - acc: 0.2159 - val_loss: 4.5250 - val_acc: 0.1186\n",
      "Epoch 39/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.0670 - acc: 0.1926 - val_loss: 4.1148 - val_acc: 0.1105\n",
      "Epoch 40/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9704 - acc: 0.2198 - val_loss: 3.8380 - val_acc: 0.1509\n",
      "Epoch 41/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9866 - acc: 0.2066 - val_loss: 4.8062 - val_acc: 0.1321\n",
      "Epoch 42/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.9230 - acc: 0.2237 - val_loss: 3.9021 - val_acc: 0.1860\n",
      "Epoch 43/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9436 - acc: 0.2234 - val_loss: 6.4572 - val_acc: 0.1159\n",
      "Epoch 44/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.8696 - acc: 0.2303 - val_loss: 5.0259 - val_acc: 0.1671\n",
      "Epoch 45/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9155 - acc: 0.2231 - val_loss: 3.3618 - val_acc: 0.1617\n",
      "Epoch 46/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9348 - acc: 0.2249 - val_loss: 5.7126 - val_acc: 0.1536\n",
      "Epoch 47/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9604 - acc: 0.2243 - val_loss: 3.5533 - val_acc: 0.1968\n",
      "Epoch 48/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8650 - acc: 0.2360 - val_loss: 7.0454 - val_acc: 0.0943\n",
      "Epoch 49/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8637 - acc: 0.2462 - val_loss: 6.8486 - val_acc: 0.0943\n",
      "Epoch 50/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8921 - acc: 0.2279 - val_loss: 8.4534 - val_acc: 0.1078\n",
      "Epoch 51/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.8132 - acc: 0.2534 - val_loss: 4.0756 - val_acc: 0.1509\n",
      "Epoch 52/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7999 - acc: 0.2579 - val_loss: 4.0781 - val_acc: 0.1267\n",
      "Epoch 53/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8609 - acc: 0.2291 - val_loss: 4.5481 - val_acc: 0.1429\n",
      "Epoch 54/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7876 - acc: 0.2522 - val_loss: 4.2513 - val_acc: 0.1105\n",
      "Epoch 55/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8597 - acc: 0.2384 - val_loss: 5.7155 - val_acc: 0.1159\n",
      "Epoch 56/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.8047 - acc: 0.2528 - val_loss: 6.1758 - val_acc: 0.0970\n",
      "Epoch 57/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.8817 - acc: 0.2318 - val_loss: 3.7928 - val_acc: 0.1644\n",
      "Epoch 58/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7911 - acc: 0.2639 - val_loss: 4.5837 - val_acc: 0.1105\n",
      "Epoch 59/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7334 - acc: 0.2719 - val_loss: 7.1138 - val_acc: 0.1617\n",
      "Epoch 60/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8191 - acc: 0.2357 - val_loss: 4.2500 - val_acc: 0.1482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.9149 - acc: 0.2366 - val_loss: 6.0667 - val_acc: 0.1644\n",
      "Epoch 62/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7993 - acc: 0.2579 - val_loss: 5.2201 - val_acc: 0.0970\n",
      "Epoch 63/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8927 - acc: 0.2405 - val_loss: 8.9181 - val_acc: 0.1132\n",
      "Epoch 64/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.8886 - acc: 0.2354 - val_loss: 7.8924 - val_acc: 0.1348\n",
      "Epoch 65/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7800 - acc: 0.2522 - val_loss: 8.2023 - val_acc: 0.0809\n",
      "Epoch 66/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7240 - acc: 0.2656 - val_loss: 6.0890 - val_acc: 0.1078\n",
      "Epoch 67/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6917 - acc: 0.2776 - val_loss: 6.4981 - val_acc: 0.1456\n",
      "Epoch 68/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.7940 - acc: 0.2582 - val_loss: 8.0691 - val_acc: 0.0889\n",
      "Epoch 69/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7626 - acc: 0.2752 - val_loss: 8.6299 - val_acc: 0.1051\n",
      "Epoch 70/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7136 - acc: 0.2743 - val_loss: 4.9547 - val_acc: 0.1806\n",
      "Epoch 71/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6837 - acc: 0.2842 - val_loss: 5.5931 - val_acc: 0.1402\n",
      "Epoch 72/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8281 - acc: 0.2417 - val_loss: 5.4635 - val_acc: 0.1213\n",
      "Epoch 73/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8113 - acc: 0.2480 - val_loss: 3.8472 - val_acc: 0.1402\n",
      "Epoch 74/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7880 - acc: 0.2585 - val_loss: 7.2931 - val_acc: 0.1590\n",
      "Epoch 75/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7542 - acc: 0.2686 - val_loss: 6.6793 - val_acc: 0.1456\n",
      "Epoch 76/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6921 - acc: 0.2716 - val_loss: 6.8266 - val_acc: 0.1644\n",
      "Epoch 77/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6587 - acc: 0.2812 - val_loss: 8.5091 - val_acc: 0.1159\n",
      "Epoch 78/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6753 - acc: 0.2719 - val_loss: 6.1187 - val_acc: 0.1590\n",
      "Epoch 79/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6335 - acc: 0.2902 - val_loss: 7.4498 - val_acc: 0.1698\n",
      "Epoch 80/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6242 - acc: 0.2899 - val_loss: 7.7274 - val_acc: 0.1024\n",
      "Epoch 81/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6676 - acc: 0.2851 - val_loss: 3.7498 - val_acc: 0.1887\n",
      "Epoch 82/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6415 - acc: 0.2890 - val_loss: 3.5236 - val_acc: 0.2965\n",
      "Epoch 83/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.5725 - acc: 0.3058 - val_loss: 5.0073 - val_acc: 0.1024\n",
      "Epoch 84/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7146 - acc: 0.2698 - val_loss: 5.8443 - val_acc: 0.1536\n",
      "Epoch 85/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.8586 - acc: 0.2435 - val_loss: 5.8836 - val_acc: 0.1240\n",
      "Epoch 86/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7339 - acc: 0.2677 - val_loss: 7.2499 - val_acc: 0.0728\n",
      "Epoch 87/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6572 - acc: 0.2860 - val_loss: 9.7281 - val_acc: 0.0943\n",
      "Epoch 88/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8065 - acc: 0.2486 - val_loss: 5.4215 - val_acc: 0.1725\n",
      "Epoch 89/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7138 - acc: 0.2794 - val_loss: 8.3990 - val_acc: 0.1456\n",
      "Epoch 90/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6275 - acc: 0.2998 - val_loss: 5.5602 - val_acc: 0.1186\n",
      "Epoch 91/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6724 - acc: 0.2842 - val_loss: 6.3948 - val_acc: 0.1725\n",
      "Epoch 92/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5421 - acc: 0.3145 - val_loss: 5.7938 - val_acc: 0.1995\n",
      "Epoch 93/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6007 - acc: 0.2977 - val_loss: 3.9130 - val_acc: 0.1698\n",
      "Epoch 94/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6196 - acc: 0.2968 - val_loss: 7.7108 - val_acc: 0.1321\n",
      "Epoch 95/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6664 - acc: 0.2863 - val_loss: 6.1642 - val_acc: 0.1698\n",
      "Epoch 96/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.9888 - acc: 0.2189 - val_loss: 4.6953 - val_acc: 0.1644\n",
      "Epoch 97/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7218 - acc: 0.2615 - val_loss: 8.0023 - val_acc: 0.1294\n",
      "Epoch 98/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6211 - acc: 0.2932 - val_loss: 4.3256 - val_acc: 0.1456\n",
      "Epoch 99/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.5844 - acc: 0.3022 - val_loss: 6.2529 - val_acc: 0.1887\n",
      "Epoch 100/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5727 - acc: 0.3064 - val_loss: 7.0291 - val_acc: 0.1860\n",
      "Epoch 101/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5858 - acc: 0.3052 - val_loss: 4.2120 - val_acc: 0.1536\n",
      "Epoch 102/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5281 - acc: 0.3237 - val_loss: 6.5902 - val_acc: 0.1294\n",
      "Epoch 103/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5667 - acc: 0.3061 - val_loss: 5.2670 - val_acc: 0.1725\n",
      "Epoch 104/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5304 - acc: 0.3255 - val_loss: 5.4540 - val_acc: 0.1725\n",
      "Epoch 105/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6352 - acc: 0.2926 - val_loss: 5.0133 - val_acc: 0.1536\n",
      "Epoch 106/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5554 - acc: 0.3058 - val_loss: 4.1818 - val_acc: 0.1860\n",
      "Epoch 107/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4987 - acc: 0.3139 - val_loss: 5.0201 - val_acc: 0.1402\n",
      "Epoch 108/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.5226 - acc: 0.3109 - val_loss: 6.7132 - val_acc: 0.1294\n",
      "Epoch 109/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5986 - acc: 0.3085 - val_loss: 7.2667 - val_acc: 0.1617\n",
      "Epoch 110/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4963 - acc: 0.3336 - val_loss: 6.6862 - val_acc: 0.1752\n",
      "Epoch 111/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.7134 - acc: 0.2830 - val_loss: 6.2020 - val_acc: 0.1671\n",
      "Epoch 112/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5226 - acc: 0.3190 - val_loss: 4.4356 - val_acc: 0.1725\n",
      "Epoch 113/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6105 - acc: 0.2920 - val_loss: 5.6504 - val_acc: 0.1806\n",
      "Epoch 114/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.5484 - acc: 0.3196 - val_loss: 3.4441 - val_acc: 0.2237\n",
      "Epoch 115/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4730 - acc: 0.3414 - val_loss: 3.1203 - val_acc: 0.2722\n",
      "Epoch 116/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5704 - acc: 0.3136 - val_loss: 7.0997 - val_acc: 0.1860\n",
      "Epoch 117/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4531 - acc: 0.3396 - val_loss: 6.9307 - val_acc: 0.1482\n",
      "Epoch 118/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4229 - acc: 0.3582 - val_loss: 6.1968 - val_acc: 0.1887\n",
      "Epoch 119/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4723 - acc: 0.3426 - val_loss: 3.8505 - val_acc: 0.1806\n",
      "Epoch 120/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6376 - acc: 0.3001 - val_loss: 6.3512 - val_acc: 0.1321\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5216 - acc: 0.3211 - val_loss: 5.4673 - val_acc: 0.1402\n",
      "Epoch 122/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5307 - acc: 0.3166 - val_loss: 9.8161 - val_acc: 0.1132\n",
      "Epoch 123/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4718 - acc: 0.3474 - val_loss: 9.3834 - val_acc: 0.1429\n",
      "Epoch 124/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4822 - acc: 0.3306 - val_loss: 8.3860 - val_acc: 0.1590\n",
      "Epoch 125/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4884 - acc: 0.3351 - val_loss: 4.5710 - val_acc: 0.1887\n",
      "Epoch 126/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4207 - acc: 0.3543 - val_loss: 2.9720 - val_acc: 0.2695\n",
      "Epoch 127/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4135 - acc: 0.3588 - val_loss: 7.5263 - val_acc: 0.1240\n",
      "Epoch 128/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4445 - acc: 0.3402 - val_loss: 6.3287 - val_acc: 0.1240\n",
      "Epoch 129/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4126 - acc: 0.3417 - val_loss: 6.1741 - val_acc: 0.1536\n",
      "Epoch 130/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4918 - acc: 0.3351 - val_loss: 4.8961 - val_acc: 0.2049\n",
      "Epoch 131/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5576 - acc: 0.3151 - val_loss: 4.9459 - val_acc: 0.1914\n",
      "Epoch 132/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6025 - acc: 0.3115 - val_loss: 4.6017 - val_acc: 0.1563\n",
      "Epoch 133/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4154 - acc: 0.3486 - val_loss: 5.1496 - val_acc: 0.1725\n",
      "Epoch 134/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4315 - acc: 0.3372 - val_loss: 7.7681 - val_acc: 0.1456\n",
      "Epoch 135/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4178 - acc: 0.3483 - val_loss: 4.5661 - val_acc: 0.1563\n",
      "Epoch 136/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4908 - acc: 0.3318 - val_loss: 5.1509 - val_acc: 0.2102\n",
      "Epoch 137/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4225 - acc: 0.3486 - val_loss: 5.5507 - val_acc: 0.2102\n",
      "Epoch 138/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4473 - acc: 0.3372 - val_loss: 7.2703 - val_acc: 0.2102\n",
      "Epoch 139/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4597 - acc: 0.3423 - val_loss: 5.3356 - val_acc: 0.1590\n",
      "Epoch 140/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4966 - acc: 0.3303 - val_loss: 8.8649 - val_acc: 0.1321\n",
      "Epoch 141/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4759 - acc: 0.3381 - val_loss: 7.0803 - val_acc: 0.1617\n",
      "Epoch 142/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4037 - acc: 0.3555 - val_loss: 6.6586 - val_acc: 0.1941\n",
      "Epoch 143/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.3792 - acc: 0.3594 - val_loss: 6.0946 - val_acc: 0.2264\n",
      "Epoch 144/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4172 - acc: 0.3465 - val_loss: 8.4233 - val_acc: 0.1806\n",
      "Epoch 145/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.5214 - acc: 0.3270 - val_loss: 7.6992 - val_acc: 0.1402\n",
      "Epoch 146/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3616 - acc: 0.3636 - val_loss: 5.5315 - val_acc: 0.1294\n",
      "Epoch 147/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3097 - acc: 0.3705 - val_loss: 3.9927 - val_acc: 0.2237\n",
      "Epoch 148/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3378 - acc: 0.3708 - val_loss: 6.9506 - val_acc: 0.1240\n",
      "Epoch 149/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.4747 - acc: 0.3297 - val_loss: 6.6521 - val_acc: 0.1887\n",
      "Epoch 150/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3456 - acc: 0.3648 - val_loss: 8.1216 - val_acc: 0.1590\n",
      "Epoch 151/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.3520 - acc: 0.3771 - val_loss: 7.2457 - val_acc: 0.1671\n",
      "Epoch 152/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3382 - acc: 0.3666 - val_loss: 4.4032 - val_acc: 0.2480\n",
      "Epoch 153/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.6651 - acc: 0.3106 - val_loss: 6.3171 - val_acc: 0.1429\n",
      "Epoch 154/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5301 - acc: 0.3229 - val_loss: 6.6895 - val_acc: 0.1779\n",
      "Epoch 155/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.3844 - acc: 0.3582 - val_loss: 8.2213 - val_acc: 0.1536\n",
      "Epoch 156/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.4138 - acc: 0.3369 - val_loss: 7.7807 - val_acc: 0.1078\n",
      "Epoch 157/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.7735 - acc: 0.2764 - val_loss: 11.9211 - val_acc: 0.0889\n",
      "Epoch 158/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.5080 - acc: 0.3333 - val_loss: 10.1116 - val_acc: 0.1536\n",
      "Epoch 159/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4010 - acc: 0.3510 - val_loss: 9.8608 - val_acc: 0.1294\n",
      "Epoch 160/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.4061 - acc: 0.3594 - val_loss: 8.6312 - val_acc: 0.1348\n",
      "Epoch 161/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3674 - acc: 0.3669 - val_loss: 6.7087 - val_acc: 0.1267\n",
      "Epoch 162/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.3256 - acc: 0.3741 - val_loss: 5.0237 - val_acc: 0.1833\n",
      "Epoch 163/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3501 - acc: 0.3708 - val_loss: 5.4938 - val_acc: 0.0970\n",
      "Epoch 164/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3605 - acc: 0.3795 - val_loss: 5.1711 - val_acc: 0.1833\n",
      "Epoch 165/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.3081 - acc: 0.3810 - val_loss: 4.7366 - val_acc: 0.2102\n",
      "Epoch 166/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.2889 - acc: 0.3839 - val_loss: 8.3394 - val_acc: 0.1644\n",
      "Epoch 167/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 3.3286 - acc: 0.1899 - val_loss: 13.4480 - val_acc: 0.0485\n",
      "Epoch 168/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.8997 - acc: 0.2318 - val_loss: 12.0050 - val_acc: 0.1267\n",
      "Epoch 169/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7644 - acc: 0.2761 - val_loss: 4.5384 - val_acc: 0.1698\n",
      "Epoch 170/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.7050 - acc: 0.2677 - val_loss: 4.0386 - val_acc: 0.2156\n",
      "Epoch 171/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.6191 - acc: 0.2953 - val_loss: 4.9994 - val_acc: 0.1752\n",
      "Epoch 172/500\n",
      "3339/3339 [==============================] - 8s 2ms/step - loss: 2.7878 - acc: 0.2594 - val_loss: 8.1887 - val_acc: 0.1321\n",
      "Epoch 173/500\n",
      "3339/3339 [==============================] - 8s 3ms/step - loss: 2.6160 - acc: 0.2986 - val_loss: 5.9815 - val_acc: 0.1429\n",
      "Epoch 174/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5506 - acc: 0.3115 - val_loss: 5.3017 - val_acc: 0.1779\n",
      "Epoch 175/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5379 - acc: 0.3184 - val_loss: 6.2864 - val_acc: 0.1456\n",
      "Epoch 176/500\n",
      "3339/3339 [==============================] - 9s 3ms/step - loss: 2.5002 - acc: 0.3318 - val_loss: 8.7019 - val_acc: 0.0512\n",
      "Epoch 177/500\n",
      "2560/3339 [======================>.......] - ETA: 1s - loss: 2.4639 - acc: 0.3363"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-914c271649a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           batch_size=128,epochs=500)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss=['categorical_crossentropy'],optimizer=Adam(),metrics=['acc']) #,loss_weights=[0,1\n",
    "model.fit(X_train,Y_train,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_valid,Y_valid),\n",
    "          batch_size=128,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 88, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 88, 64, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 88, 64, 32)        65568     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 44, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 44, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 44, 32, 64)        262208    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 22, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 22528)             0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 2048)              46139392  \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "clf (Dense)                  (None, 41)                5289      \n",
      "=================================================================\n",
      "Total params: 51,441,769\n",
      "Trainable params: 51,441,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_size = 8\n",
    "second_size = 8\n",
    "input_img = Input(shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))\n",
    "#block1\n",
    "conv = Conv2D(32,(first_size,second_size),padding='same',activation='relu')(input_img)\n",
    "conv = Conv2D(32,(first_size,second_size),padding='same',activation='relu')(conv)\n",
    "maxpool = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "\n",
    "# block2\n",
    "conv = Conv2D(64,(first_size,second_size),padding='same',activation='relu')(maxpool)\n",
    "conv = Conv2D(64,(first_size,second_size),padding='same',activation='relu')(conv)\n",
    "cnn = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "\n",
    "#block 3\n",
    "# conv = Conv2D(128,(first_size,second_size),padding='same',activation='relu')(maxpool)\n",
    "# conv = Conv2D(128,(first_size,second_size),padding='same',activation='relu')(conv)\n",
    "# cnn = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "\n",
    "#block4\n",
    "# conv = Conv2D(256,(int(first_size/2),int(second_size/2)),padding='same',activation='relu' )(maxpool)\n",
    "# conv = Conv2D(256,(int(first_size/2),int(second_size/2)),padding='same',activation='relu')(conv)\n",
    "# cnn = AveragePooling2D(pool_size=(1,4))(conv)\n",
    "\n",
    "\n",
    "\n",
    "# AUTO ENCODER BLOCK\n",
    "shape = K.int_shape(cnn)\n",
    "flat = Flatten()(cnn)\n",
    "x = Dense(4096,activation='relu')(flat)\n",
    "encoder = Dense(2048,name='encoder')(x)\n",
    "d = Dense(4096,activation='relu')(encoder)\n",
    "d = Dense(shape[1] * shape[2] * shape[3])(d)\n",
    "d = Reshape((shape[1], shape[2], shape[3]))(d)\n",
    "\n",
    "# d = UpSampling2D((1,4))(d)\n",
    "# d = Conv2DTranspose(256,(int(first_size/2),int(second_size/2)),padding='same',activation='relu')(d)\n",
    "# d = Conv2DTranspose(256,(int(first_size/2),int(second_size/2)),padding='same',activation='relu')(d)\n",
    "\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(128,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "d = Conv2DTranspose(128,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(64,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "d = Conv2DTranspose(64,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "d = UpSampling2D((2,2))(d)\n",
    "d = Conv2DTranspose(32,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "d = Conv2DTranspose(32,(first_size,second_size),padding='same',activation='relu')(d)\n",
    "decoder = Conv2DTranspose(1,(first_size,second_size),padding='same',name='AE')(d)\n",
    "#  = LeakyReLU(name='AE',alpha=0.33)(d)\n",
    "# d = Conv2DTranspose(1,(first_size,second_size),padding='same')(d)\n",
    "# decoder = LeakyReLU(alpha=0.33)(d)\n",
    "\n",
    "\n",
    "# clf block\n",
    "# x = Reshape((shape[1]*shape[2]*shape[3]))(cnn)\n",
    "x = Flatten()(cnn)\n",
    "# x = TimeDistributed(Dense(128),input_shape=(11,2,256))(cnn)\n",
    "# x = LSTM(512,return_sequences=True)(x)\n",
    "# x = GRU(512,return_sequences=False)(x)\n",
    "# x = Concatenate()([x,encoder])\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "clf = Dense(41,activation='softmax',name='clf')(x)\n",
    "\n",
    "model = Model(inputs=input_img, outputs=clf)\n",
    "# model = Model(input_img,decoder)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
