{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from collections import Counter\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "# import pandas as pd\n",
    "import glob\n",
    "from sklearn.semi_supervised import *\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import pandas as pd # data frame\n",
    "import numpy as np # matrix math\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "import matplotlib.pyplot as plt # to view graphs\n",
    "import wave\n",
    "from math import log, floor\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import KMeans\n",
    "import sys, os\n",
    "import random,math\n",
    "from tqdm import tqdm ##\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "from keras.utils import *\n",
    "from sklearn.ensemble import *\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "from logHandler import Logger\n",
    "from utils import readCSV, getPath, writePickle,readPickle\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import resnet\n",
    "from random_eraser import get_random_eraser\n",
    "from mixup_generator import MixupGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def normalize(X_train, X_val):\n",
    "    X_train = (X_train - mean)/(std)\n",
    "#     X_train = (X_train - min_)/range_\n",
    "    X_val = (X_val - mean)/(std)\n",
    "#     X_val = (X_val - min_)/range_\n",
    "\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# min_ = np.swapaxes(min_,0,1)\n",
    "# mean = np.swapaxes(mean,0,1)\n",
    "# range_ = np.swapaxes(range_,0,1)\n",
    "# std = np.swapaxes(std,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_valid(X_train,Y_train,X_valid,Y_valid,fold):\n",
    "    model = [resnet.ResnetBuilder.build_resnet_18((1, X_train.shape[1], X_train.shape[2]), 41),\n",
    "             resnet.ResnetBuilder.build_resnet_34((1,X_train.shape[1], X_train.shape[2]), 41),\n",
    "             resnet.ResnetBuilder.build_resnet_50((1, X_train.shape[1], X_train.shape[2]), 41) ,]\n",
    "#              resnet.ResnetBuilder.build_resnet_101((1, X_train.shape[1], X_train.shape[2]), 41),]\n",
    "#              resnet.ResnetBuilder.build_resnet_152((1, X_train.shape[1], X_train.shape[2]), 41)]\n",
    "    kk = random.randint(0, 2)#4)\n",
    "    model = model[kk]\n",
    "    print('using resnet model: '+str(kk))\n",
    "\n",
    "    model.summary()\n",
    "    if kk>=3:\n",
    "        batchSize=[32]\n",
    "    elif kk>2:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif kk>=1:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64,128,256]\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=100\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    if not os.path.exists(saveD):\n",
    "        os.makedirs(saveD)\n",
    "    opt = Adam()#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        width_shift_range=0.1*random.random(),\n",
    "        height_shift_range=0.1*random.random(),\n",
    "        shear_range=0.05*random.random(),\n",
    "        preprocessing_function=get_random_eraser(p=0.82+0.13*random.random(),v_l=np.min(X_train), v_h=np.max(X_train)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_train)\n",
    "    generator = MixupGenerator(X_train, Y_train, alpha=1, \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    print('using resnet model: '+str(kk))\n",
    "    if not os.path.exists(logD):\n",
    "        os.makedirs(logD)\n",
    "    history = History()\n",
    "    callback=[\n",
    "    #     ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/2),\n",
    "    #                                   min_lr=1e-4,mode='min', cooldown=1 ),\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_resnet'+str(kk)+'-.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_resnet'+str(kk)),\n",
    "        history#,batch_size=batch_size, write_graph=True, write_grads=False, write_images=True)\n",
    "    ]\n",
    "\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=X_train.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch\n",
    "                       )\n",
    "    return model,kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semi_data(X_train,Y_train):\n",
    "    X_un_ver = np.load('feature/'+feature_type+'/semi/fbank4/X_un_ver.npy')\n",
    "    X_test_ver = np.load('feature/'+feature_type+'/semi/fbank4/X_test_ver.npy')\n",
    "    X = np.concatenate((X_un_ver,X_test_ver))\n",
    "    Y_un_ver = np.load('feature/'+feature_type+'/semi/fbank4/Y_un_ver.npy')\n",
    "    Y_test_ver = np.load('feature/'+feature_type+'/semi/fbank4/Y_test_ver.npy')\n",
    "    Y = np.concatenate((Y_un_ver,Y_test_ver))\n",
    "    Y = to_categorical(Y,num_classes=41)\n",
    "    X_semi = np.concatenate((X_train,X))\n",
    "    Y_semi = np.concatenate((Y_train,Y))\n",
    "    X_semi , Y_semi = _shuffle(X_semi,Y_semi)\n",
    "    print(X_semi.shape , Y_semi.shape)\n",
    "    return X_semi , Y_semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = resnet.ResnetBuilder.build_resnet_50((1, 64, 431), 41)\n",
    "# model.summary()\n",
    "def train_unverified(model,X_semi,Y_semi,fold,kk):\n",
    "    name = glob.glob('model/'+feature_type+'/'+'LGD_fold'+str(fold)+'_resnet'+str(kk)+'-**')[0]\n",
    "    print('semi loading: '+ name)\n",
    "    model = load_model(name)\n",
    "    if kk==4:\n",
    "        batchSize=[32]\n",
    "    elif kk>=2:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif kk>=1:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64,128,256]\n",
    "#     batchSize=[32,64,128,256] ##ERR?\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=100\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    opt = Adam(lr=0.0001,decay=1e-6)#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "#         featurewise_std_normalization=True,\n",
    "        width_shift_range=0.05+0.15*random.random(),\n",
    "        height_shift_range=0.05+0.15*random.random(),\n",
    "        shear_range=0.084375+0.253125*random.random(),\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(X_semi), v_h=np.max(X_semi)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_semi)\n",
    "    test_datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True)\n",
    "    generator = MixupGenerator(X_semi, Y_semi, alpha=0.4+0.6*random.random(), \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "    \n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    history = History()\n",
    "    callback=[\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/10),min_lr=4e-6,\n",
    "                          mode='min', cooldown=1,verbose=1 ), #0.2,/25\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_semi_resnet'+str(kk)+'.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,\n",
    "                        mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_semi_resnet'+str(kk)),\n",
    "        history\n",
    "    ]\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=X_semi.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch,\n",
    "                        initial_epoch = int(patien/25)\n",
    "                       )\n",
    "#     model.fit(X_semi,Y_semi,\n",
    "#               shuffle=True,\n",
    "#               callbacks=callback, \n",
    "#               class_weight='auto',\n",
    "#               validation_data=(X_valid,Y_valid),\n",
    "#               batch_size=batchSize,\n",
    "#               epochs=epoch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'mfcc7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized parameters\n",
    "mean = np.load('feature/'+feature_type+'/mean.npy')\n",
    "std = np.load('feature/'+feature_type+'/std.npy')\n",
    "min_ = np.load('feature/'+feature_type+'/min.npy')\n",
    "range_ = np.load('feature/'+feature_type+'/range.npy')\n",
    "\n",
    "\n",
    "base_path = 'feature/'+feature_type+'/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/'+feature_type+'/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10\n",
    "\n",
    "val_set_num = [7,8,9]#str(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold6_mfcc7===\n",
      "using resnet model: 2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 30, 130, 64)  3200        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 30, 130, 64)  256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 30, 130, 64)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 15, 65, 64)   0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 15, 65, 64)   4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 15, 65, 64)   256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 15, 65, 64)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 15, 65, 64)   36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 15, 65, 64)   256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 15, 65, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 15, 65, 256)  16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 15, 65, 256)  16640       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 15, 65, 256)  0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 15, 65, 256)  1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 15, 65, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 15, 65, 64)   16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 15, 65, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 15, 65, 64)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 65, 64)   36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 15, 65, 64)   256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 15, 65, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 15, 65, 256)  16640       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 15, 65, 256)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 15, 65, 256)  1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 15, 65, 256)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 15, 65, 64)   16448       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 15, 65, 64)   256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 15, 65, 64)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 15, 65, 64)   36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 15, 65, 64)   256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 15, 65, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 15, 65, 256)  16640       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 15, 65, 256)  0           add_26[0][0]                     \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 15, 65, 256)  1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 15, 65, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 33, 128)   32896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 33, 128)   512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 33, 128)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 33, 128)   147584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 33, 128)   512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 33, 128)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 33, 512)   131584      add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 33, 512)   66048       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 33, 512)   0           conv2d_71[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 33, 512)   2048        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 33, 512)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 33, 128)   65664       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 33, 128)   512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 33, 128)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 33, 128)   147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 33, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 33, 128)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 33, 512)   66048       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 33, 512)   0           add_28[0][0]                     \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 33, 512)   2048        add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 33, 512)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 33, 128)   65664       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 33, 128)   512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 33, 128)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 33, 128)   147584      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 33, 128)   512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 33, 128)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 33, 512)   66048       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 33, 512)   0           add_29[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 33, 512)   2048        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 33, 512)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 33, 128)   65664       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 33, 128)   512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 33, 128)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 33, 128)   147584      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 33, 128)   512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 33, 128)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 33, 512)   66048       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 33, 512)   0           add_30[0][0]                     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 33, 512)   2048        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 33, 512)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 17, 256)   131328      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 17, 256)   1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 17, 256)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 17, 256)   590080      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 17, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 17, 256)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 17, 1024)  525312      add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 17, 1024)  263168      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 4, 17, 1024)  0           conv2d_84[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 17, 1024)  4096        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 17, 256)   262400      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 17, 256)   1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 17, 256)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 17, 256)   590080      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 17, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 17, 256)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 17, 1024)  263168      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 17, 1024)  0           add_32[0][0]                     \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 17, 1024)  4096        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 17, 256)   262400      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 17, 256)   1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 17, 256)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 17, 256)   590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 17, 256)   1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 17, 256)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 17, 1024)  263168      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 17, 1024)  0           add_33[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 17, 1024)  4096        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 17, 256)   262400      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 17, 256)   1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 17, 256)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 17, 256)   590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 17, 256)   1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 17, 256)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 17, 1024)  263168      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 17, 1024)  0           add_34[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 17, 1024)  4096        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 17, 256)   262400      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 17, 256)   1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 17, 256)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 17, 256)   590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 17, 256)   1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 17, 256)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 17, 1024)  263168      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4, 17, 1024)  0           add_35[0][0]                     \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 17, 1024)  4096        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 4, 17, 256)   262400      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 17, 256)   1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 17, 256)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 4, 17, 256)   590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 17, 256)   1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 17, 256)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 4, 17, 1024)  263168      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 4, 17, 1024)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 17, 1024)  4096        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 9, 512)    524800      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 2, 9, 512)    2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 9, 512)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 9, 512)    2359808     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 2, 9, 512)    2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 9, 512)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 9, 2048)   2099200     add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 2, 9, 2048)   0           conv2d_103[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 9, 2048)   8192        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 9, 512)    1049088     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 9, 512)    2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 9, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 9, 512)    2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 9, 512)    2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 9, 512)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 2, 9, 2048)   0           add_38[0][0]                     \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 9, 2048)   8192        add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 9, 512)    1049088     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 9, 512)    2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 9, 512)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 9, 512)    2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 9, 512)    2048        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 9, 512)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 2, 9, 2048)   0           add_39[0][0]                     \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 9, 2048)   8192        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 2048)   0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 51)           104499      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 51)           204         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 51)           0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 41)           2132        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,672,915\n",
      "Trainable params: 23,627,373\n",
      "Non-trainable params: 45,542\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "52/52 [==============================] - 16s 302ms/step - loss: 9.0078 - acc: 0.0604 - val_loss: 8.8012 - val_acc: 0.0647\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.06469, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 2/3000\n",
      "52/52 [==============================] - 9s 165ms/step - loss: 7.7923 - acc: 0.0739 - val_loss: 7.1946 - val_acc: 0.0997\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.06469 to 0.09973, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 3/3000\n",
      "52/52 [==============================] - 9s 165ms/step - loss: 6.8076 - acc: 0.0995 - val_loss: 6.8510 - val_acc: 0.0782\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.09973\n",
      "Epoch 4/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 6.0597 - acc: 0.1187 - val_loss: 6.1507 - val_acc: 0.0485\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.09973\n",
      "Epoch 5/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 5.5071 - acc: 0.1385 - val_loss: 5.0723 - val_acc: 0.1725\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.09973 to 0.17251, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 6/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 5.0812 - acc: 0.1611 - val_loss: 5.1313 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.17251\n",
      "Epoch 7/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.7786 - acc: 0.1782 - val_loss: 4.7712 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.17251\n",
      "Epoch 8/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.4948 - acc: 0.2025 - val_loss: 4.4422 - val_acc: 0.1617\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.17251\n",
      "Epoch 9/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.3373 - acc: 0.1926 - val_loss: 4.0359 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.17251 to 0.22102, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 10/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.1431 - acc: 0.2148 - val_loss: 4.6654 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.22102\n",
      "Epoch 11/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.0411 - acc: 0.2248 - val_loss: 3.5581 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.22102 to 0.28571, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 12/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.8676 - acc: 0.2566 - val_loss: 3.3080 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.28571 to 0.33154, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 13/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.8171 - acc: 0.2530 - val_loss: 3.8054 - val_acc: 0.2237\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.33154\n",
      "Epoch 14/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.6785 - acc: 0.2629 - val_loss: 3.2006 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.33154\n",
      "Epoch 15/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.5947 - acc: 0.2803 - val_loss: 3.1565 - val_acc: 0.2884\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.33154\n",
      "Epoch 16/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.5340 - acc: 0.2885 - val_loss: 3.4855 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.33154\n",
      "Epoch 17/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.5057 - acc: 0.2891 - val_loss: 3.0113 - val_acc: 0.3073\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.33154\n",
      "Epoch 18/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.4127 - acc: 0.2990 - val_loss: 3.7160 - val_acc: 0.2264\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.33154\n",
      "Epoch 19/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.3926 - acc: 0.3008 - val_loss: 3.2824 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.33154\n",
      "Epoch 20/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.2901 - acc: 0.3206 - val_loss: 2.9585 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.33154\n",
      "Epoch 21/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.2705 - acc: 0.3173 - val_loss: 2.9076 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.33154\n",
      "Epoch 22/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.2673 - acc: 0.3176 - val_loss: 2.8678 - val_acc: 0.3585\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.33154 to 0.35849, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 23/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1713 - acc: 0.3468 - val_loss: 2.7876 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.35849\n",
      "Epoch 24/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1698 - acc: 0.3534 - val_loss: 2.9918 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.35849\n",
      "Epoch 25/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1316 - acc: 0.3588 - val_loss: 2.4975 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.35849 to 0.42318, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 26/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.1095 - acc: 0.3477 - val_loss: 2.5587 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.42318\n",
      "Epoch 27/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.0763 - acc: 0.3660 - val_loss: 2.4178 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.42318 to 0.46092, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 28/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.0530 - acc: 0.3840 - val_loss: 3.0256 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.46092\n",
      "Epoch 29/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.0075 - acc: 0.3864 - val_loss: 2.5218 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.46092\n",
      "Epoch 30/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9867 - acc: 0.3873 - val_loss: 2.7858 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.46092\n",
      "Epoch 31/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9579 - acc: 0.4075 - val_loss: 2.4868 - val_acc: 0.4043\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.46092\n",
      "Epoch 32/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9484 - acc: 0.4099 - val_loss: 2.3469 - val_acc: 0.4582\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.46092\n",
      "Epoch 33/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9649 - acc: 0.3861 - val_loss: 2.7358 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.46092\n",
      "Epoch 34/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9280 - acc: 0.3984 - val_loss: 2.3958 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.46092\n",
      "Epoch 35/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9139 - acc: 0.4084 - val_loss: 2.6968 - val_acc: 0.3208\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.46092\n",
      "Epoch 36/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8619 - acc: 0.4366 - val_loss: 2.2023 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.46092 to 0.47709, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 37/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8699 - acc: 0.4252 - val_loss: 2.1951 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.47709 to 0.48248, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 38/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8501 - acc: 0.4453 - val_loss: 3.1784 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.48248\n",
      "Epoch 39/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8751 - acc: 0.4288 - val_loss: 3.1952 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.48248\n",
      "Epoch 40/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8411 - acc: 0.4348 - val_loss: 2.6161 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.48248\n",
      "Epoch 41/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8229 - acc: 0.4414 - val_loss: 2.5088 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.48248\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7563 - acc: 0.4567 - val_loss: 2.3626 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.48248\n",
      "Epoch 43/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7926 - acc: 0.4555 - val_loss: 2.2227 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.48248\n",
      "Epoch 44/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7858 - acc: 0.4444 - val_loss: 2.5051 - val_acc: 0.3747\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.48248\n",
      "Epoch 45/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7206 - acc: 0.4627 - val_loss: 2.0551 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.48248 to 0.52291, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 46/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7352 - acc: 0.4769 - val_loss: 2.2166 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.52291\n",
      "Epoch 47/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7277 - acc: 0.4675 - val_loss: 2.2076 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.52291\n",
      "Epoch 48/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7058 - acc: 0.4901 - val_loss: 2.1263 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.52291\n",
      "Epoch 49/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6871 - acc: 0.4814 - val_loss: 2.2381 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.52291\n",
      "Epoch 50/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6934 - acc: 0.4766 - val_loss: 2.4774 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.52291\n",
      "Epoch 51/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6935 - acc: 0.4688 - val_loss: 2.0119 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.52291\n",
      "Epoch 52/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7077 - acc: 0.4700 - val_loss: 2.1969 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.52291\n",
      "Epoch 53/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6741 - acc: 0.4787 - val_loss: 2.0153 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.52291\n",
      "Epoch 54/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6455 - acc: 0.4937 - val_loss: 1.9388 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.52291 to 0.53908, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 55/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.6448 - acc: 0.4970 - val_loss: 2.4240 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.53908\n",
      "Epoch 56/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6161 - acc: 0.5057 - val_loss: 2.1865 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.53908\n",
      "Epoch 57/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.6176 - acc: 0.5117 - val_loss: 1.9232 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.53908\n",
      "Epoch 58/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6244 - acc: 0.5117 - val_loss: 2.3630 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.53908\n",
      "Epoch 59/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6506 - acc: 0.4967 - val_loss: 2.0702 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.53908\n",
      "Epoch 60/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6286 - acc: 0.5093 - val_loss: 1.9066 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.53908\n",
      "Epoch 61/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5787 - acc: 0.5156 - val_loss: 2.1767 - val_acc: 0.4744\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.53908\n",
      "Epoch 62/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.5913 - acc: 0.5081 - val_loss: 2.5118 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.53908\n",
      "Epoch 63/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6107 - acc: 0.5015 - val_loss: 2.7139 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.53908\n",
      "Epoch 64/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5745 - acc: 0.5207 - val_loss: 1.8559 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.53908 to 0.53908, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 65/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5921 - acc: 0.5099 - val_loss: 1.9643 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.53908\n",
      "Epoch 66/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5575 - acc: 0.5228 - val_loss: 1.7995 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.53908 to 0.57951, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 67/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5362 - acc: 0.5388 - val_loss: 2.1406 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.57951\n",
      "Epoch 68/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5347 - acc: 0.5394 - val_loss: 1.9245 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.57951\n",
      "Epoch 69/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5274 - acc: 0.5288 - val_loss: 2.1583 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.57951\n",
      "Epoch 70/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.5453 - acc: 0.5328 - val_loss: 2.0253 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.57951\n",
      "Epoch 71/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4910 - acc: 0.5508 - val_loss: 2.1252 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.57951\n",
      "Epoch 72/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5504 - acc: 0.5319 - val_loss: 2.0971 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.57951\n",
      "Epoch 73/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5318 - acc: 0.5367 - val_loss: 1.9899 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.57951\n",
      "Epoch 74/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4989 - acc: 0.5532 - val_loss: 1.6394 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.57951 to 0.61456, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 75/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4842 - acc: 0.5487 - val_loss: 1.7656 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.61456\n",
      "Epoch 76/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5371 - acc: 0.5291 - val_loss: 1.7239 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.61456\n",
      "Epoch 77/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5124 - acc: 0.5361 - val_loss: 1.7698 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.61456\n",
      "Epoch 78/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5017 - acc: 0.5487 - val_loss: 1.9586 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.61456\n",
      "Epoch 79/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.4712 - acc: 0.5529 - val_loss: 1.6697 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.61456\n",
      "Epoch 80/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.4605 - acc: 0.5523 - val_loss: 1.5836 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.61456 to 0.61725, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 81/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4961 - acc: 0.5547 - val_loss: 1.7742 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.61725\n",
      "Epoch 82/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4774 - acc: 0.5586 - val_loss: 1.6207 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.61725 to 0.61995, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 83/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4501 - acc: 0.5571 - val_loss: 1.6488 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.61995\n",
      "Epoch 84/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4187 - acc: 0.5691 - val_loss: 1.6502 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.61995 to 0.63342, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 85/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4170 - acc: 0.5673 - val_loss: 1.7660 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.63342\n",
      "Epoch 86/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4438 - acc: 0.5634 - val_loss: 1.8612 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.63342\n",
      "Epoch 87/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4751 - acc: 0.5493 - val_loss: 2.3727 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.63342\n",
      "Epoch 88/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4273 - acc: 0.5628 - val_loss: 1.9449 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.63342\n",
      "Epoch 89/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3896 - acc: 0.5862 - val_loss: 1.7312 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.63342\n",
      "Epoch 90/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3998 - acc: 0.5679 - val_loss: 1.7836 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.63342\n",
      "Epoch 91/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4281 - acc: 0.5748 - val_loss: 2.1083 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.63342\n",
      "Epoch 92/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4173 - acc: 0.5613 - val_loss: 1.6336 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.63342\n",
      "Epoch 93/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3887 - acc: 0.5904 - val_loss: 1.7706 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.63342\n",
      "Epoch 94/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4042 - acc: 0.5772 - val_loss: 1.6930 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.63342\n",
      "Epoch 95/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4080 - acc: 0.5799 - val_loss: 1.9109 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.63342\n",
      "Epoch 96/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3989 - acc: 0.5838 - val_loss: 1.8001 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.63342\n",
      "Epoch 97/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4197 - acc: 0.5679 - val_loss: 1.8148 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.63342\n",
      "Epoch 98/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4006 - acc: 0.5688 - val_loss: 1.8318 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.63342\n",
      "Epoch 99/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.3876 - acc: 0.5847 - val_loss: 2.2235 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.63342\n",
      "Epoch 100/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4130 - acc: 0.5947 - val_loss: 1.9869 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.63342\n",
      "Epoch 101/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3758 - acc: 0.5938 - val_loss: 1.7337 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.63342\n",
      "Epoch 102/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3630 - acc: 0.5959 - val_loss: 1.7142 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.63342\n",
      "Epoch 103/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3577 - acc: 0.6010 - val_loss: 2.3889 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.63342\n",
      "Epoch 104/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3719 - acc: 0.5877 - val_loss: 2.0028 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.63342\n",
      "Epoch 105/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3756 - acc: 0.5944 - val_loss: 2.2543 - val_acc: 0.4555\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.63342\n",
      "Epoch 106/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3569 - acc: 0.5998 - val_loss: 1.7596 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.63342\n",
      "Epoch 107/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3370 - acc: 0.5826 - val_loss: 1.5411 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.63342 to 0.63342, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 108/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3126 - acc: 0.6187 - val_loss: 1.6769 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.63342\n",
      "Epoch 109/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3468 - acc: 0.5919 - val_loss: 1.6184 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.63342\n",
      "Epoch 110/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3255 - acc: 0.6076 - val_loss: 2.6025 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.63342\n",
      "Epoch 111/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3442 - acc: 0.5971 - val_loss: 2.0368 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.63342\n",
      "Epoch 112/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3252 - acc: 0.6055 - val_loss: 1.6933 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.63342\n",
      "Epoch 113/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3456 - acc: 0.6010 - val_loss: 1.8006 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.63342\n",
      "Epoch 114/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3243 - acc: 0.6169 - val_loss: 1.5640 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.63342\n",
      "Epoch 115/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3475 - acc: 0.6001 - val_loss: 1.6311 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.63342\n",
      "Epoch 116/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2903 - acc: 0.6190 - val_loss: 2.2205 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.63342\n",
      "Epoch 117/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3082 - acc: 0.6118 - val_loss: 1.5214 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00117: val_acc improved from 0.63342 to 0.64420, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 118/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3326 - acc: 0.5998 - val_loss: 1.7747 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.64420\n",
      "Epoch 119/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.2695 - acc: 0.6262 - val_loss: 1.8849 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.64420\n",
      "Epoch 120/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2977 - acc: 0.6151 - val_loss: 1.5967 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00120: val_acc improved from 0.64420 to 0.65229, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 121/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3139 - acc: 0.6064 - val_loss: 1.7825 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.65229\n",
      "Epoch 122/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2890 - acc: 0.6196 - val_loss: 1.9102 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.65229\n",
      "Epoch 123/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2794 - acc: 0.6343 - val_loss: 1.7520 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.65229\n",
      "Epoch 124/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2883 - acc: 0.6133 - val_loss: 1.7666 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.65229\n",
      "Epoch 125/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.3234 - acc: 0.6013 - val_loss: 1.5330 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.65229\n",
      "Epoch 126/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2843 - acc: 0.6283 - val_loss: 1.8783 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.65229\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2800 - acc: 0.6181 - val_loss: 1.5761 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.65229\n",
      "Epoch 128/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3150 - acc: 0.6142 - val_loss: 1.4272 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.65229 to 0.67655, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 129/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3006 - acc: 0.6253 - val_loss: 2.2490 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.67655\n",
      "Epoch 130/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2788 - acc: 0.6196 - val_loss: 1.5255 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00130: val_acc improved from 0.67655 to 0.67925, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 131/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2993 - acc: 0.6247 - val_loss: 2.0299 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.67925\n",
      "Epoch 132/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.2719 - acc: 0.6232 - val_loss: 1.8330 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.67925\n",
      "Epoch 133/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2984 - acc: 0.6262 - val_loss: 2.3189 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.67925\n",
      "Epoch 134/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2864 - acc: 0.6214 - val_loss: 1.6195 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.67925\n",
      "Epoch 135/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2325 - acc: 0.6487 - val_loss: 1.9097 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.67925\n",
      "Epoch 136/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2592 - acc: 0.6298 - val_loss: 2.1827 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.67925\n",
      "Epoch 137/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2475 - acc: 0.6328 - val_loss: 1.4780 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.67925\n",
      "Epoch 138/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2445 - acc: 0.6352 - val_loss: 1.7472 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.67925\n",
      "Epoch 139/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2161 - acc: 0.6457 - val_loss: 1.4732 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00139: val_acc improved from 0.67925 to 0.69811, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 140/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2352 - acc: 0.6334 - val_loss: 2.6811 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.69811\n",
      "Epoch 141/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2137 - acc: 0.6529 - val_loss: 1.7002 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.69811\n",
      "Epoch 142/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.2374 - acc: 0.6454 - val_loss: 1.4700 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.69811\n",
      "Epoch 143/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2270 - acc: 0.6487 - val_loss: 1.5221 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.69811\n",
      "Epoch 144/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2415 - acc: 0.6340 - val_loss: 1.5359 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.69811\n",
      "Epoch 145/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2164 - acc: 0.6406 - val_loss: 1.4921 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.69811\n",
      "Epoch 146/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2294 - acc: 0.6484 - val_loss: 1.3070 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00146: val_acc improved from 0.69811 to 0.71429, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 147/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2073 - acc: 0.6523 - val_loss: 1.5837 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.71429\n",
      "Epoch 148/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2273 - acc: 0.6328 - val_loss: 1.5851 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.71429\n",
      "Epoch 149/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2226 - acc: 0.6313 - val_loss: 1.8332 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.71429\n",
      "Epoch 150/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2020 - acc: 0.6502 - val_loss: 1.8781 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.71429\n",
      "Epoch 151/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2270 - acc: 0.6421 - val_loss: 1.8926 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.71429\n",
      "Epoch 152/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1937 - acc: 0.6538 - val_loss: 1.4920 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.71429\n",
      "Epoch 153/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2190 - acc: 0.6433 - val_loss: 1.4252 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.71429\n",
      "Epoch 154/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1719 - acc: 0.6734 - val_loss: 1.7099 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.71429\n",
      "Epoch 155/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2116 - acc: 0.6424 - val_loss: 1.5435 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.71429\n",
      "Epoch 156/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2036 - acc: 0.6544 - val_loss: 1.8498 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.71429\n",
      "Epoch 157/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1871 - acc: 0.6544 - val_loss: 1.7439 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.71429\n",
      "Epoch 158/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1813 - acc: 0.6466 - val_loss: 1.6299 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.71429\n",
      "Epoch 159/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1743 - acc: 0.6641 - val_loss: 2.0368 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.71429\n",
      "Epoch 160/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1658 - acc: 0.6532 - val_loss: 1.4182 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.71429\n",
      "Epoch 161/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1776 - acc: 0.6617 - val_loss: 1.2761 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00161: val_acc improved from 0.71429 to 0.73854, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 162/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2001 - acc: 0.6623 - val_loss: 1.6829 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.73854\n",
      "Epoch 163/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1867 - acc: 0.6490 - val_loss: 1.6841 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.73854\n",
      "Epoch 164/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1782 - acc: 0.6608 - val_loss: 1.5158 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.73854\n",
      "Epoch 165/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1578 - acc: 0.6638 - val_loss: 1.5450 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.73854\n",
      "Epoch 166/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1941 - acc: 0.6463 - val_loss: 1.6504 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.73854\n",
      "Epoch 167/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1662 - acc: 0.6725 - val_loss: 1.5405 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.73854\n",
      "Epoch 168/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1472 - acc: 0.6830 - val_loss: 2.1457 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.73854\n",
      "Epoch 169/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1699 - acc: 0.6698 - val_loss: 1.4664 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.73854\n",
      "Epoch 170/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1680 - acc: 0.6731 - val_loss: 1.6261 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.73854\n",
      "Epoch 171/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1360 - acc: 0.6863 - val_loss: 1.4699 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.73854\n",
      "Epoch 172/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.1501 - acc: 0.6728 - val_loss: 1.6077 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.73854\n",
      "Epoch 173/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1486 - acc: 0.6695 - val_loss: 1.5463 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.73854\n",
      "Epoch 174/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1287 - acc: 0.6698 - val_loss: 1.4311 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.73854\n",
      "Epoch 175/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1964 - acc: 0.6641 - val_loss: 1.4057 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.73854\n",
      "Epoch 176/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1522 - acc: 0.6644 - val_loss: 1.4480 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.73854\n",
      "Epoch 177/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1193 - acc: 0.6926 - val_loss: 1.6824 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.73854\n",
      "Epoch 178/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1355 - acc: 0.6590 - val_loss: 1.5295 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.73854\n",
      "Epoch 179/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1325 - acc: 0.6836 - val_loss: 1.6720 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.73854\n",
      "Epoch 180/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1449 - acc: 0.6707 - val_loss: 1.5943 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.73854\n",
      "Epoch 181/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1488 - acc: 0.6674 - val_loss: 1.3472 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.73854\n",
      "Epoch 182/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1423 - acc: 0.6713 - val_loss: 1.7502 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.73854\n",
      "Epoch 183/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1157 - acc: 0.6803 - val_loss: 1.7693 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.73854\n",
      "Epoch 184/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1416 - acc: 0.6713 - val_loss: 1.5800 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.73854\n",
      "Epoch 185/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1131 - acc: 0.6860 - val_loss: 1.4801 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.73854\n",
      "Epoch 186/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0909 - acc: 0.6998 - val_loss: 1.9365 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.73854\n",
      "Epoch 187/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1417 - acc: 0.6797 - val_loss: 1.3623 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.73854\n",
      "Epoch 188/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1330 - acc: 0.6698 - val_loss: 1.9821 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.73854\n",
      "Epoch 189/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1625 - acc: 0.6689 - val_loss: 1.5488 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.73854\n",
      "Epoch 190/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0999 - acc: 0.6911 - val_loss: 1.6300 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.73854\n",
      "Epoch 191/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0861 - acc: 0.7016 - val_loss: 1.4566 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.73854\n",
      "Epoch 192/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0743 - acc: 0.6890 - val_loss: 1.4044 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.73854\n",
      "Epoch 193/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1019 - acc: 0.6965 - val_loss: 1.4930 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.73854\n",
      "Epoch 194/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0903 - acc: 0.6860 - val_loss: 1.6816 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.73854\n",
      "Epoch 195/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1271 - acc: 0.6809 - val_loss: 1.4972 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.73854\n",
      "Epoch 196/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0927 - acc: 0.6842 - val_loss: 1.7708 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.73854\n",
      "Epoch 197/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0794 - acc: 0.6902 - val_loss: 1.3936 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.73854\n",
      "Epoch 198/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1039 - acc: 0.6878 - val_loss: 1.4930 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.73854\n",
      "Epoch 199/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0987 - acc: 0.6875 - val_loss: 1.5455 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.73854\n",
      "Epoch 200/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0753 - acc: 0.6833 - val_loss: 1.5880 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.73854\n",
      "Epoch 201/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1070 - acc: 0.6809 - val_loss: 1.3933 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.73854\n",
      "Epoch 202/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0492 - acc: 0.7019 - val_loss: 1.5277 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.73854\n",
      "Epoch 203/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0581 - acc: 0.7082 - val_loss: 1.6518 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.73854\n",
      "Epoch 204/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0582 - acc: 0.6998 - val_loss: 1.5264 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.73854\n",
      "Epoch 205/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0887 - acc: 0.6974 - val_loss: 1.7066 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.73854\n",
      "Epoch 206/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0690 - acc: 0.6953 - val_loss: 1.6554 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.73854\n",
      "Epoch 207/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0809 - acc: 0.7010 - val_loss: 1.3386 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.73854\n",
      "Epoch 208/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1010 - acc: 0.6977 - val_loss: 1.3980 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.73854\n",
      "Epoch 209/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0605 - acc: 0.6932 - val_loss: 1.3727 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.73854\n",
      "Epoch 210/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0765 - acc: 0.6839 - val_loss: 1.3705 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.73854\n",
      "Epoch 211/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0864 - acc: 0.6884 - val_loss: 1.5457 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.73854\n",
      "Epoch 212/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0625 - acc: 0.7028 - val_loss: 1.5837 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.73854\n",
      "Epoch 213/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0982 - acc: 0.6767 - val_loss: 1.8036 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.73854\n",
      "Epoch 214/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0587 - acc: 0.6944 - val_loss: 1.3289 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.73854\n",
      "Epoch 215/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0285 - acc: 0.7097 - val_loss: 1.6219 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.73854\n",
      "Epoch 216/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0809 - acc: 0.6845 - val_loss: 1.4737 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.73854\n",
      "Epoch 217/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0758 - acc: 0.6974 - val_loss: 1.7397 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.73854\n",
      "Epoch 218/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0728 - acc: 0.6923 - val_loss: 1.5653 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.73854\n",
      "Epoch 219/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0615 - acc: 0.6947 - val_loss: 1.2256 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.73854\n",
      "Epoch 220/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0452 - acc: 0.6995 - val_loss: 1.2453 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.73854\n",
      "Epoch 221/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0667 - acc: 0.6989 - val_loss: 1.3980 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.73854\n",
      "Epoch 222/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0557 - acc: 0.7034 - val_loss: 1.3150 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.73854\n",
      "Epoch 223/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0564 - acc: 0.7094 - val_loss: 1.1846 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00223: val_acc improved from 0.73854 to 0.75472, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 224/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0289 - acc: 0.7067 - val_loss: 1.6076 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.75472\n",
      "Epoch 225/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0418 - acc: 0.7025 - val_loss: 1.7365 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.75472\n",
      "Epoch 226/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0578 - acc: 0.7022 - val_loss: 2.0155 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.75472\n",
      "Epoch 227/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0297 - acc: 0.7127 - val_loss: 1.6915 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.75472\n",
      "Epoch 228/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0353 - acc: 0.7106 - val_loss: 1.5815 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.75472\n",
      "Epoch 229/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0576 - acc: 0.7085 - val_loss: 1.6210 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.75472\n",
      "Epoch 230/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0415 - acc: 0.7007 - val_loss: 1.8627 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.75472\n",
      "Epoch 231/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0569 - acc: 0.7070 - val_loss: 1.6277 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.75472\n",
      "Epoch 232/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0104 - acc: 0.7248 - val_loss: 1.6340 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.75472\n",
      "Epoch 233/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0355 - acc: 0.7151 - val_loss: 1.4085 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.75472\n",
      "Epoch 234/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0559 - acc: 0.6923 - val_loss: 1.8373 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.75472\n",
      "Epoch 235/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0562 - acc: 0.7028 - val_loss: 1.4631 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.75472\n",
      "Epoch 236/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0405 - acc: 0.7172 - val_loss: 1.3639 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.75472\n",
      "Epoch 237/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0442 - acc: 0.7028 - val_loss: 1.6474 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.75472\n",
      "Epoch 238/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0226 - acc: 0.7233 - val_loss: 1.2074 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.75472\n",
      "Epoch 239/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0584 - acc: 0.7043 - val_loss: 2.1377 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.75472\n",
      "Epoch 240/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0060 - acc: 0.7269 - val_loss: 1.3005 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.75472\n",
      "Epoch 241/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0033 - acc: 0.7181 - val_loss: 1.3599 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.75472\n",
      "Epoch 242/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0363 - acc: 0.7100 - val_loss: 1.2934 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.75472\n",
      "Epoch 243/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0279 - acc: 0.7230 - val_loss: 1.5538 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.75472\n",
      "Epoch 244/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9943 - acc: 0.7139 - val_loss: 1.3820 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.75472\n",
      "Epoch 245/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0102 - acc: 0.7257 - val_loss: 1.3939 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.75472\n",
      "Epoch 246/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9960 - acc: 0.7145 - val_loss: 1.3716 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.75472\n",
      "Epoch 247/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0222 - acc: 0.7215 - val_loss: 1.5226 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.75472\n",
      "Epoch 248/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9983 - acc: 0.7251 - val_loss: 1.4802 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.75472\n",
      "Epoch 249/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0333 - acc: 0.7055 - val_loss: 1.5901 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.75472\n",
      "Epoch 250/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0179 - acc: 0.7157 - val_loss: 1.7575 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.75472\n",
      "Epoch 251/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9906 - acc: 0.7124 - val_loss: 1.3221 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.75472\n",
      "Epoch 252/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9941 - acc: 0.7368 - val_loss: 1.4486 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.75472\n",
      "Epoch 253/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9833 - acc: 0.7353 - val_loss: 1.4627 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.75472\n",
      "Epoch 254/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0107 - acc: 0.7191 - val_loss: 1.7021 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.75472\n",
      "Epoch 255/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0227 - acc: 0.7296 - val_loss: 1.5135 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.75472\n",
      "Epoch 256/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9625 - acc: 0.7350 - val_loss: 1.4742 - val_acc: 0.6873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00256: val_acc did not improve from 0.75472\n",
      "Epoch 257/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9667 - acc: 0.7275 - val_loss: 1.6442 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.75472\n",
      "Epoch 258/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 1.9630 - acc: 0.7254 - val_loss: 1.5204 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.75472\n",
      "Epoch 259/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9764 - acc: 0.7287 - val_loss: 1.3257 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.75472\n",
      "Epoch 260/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0032 - acc: 0.7130 - val_loss: 1.5825 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.75472\n",
      "Epoch 261/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9750 - acc: 0.7290 - val_loss: 1.5924 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.75472\n",
      "Epoch 262/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0059 - acc: 0.7206 - val_loss: 1.4657 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.75472\n",
      "Epoch 263/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9981 - acc: 0.7218 - val_loss: 1.5934 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.75472\n",
      "Epoch 264/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9919 - acc: 0.7347 - val_loss: 1.9532 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.75472\n",
      "Epoch 265/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9876 - acc: 0.7236 - val_loss: 1.3770 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.75472\n",
      "Epoch 266/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9798 - acc: 0.7314 - val_loss: 1.7180 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.75472\n",
      "Epoch 267/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9871 - acc: 0.7269 - val_loss: 1.4601 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.75472\n",
      "Epoch 268/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9801 - acc: 0.7197 - val_loss: 1.3920 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.75472\n",
      "Epoch 269/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9879 - acc: 0.7266 - val_loss: 1.6073 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.75472\n",
      "Epoch 270/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9960 - acc: 0.7221 - val_loss: 1.5377 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.75472\n",
      "Epoch 271/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9789 - acc: 0.7338 - val_loss: 1.6030 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.75472\n",
      "Epoch 272/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9529 - acc: 0.7410 - val_loss: 1.5359 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.75472\n",
      "Epoch 273/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 1.9685 - acc: 0.7269 - val_loss: 1.3707 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.75472\n",
      "Epoch 274/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9854 - acc: 0.7305 - val_loss: 1.4053 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.75472\n",
      "Epoch 275/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9833 - acc: 0.7311 - val_loss: 1.5784 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.75472\n",
      "Epoch 276/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9735 - acc: 0.7296 - val_loss: 1.3263 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.75472\n",
      "Epoch 277/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9634 - acc: 0.7272 - val_loss: 1.2789 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.75472\n",
      "Epoch 278/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9360 - acc: 0.7473 - val_loss: 1.5227 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.75472\n",
      "Epoch 279/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9588 - acc: 0.7371 - val_loss: 1.9124 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.75472\n",
      "Epoch 280/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9537 - acc: 0.7257 - val_loss: 1.8593 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.75472\n",
      "Epoch 281/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9641 - acc: 0.7305 - val_loss: 1.4890 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.75472\n",
      "Epoch 282/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9563 - acc: 0.7254 - val_loss: 1.6478 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.75472\n",
      "Epoch 283/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9715 - acc: 0.7317 - val_loss: 1.3524 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.75472\n",
      "Epoch 284/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9571 - acc: 0.7314 - val_loss: 1.4591 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.75472\n",
      "Epoch 285/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9734 - acc: 0.7302 - val_loss: 1.1844 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.75472\n",
      "Epoch 286/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9525 - acc: 0.7431 - val_loss: 1.6666 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.75472\n",
      "Epoch 287/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9739 - acc: 0.7287 - val_loss: 1.3025 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.75472\n",
      "Epoch 288/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9474 - acc: 0.7368 - val_loss: 1.2204 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.75472\n",
      "Epoch 289/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9717 - acc: 0.7293 - val_loss: 1.5128 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.75472\n",
      "Epoch 290/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9448 - acc: 0.7380 - val_loss: 1.3667 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.75472\n",
      "Epoch 291/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9392 - acc: 0.7401 - val_loss: 1.2777 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.75472\n",
      "Epoch 292/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9497 - acc: 0.7353 - val_loss: 1.3100 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.75472\n",
      "Epoch 293/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9511 - acc: 0.7494 - val_loss: 1.7024 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.75472\n",
      "Epoch 294/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9532 - acc: 0.7359 - val_loss: 1.5818 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.75472\n",
      "Epoch 295/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9413 - acc: 0.7428 - val_loss: 1.2527 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.75472\n",
      "Epoch 296/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9148 - acc: 0.7473 - val_loss: 1.3989 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.75472\n",
      "Epoch 297/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9377 - acc: 0.7413 - val_loss: 1.4955 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.75472\n",
      "Epoch 298/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9257 - acc: 0.7491 - val_loss: 1.3775 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.75472\n",
      "Epoch 299/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9525 - acc: 0.7467 - val_loss: 1.4076 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.75472\n",
      "Epoch 300/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9121 - acc: 0.7455 - val_loss: 1.8934 - val_acc: 0.5445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_acc did not improve from 0.75472\n",
      "Epoch 301/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9486 - acc: 0.7476 - val_loss: 1.4373 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.75472\n",
      "Epoch 302/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9343 - acc: 0.7437 - val_loss: 1.2516 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.75472\n",
      "Epoch 303/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9653 - acc: 0.7251 - val_loss: 1.2921 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.75472\n",
      "Epoch 304/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9360 - acc: 0.7365 - val_loss: 1.4472 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.75472\n",
      "Epoch 305/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9468 - acc: 0.7299 - val_loss: 1.1882 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.75472\n",
      "Epoch 306/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8945 - acc: 0.7587 - val_loss: 1.2375 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.75472\n",
      "Epoch 307/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9359 - acc: 0.7449 - val_loss: 1.4035 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.75472\n",
      "Epoch 308/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9190 - acc: 0.7458 - val_loss: 1.4114 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.75472\n",
      "Epoch 309/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9059 - acc: 0.7521 - val_loss: 1.3555 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.75472\n",
      "Epoch 310/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9384 - acc: 0.7413 - val_loss: 1.5425 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.75472\n",
      "Epoch 311/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9146 - acc: 0.7512 - val_loss: 1.1305 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00311: val_acc improved from 0.75472 to 0.78437, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 312/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9340 - acc: 0.7395 - val_loss: 1.1232 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.78437\n",
      "Epoch 313/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9383 - acc: 0.7410 - val_loss: 1.3640 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.78437\n",
      "Epoch 314/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9364 - acc: 0.7305 - val_loss: 1.4005 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.78437\n",
      "Epoch 315/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9400 - acc: 0.7323 - val_loss: 1.2315 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.78437\n",
      "Epoch 316/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9185 - acc: 0.7446 - val_loss: 1.4016 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.78437\n",
      "Epoch 317/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9419 - acc: 0.7416 - val_loss: 1.2714 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.78437\n",
      "Epoch 318/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9275 - acc: 0.7347 - val_loss: 1.5025 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.78437\n",
      "Epoch 319/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8924 - acc: 0.7578 - val_loss: 1.3672 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.78437\n",
      "Epoch 320/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9215 - acc: 0.7485 - val_loss: 1.5658 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.78437\n",
      "Epoch 321/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9232 - acc: 0.7473 - val_loss: 1.2987 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.78437\n",
      "Epoch 322/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9306 - acc: 0.7452 - val_loss: 1.5355 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.78437\n",
      "Epoch 323/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9130 - acc: 0.7578 - val_loss: 1.4953 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.78437\n",
      "Epoch 324/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9056 - acc: 0.7491 - val_loss: 1.3570 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.78437\n",
      "Epoch 325/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8896 - acc: 0.7515 - val_loss: 1.3942 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.78437\n",
      "Epoch 326/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9399 - acc: 0.7425 - val_loss: 1.3895 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.78437\n",
      "Epoch 327/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9140 - acc: 0.7494 - val_loss: 1.8051 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.78437\n",
      "Epoch 328/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9032 - acc: 0.7542 - val_loss: 1.2058 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.78437\n",
      "Epoch 329/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9160 - acc: 0.7491 - val_loss: 1.2455 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.78437\n",
      "Epoch 330/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8971 - acc: 0.7518 - val_loss: 1.4719 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.78437\n",
      "Epoch 331/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9251 - acc: 0.7422 - val_loss: 1.2728 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.78437\n",
      "Epoch 332/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9059 - acc: 0.7479 - val_loss: 1.3279 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.78437\n",
      "Epoch 333/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9113 - acc: 0.7539 - val_loss: 1.2210 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.78437\n",
      "Epoch 334/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9021 - acc: 0.7413 - val_loss: 1.3885 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.78437\n",
      "Epoch 335/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9120 - acc: 0.7554 - val_loss: 1.6344 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.78437\n",
      "Epoch 336/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9075 - acc: 0.7476 - val_loss: 1.6357 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.78437\n",
      "Epoch 337/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8894 - acc: 0.7584 - val_loss: 1.1828 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.78437\n",
      "Epoch 338/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9199 - acc: 0.7587 - val_loss: 1.3664 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.78437\n",
      "Epoch 339/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8869 - acc: 0.7632 - val_loss: 2.0296 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.78437\n",
      "Epoch 340/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9338 - acc: 0.7479 - val_loss: 1.3554 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.78437\n",
      "Epoch 341/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9092 - acc: 0.7491 - val_loss: 1.2951 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.78437\n",
      "Epoch 342/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9084 - acc: 0.7437 - val_loss: 1.5286 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.78437\n",
      "Epoch 343/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8889 - acc: 0.7566 - val_loss: 1.7760 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.78437\n",
      "Epoch 344/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9225 - acc: 0.7527 - val_loss: 1.5387 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.78437\n",
      "Epoch 345/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8721 - acc: 0.7635 - val_loss: 1.1735 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.78437\n",
      "Epoch 346/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9062 - acc: 0.7527 - val_loss: 1.5853 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.78437\n",
      "Epoch 347/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9140 - acc: 0.7533 - val_loss: 1.5410 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.78437\n",
      "Epoch 348/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8747 - acc: 0.7566 - val_loss: 1.4062 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.78437\n",
      "Epoch 349/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8860 - acc: 0.7575 - val_loss: 1.3570 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.78437\n",
      "Epoch 350/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8975 - acc: 0.7461 - val_loss: 1.2302 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.78437\n",
      "Epoch 351/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8896 - acc: 0.7605 - val_loss: 1.4106 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.78437\n",
      "Epoch 352/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8951 - acc: 0.7527 - val_loss: 1.8326 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.78437\n",
      "Epoch 353/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8704 - acc: 0.7599 - val_loss: 1.2223 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.78437\n",
      "Epoch 354/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8483 - acc: 0.7758 - val_loss: 1.4831 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.78437\n",
      "Epoch 355/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8671 - acc: 0.7548 - val_loss: 1.1950 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.78437\n",
      "Epoch 356/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8632 - acc: 0.7620 - val_loss: 1.7094 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.78437\n",
      "Epoch 357/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9128 - acc: 0.7557 - val_loss: 1.2252 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.78437\n",
      "Epoch 358/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8998 - acc: 0.7518 - val_loss: 1.7532 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.78437\n",
      "Epoch 359/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8982 - acc: 0.7443 - val_loss: 1.2971 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.78437\n",
      "Epoch 360/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8651 - acc: 0.7608 - val_loss: 1.3090 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.78437\n",
      "Epoch 361/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8786 - acc: 0.7608 - val_loss: 1.2693 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.78437\n",
      "Epoch 362/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8981 - acc: 0.7641 - val_loss: 1.4752 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.78437\n",
      "Epoch 363/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8608 - acc: 0.7671 - val_loss: 1.3857 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.78437\n",
      "Epoch 364/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8979 - acc: 0.7596 - val_loss: 1.5792 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.78437\n",
      "Epoch 365/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8698 - acc: 0.7581 - val_loss: 1.4988 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.78437\n",
      "Epoch 366/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8405 - acc: 0.7668 - val_loss: 1.8383 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.78437\n",
      "Epoch 367/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8727 - acc: 0.7659 - val_loss: 1.3737 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.78437\n",
      "Epoch 368/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8766 - acc: 0.7614 - val_loss: 1.3314 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.78437\n",
      "Epoch 369/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8755 - acc: 0.7662 - val_loss: 1.3059 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.78437\n",
      "Epoch 370/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8594 - acc: 0.7590 - val_loss: 1.2743 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.78437\n",
      "Epoch 371/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9000 - acc: 0.7431 - val_loss: 1.4582 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.78437\n",
      "Epoch 372/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8837 - acc: 0.7587 - val_loss: 1.8025 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.78437\n",
      "Epoch 373/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8835 - acc: 0.7638 - val_loss: 1.9964 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.78437\n",
      "Epoch 374/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8763 - acc: 0.7623 - val_loss: 1.2459 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.78437\n",
      "Epoch 375/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8801 - acc: 0.7599 - val_loss: 1.6808 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.78437\n",
      "Epoch 376/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8806 - acc: 0.7575 - val_loss: 1.2495 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.78437\n",
      "Epoch 377/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8515 - acc: 0.7608 - val_loss: 1.5058 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.78437\n",
      "Epoch 378/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8747 - acc: 0.7659 - val_loss: 1.9892 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.78437\n",
      "Epoch 379/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8737 - acc: 0.7605 - val_loss: 1.5133 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.78437\n",
      "Epoch 380/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8694 - acc: 0.7623 - val_loss: 1.3081 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.78437\n",
      "Epoch 381/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8503 - acc: 0.7698 - val_loss: 1.5720 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.78437\n",
      "Epoch 382/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8476 - acc: 0.7623 - val_loss: 1.4492 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.78437\n",
      "Epoch 383/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8687 - acc: 0.7575 - val_loss: 1.4458 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.78437\n",
      "Epoch 384/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8628 - acc: 0.7569 - val_loss: 1.1993 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.78437\n",
      "Epoch 385/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8469 - acc: 0.7644 - val_loss: 1.4546 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.78437\n",
      "Epoch 386/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8835 - acc: 0.7551 - val_loss: 1.7115 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.78437\n",
      "Epoch 387/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8621 - acc: 0.7599 - val_loss: 1.3058 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.78437\n",
      "Epoch 388/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8586 - acc: 0.7602 - val_loss: 1.7172 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.78437\n",
      "Epoch 389/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8674 - acc: 0.7647 - val_loss: 1.6238 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.78437\n",
      "Epoch 390/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9079 - acc: 0.7476 - val_loss: 1.3803 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.78437\n",
      "Epoch 391/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8765 - acc: 0.7545 - val_loss: 1.2727 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.78437\n",
      "Epoch 392/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8502 - acc: 0.7665 - val_loss: 1.4175 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.78437\n",
      "Epoch 393/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8420 - acc: 0.7656 - val_loss: 1.4988 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.78437\n",
      "Epoch 394/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8388 - acc: 0.7773 - val_loss: 1.2329 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.78437\n",
      "Epoch 395/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8460 - acc: 0.7686 - val_loss: 1.2715 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.78437\n",
      "Epoch 396/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8373 - acc: 0.7822 - val_loss: 1.4598 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.78437\n",
      "Epoch 397/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8607 - acc: 0.7608 - val_loss: 1.3958 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.78437\n",
      "Epoch 398/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8455 - acc: 0.7764 - val_loss: 1.4083 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.78437\n",
      "Epoch 399/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8611 - acc: 0.7728 - val_loss: 1.7200 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.78437\n",
      "Epoch 400/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8609 - acc: 0.7605 - val_loss: 1.4832 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.78437\n",
      "Epoch 401/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8586 - acc: 0.7686 - val_loss: 1.2815 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.78437\n",
      "Epoch 402/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8374 - acc: 0.7692 - val_loss: 1.1837 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.78437\n",
      "Epoch 403/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8422 - acc: 0.7689 - val_loss: 1.2669 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.78437\n",
      "Epoch 404/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8665 - acc: 0.7545 - val_loss: 1.3666 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.78437\n",
      "Epoch 405/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8783 - acc: 0.7635 - val_loss: 1.5081 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.78437\n",
      "Epoch 406/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8508 - acc: 0.7650 - val_loss: 1.3435 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.78437\n",
      "Epoch 407/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8218 - acc: 0.7635 - val_loss: 1.2239 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.78437\n",
      "Epoch 408/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8493 - acc: 0.7704 - val_loss: 1.6202 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.78437\n",
      "Epoch 409/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8524 - acc: 0.7650 - val_loss: 1.4229 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.78437\n",
      "Epoch 410/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8487 - acc: 0.7608 - val_loss: 1.8951 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.78437\n",
      "Epoch 411/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8423 - acc: 0.7686 - val_loss: 1.6097 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.78437\n",
      "Epoch 412/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8572 - acc: 0.7581 - val_loss: 1.3260 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.78437\n",
      "Epoch 00412: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_6===\n",
      "semi loading: model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 5/3000\n",
      "106/106 [==============================] - 18s 168ms/step - loss: 1.9593 - acc: 0.7164 - val_loss: 0.9806 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.82210, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 6/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8859 - acc: 0.7379 - val_loss: 0.9923 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82210\n",
      "Epoch 7/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8495 - acc: 0.7500 - val_loss: 1.0046 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82210\n",
      "Epoch 8/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8313 - acc: 0.7577 - val_loss: 0.9608 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82210 to 0.83288, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 9/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8170 - acc: 0.7683 - val_loss: 0.9443 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83288\n",
      "Epoch 10/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8022 - acc: 0.7656 - val_loss: 0.9749 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83288\n",
      "Epoch 11/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7952 - acc: 0.7698 - val_loss: 0.9576 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83288\n",
      "Epoch 12/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8134 - acc: 0.7612 - val_loss: 0.9263 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83288 to 0.83827, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 13/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7978 - acc: 0.7633 - val_loss: 0.9556 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83827\n",
      "Epoch 14/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7776 - acc: 0.7765 - val_loss: 0.9807 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83827\n",
      "Epoch 15/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7659 - acc: 0.7780 - val_loss: 0.9202 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83827\n",
      "Epoch 16/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7892 - acc: 0.7677 - val_loss: 0.9262 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.83827 to 0.84367, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 17/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7712 - acc: 0.7683 - val_loss: 0.9003 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84367\n",
      "Epoch 18/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7413 - acc: 0.7807 - val_loss: 0.9254 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84367\n",
      "Epoch 19/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7613 - acc: 0.7706 - val_loss: 0.9767 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84367\n",
      "Epoch 20/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7410 - acc: 0.7916 - val_loss: 0.9688 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84367\n",
      "Epoch 21/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7343 - acc: 0.7910 - val_loss: 0.9361 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84367\n",
      "Epoch 22/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7389 - acc: 0.7863 - val_loss: 0.9688 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84367\n",
      "Epoch 23/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7437 - acc: 0.7774 - val_loss: 0.9533 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84367\n",
      "Epoch 24/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7413 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84367\n",
      "Epoch 25/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7035 - acc: 0.8078 - val_loss: 0.9418 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.84367 to 0.84636, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 26/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6995 - acc: 0.7913 - val_loss: 0.9274 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84636\n",
      "Epoch 27/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7336 - acc: 0.7780 - val_loss: 0.9166 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84636\n",
      "Epoch 28/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6886 - acc: 0.7877 - val_loss: 0.9178 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84636\n",
      "Epoch 29/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7124 - acc: 0.7824 - val_loss: 0.9044 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84636\n",
      "Epoch 30/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7278 - acc: 0.7869 - val_loss: 0.8978 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84636\n",
      "Epoch 31/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7193 - acc: 0.7851 - val_loss: 0.9155 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84636\n",
      "Epoch 32/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7097 - acc: 0.7880 - val_loss: 0.9354 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84636\n",
      "Epoch 33/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6808 - acc: 0.7901 - val_loss: 0.9159 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84636\n",
      "Epoch 34/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7093 - acc: 0.7883 - val_loss: 0.9447 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84636\n",
      "Epoch 35/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7078 - acc: 0.7904 - val_loss: 0.9341 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84636\n",
      "Epoch 36/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6973 - acc: 0.7922 - val_loss: 0.9159 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84636\n",
      "Epoch 37/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6938 - acc: 0.7919 - val_loss: 0.9144 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84636\n",
      "Epoch 38/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6969 - acc: 0.7963 - val_loss: 0.9131 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84636\n",
      "Epoch 39/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6910 - acc: 0.8010 - val_loss: 0.8801 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84636\n",
      "Epoch 40/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6716 - acc: 0.7880 - val_loss: 0.9151 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84636\n",
      "Epoch 41/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6695 - acc: 0.8034 - val_loss: 0.9094 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84636\n",
      "Epoch 42/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6599 - acc: 0.8010 - val_loss: 0.8965 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84636\n",
      "Epoch 43/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6850 - acc: 0.7948 - val_loss: 0.9399 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84636\n",
      "Epoch 44/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6741 - acc: 0.7889 - val_loss: 0.8536 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84636\n",
      "Epoch 45/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6892 - acc: 0.7983 - val_loss: 0.9524 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84636\n",
      "Epoch 46/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6933 - acc: 0.7889 - val_loss: 0.8587 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84636\n",
      "Epoch 47/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6670 - acc: 0.7966 - val_loss: 0.9131 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84636\n",
      "Epoch 48/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6649 - acc: 0.7963 - val_loss: 0.9215 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84636\n",
      "Epoch 49/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6694 - acc: 0.7942 - val_loss: 0.9298 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84636\n",
      "Epoch 50/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6605 - acc: 0.7992 - val_loss: 0.9141 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84636\n",
      "Epoch 51/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6542 - acc: 0.8072 - val_loss: 0.8789 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84636\n",
      "Epoch 52/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6906 - acc: 0.7871 - val_loss: 0.9240 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84636\n",
      "Epoch 53/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6667 - acc: 0.7983 - val_loss: 0.9065 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84636\n",
      "Epoch 54/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6600 - acc: 0.7975 - val_loss: 0.8703 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84636\n",
      "Epoch 55/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6450 - acc: 0.8104 - val_loss: 0.9047 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84636\n",
      "Epoch 56/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.6636 - acc: 0.7975 - val_loss: 0.9042 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84636\n",
      "Epoch 57/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6469 - acc: 0.7998 - val_loss: 0.8972 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84636\n",
      "Epoch 58/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6569 - acc: 0.7966 - val_loss: 0.9071 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84636\n",
      "Epoch 59/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6473 - acc: 0.7948 - val_loss: 0.8987 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84636\n",
      "Epoch 60/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6375 - acc: 0.8019 - val_loss: 0.9196 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84636\n",
      "Epoch 61/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6349 - acc: 0.8066 - val_loss: 0.9291 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84636\n",
      "Epoch 62/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6530 - acc: 0.8075 - val_loss: 0.9018 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84636\n",
      "Epoch 63/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6502 - acc: 0.8022 - val_loss: 0.9213 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84636\n",
      "Epoch 64/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6244 - acc: 0.8131 - val_loss: 0.9009 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84636\n",
      "Epoch 65/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6425 - acc: 0.7945 - val_loss: 0.8687 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84636\n",
      "Epoch 66/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6607 - acc: 0.8010 - val_loss: 0.8674 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84636\n",
      "Epoch 67/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6170 - acc: 0.8160 - val_loss: 0.8571 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84636\n",
      "Epoch 68/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6367 - acc: 0.8037 - val_loss: 0.8915 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84636\n",
      "Epoch 69/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6451 - acc: 0.7992 - val_loss: 0.8814 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84636\n",
      "Epoch 70/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6532 - acc: 0.8087 - val_loss: 0.9165 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84636\n",
      "Epoch 71/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6078 - acc: 0.7922 - val_loss: 0.8813 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84636\n",
      "Epoch 72/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6317 - acc: 0.8119 - val_loss: 0.9038 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84636\n",
      "Epoch 73/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6041 - acc: 0.8287 - val_loss: 0.8455 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84636\n",
      "Epoch 74/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6099 - acc: 0.7995 - val_loss: 0.8765 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84636\n",
      "Epoch 75/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6183 - acc: 0.8116 - val_loss: 0.8918 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84636\n",
      "Epoch 76/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6301 - acc: 0.8025 - val_loss: 0.8724 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84636\n",
      "Epoch 77/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6090 - acc: 0.8045 - val_loss: 0.9156 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84636\n",
      "Epoch 78/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6225 - acc: 0.7957 - val_loss: 0.8668 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84636\n",
      "Epoch 79/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6098 - acc: 0.8048 - val_loss: 0.9584 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84636\n",
      "Epoch 80/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6313 - acc: 0.8081 - val_loss: 0.9107 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84636\n",
      "Epoch 81/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6200 - acc: 0.8122 - val_loss: 0.8731 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84636\n",
      "Epoch 82/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6179 - acc: 0.8069 - val_loss: 0.8716 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.84636\n",
      "Epoch 83/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6028 - acc: 0.8178 - val_loss: 0.8547 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.84636\n",
      "Epoch 84/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6059 - acc: 0.8045 - val_loss: 0.8380 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.84636\n",
      "Epoch 85/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5728 - acc: 0.8249 - val_loss: 0.8538 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.84636\n",
      "Epoch 86/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5942 - acc: 0.8128 - val_loss: 0.8579 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.84636\n",
      "Epoch 87/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6193 - acc: 0.8060 - val_loss: 0.8540 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.84636\n",
      "Epoch 88/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6233 - acc: 0.8090 - val_loss: 0.8986 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.84636\n",
      "Epoch 89/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6174 - acc: 0.8154 - val_loss: 0.8501 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.84636\n",
      "Epoch 90/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6007 - acc: 0.8042 - val_loss: 0.8355 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.84636 to 0.84906, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 91/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6037 - acc: 0.8154 - val_loss: 0.9272 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.84906\n",
      "Epoch 92/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5829 - acc: 0.8267 - val_loss: 0.8919 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.84906\n",
      "Epoch 93/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6124 - acc: 0.8010 - val_loss: 0.8651 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.84906\n",
      "Epoch 94/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6071 - acc: 0.8149 - val_loss: 0.9222 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.84906\n",
      "Epoch 95/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5799 - acc: 0.8308 - val_loss: 0.8773 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.84906\n",
      "Epoch 96/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5956 - acc: 0.8087 - val_loss: 0.8592 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.84906\n",
      "Epoch 97/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6134 - acc: 0.8081 - val_loss: 0.8502 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.84906\n",
      "Epoch 98/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5870 - acc: 0.8149 - val_loss: 0.8212 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.84906\n",
      "Epoch 99/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5699 - acc: 0.8275 - val_loss: 0.8412 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.84906\n",
      "Epoch 100/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5922 - acc: 0.8116 - val_loss: 0.8228 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.84906\n",
      "Epoch 101/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5854 - acc: 0.8160 - val_loss: 0.8449 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.84906\n",
      "Epoch 102/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5408 - acc: 0.8290 - val_loss: 0.8311 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.84906 to 0.85175, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 103/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5800 - acc: 0.8146 - val_loss: 0.8426 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85175\n",
      "Epoch 104/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5441 - acc: 0.8258 - val_loss: 0.8484 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85175\n",
      "Epoch 105/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5730 - acc: 0.8149 - val_loss: 0.8670 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85175\n",
      "Epoch 106/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5628 - acc: 0.8184 - val_loss: 0.8248 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85175\n",
      "Epoch 107/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5826 - acc: 0.8175 - val_loss: 0.8329 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85175\n",
      "Epoch 108/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5658 - acc: 0.8208 - val_loss: 0.8368 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85175\n",
      "Epoch 109/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5769 - acc: 0.8160 - val_loss: 0.8702 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85175\n",
      "Epoch 110/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5305 - acc: 0.8267 - val_loss: 0.8167 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00110: val_acc improved from 0.85175 to 0.85445, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 111/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5356 - acc: 0.8358 - val_loss: 0.8319 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.85445\n",
      "Epoch 112/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5711 - acc: 0.8240 - val_loss: 0.8106 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.85445\n",
      "Epoch 113/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5614 - acc: 0.8243 - val_loss: 0.8242 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.85445\n",
      "Epoch 114/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5628 - acc: 0.8231 - val_loss: 0.8303 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.85445\n",
      "Epoch 115/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5381 - acc: 0.8296 - val_loss: 0.8368 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.85445\n",
      "Epoch 116/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5524 - acc: 0.8222 - val_loss: 0.8392 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.85445\n",
      "Epoch 117/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5478 - acc: 0.8125 - val_loss: 0.8575 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.85445\n",
      "Epoch 118/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5753 - acc: 0.8181 - val_loss: 0.8268 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.85445\n",
      "Epoch 119/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5540 - acc: 0.8308 - val_loss: 0.8243 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.85445\n",
      "Epoch 120/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5441 - acc: 0.8284 - val_loss: 0.8498 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.85445\n",
      "Epoch 121/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5264 - acc: 0.8272 - val_loss: 0.8483 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.85445\n",
      "Epoch 122/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5736 - acc: 0.8193 - val_loss: 0.8368 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.85445\n",
      "Epoch 123/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5218 - acc: 0.8387 - val_loss: 0.8210 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.85445\n",
      "Epoch 124/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5470 - acc: 0.8196 - val_loss: 0.8299 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.85445\n",
      "Epoch 125/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5598 - acc: 0.8293 - val_loss: 0.8301 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.85445\n",
      "Epoch 126/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5428 - acc: 0.8287 - val_loss: 0.8302 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.85445\n",
      "Epoch 127/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5435 - acc: 0.8149 - val_loss: 0.8135 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.85445\n",
      "Epoch 128/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5488 - acc: 0.8157 - val_loss: 0.8296 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.85445\n",
      "Epoch 129/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5319 - acc: 0.8278 - val_loss: 0.8220 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.85445\n",
      "Epoch 130/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5199 - acc: 0.8314 - val_loss: 0.8433 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.85445\n",
      "Epoch 131/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5432 - acc: 0.8166 - val_loss: 0.8130 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.85445\n",
      "Epoch 132/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5291 - acc: 0.8325 - val_loss: 0.8269 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.85445\n",
      "Epoch 133/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5428 - acc: 0.8258 - val_loss: 0.8149 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.85445\n",
      "Epoch 134/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5313 - acc: 0.8337 - val_loss: 0.8191 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.85445\n",
      "Epoch 135/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5497 - acc: 0.8246 - val_loss: 0.8211 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.85445\n",
      "Epoch 136/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5396 - acc: 0.8323 - val_loss: 0.8244 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.85445\n",
      "Epoch 137/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5347 - acc: 0.8373 - val_loss: 0.8214 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.85445\n",
      "Epoch 138/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5533 - acc: 0.8331 - val_loss: 0.8387 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.85445\n",
      "Epoch 139/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5176 - acc: 0.8405 - val_loss: 0.8283 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.85445\n",
      "Epoch 140/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5462 - acc: 0.8278 - val_loss: 0.8142 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.85445\n",
      "Epoch 141/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5518 - acc: 0.8178 - val_loss: 0.8524 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.85445\n",
      "Epoch 142/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5262 - acc: 0.8384 - val_loss: 0.8303 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.85445\n",
      "Epoch 143/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5249 - acc: 0.8373 - val_loss: 0.8151 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.85445\n",
      "Epoch 144/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5280 - acc: 0.8278 - val_loss: 0.8338 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.85445\n",
      "Epoch 145/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5264 - acc: 0.8290 - val_loss: 0.8273 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.85445\n",
      "Epoch 146/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5120 - acc: 0.8343 - val_loss: 0.8261 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.85445\n",
      "Epoch 147/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5642 - acc: 0.8210 - val_loss: 0.8222 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.85445\n",
      "Epoch 148/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5203 - acc: 0.8343 - val_loss: 0.8419 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.85445\n",
      "Epoch 149/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5164 - acc: 0.8370 - val_loss: 0.8394 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.85445\n",
      "Epoch 150/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5204 - acc: 0.8264 - val_loss: 0.8287 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.85445\n",
      "Epoch 151/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5269 - acc: 0.8323 - val_loss: 0.8464 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.85445\n",
      "Epoch 152/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5109 - acc: 0.8399 - val_loss: 0.8394 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.85445\n",
      "Epoch 153/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5272 - acc: 0.8290 - val_loss: 0.8336 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.85445\n",
      "Epoch 154/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5168 - acc: 0.8272 - val_loss: 0.8227 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.85445\n",
      "Epoch 155/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5262 - acc: 0.8272 - val_loss: 0.8353 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.85445\n",
      "Epoch 156/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5380 - acc: 0.8317 - val_loss: 0.8463 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.85445\n",
      "Epoch 157/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5186 - acc: 0.8284 - val_loss: 0.8364 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.85445\n",
      "Epoch 158/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5436 - acc: 0.8246 - val_loss: 0.8396 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.85445\n",
      "Epoch 159/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8325 - val_loss: 0.8424 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.85445\n",
      "Epoch 160/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5341 - acc: 0.8255 - val_loss: 0.8329 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.85445\n",
      "Epoch 161/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5280 - acc: 0.8317 - val_loss: 0.8337 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.85445\n",
      "Epoch 162/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5633 - acc: 0.8075 - val_loss: 0.8252 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.85445\n",
      "Epoch 163/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5163 - acc: 0.8290 - val_loss: 0.8274 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.85445\n",
      "Epoch 164/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5313 - acc: 0.8311 - val_loss: 0.8287 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.85445\n",
      "Epoch 165/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5163 - acc: 0.8311 - val_loss: 0.8331 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.85445\n",
      "Epoch 166/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5487 - acc: 0.8225 - val_loss: 0.8199 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.85445\n",
      "Epoch 167/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5183 - acc: 0.8296 - val_loss: 0.8233 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.85445\n",
      "Epoch 168/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5268 - acc: 0.8228 - val_loss: 0.8191 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.85445\n",
      "Epoch 169/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5006 - acc: 0.8355 - val_loss: 0.8322 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.85445\n",
      "Epoch 170/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5069 - acc: 0.8390 - val_loss: 0.8229 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.85445\n",
      "Epoch 171/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5272 - acc: 0.8240 - val_loss: 0.8278 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.85445\n",
      "Epoch 172/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5325 - acc: 0.8196 - val_loss: 0.8342 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.85445\n",
      "Epoch 173/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5212 - acc: 0.8343 - val_loss: 0.8248 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.85445\n",
      "Epoch 174/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5159 - acc: 0.8278 - val_loss: 0.8145 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.85445\n",
      "Epoch 175/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4982 - acc: 0.8458 - val_loss: 0.8282 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.85445\n",
      "Epoch 176/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5203 - acc: 0.8314 - val_loss: 0.8193 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.85445\n",
      "Epoch 177/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5334 - acc: 0.8287 - val_loss: 0.8296 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.85445\n",
      "Epoch 178/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5200 - acc: 0.8210 - val_loss: 0.8222 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.85445\n",
      "Epoch 179/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5203 - acc: 0.8334 - val_loss: 0.8251 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.85445\n",
      "Epoch 180/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8352 - val_loss: 0.8269 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.85445\n",
      "Epoch 181/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5229 - acc: 0.8275 - val_loss: 0.8190 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.85445\n",
      "Epoch 182/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5091 - acc: 0.8281 - val_loss: 0.8274 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.85445\n",
      "Epoch 183/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5009 - acc: 0.8390 - val_loss: 0.8222 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.85445\n",
      "Epoch 184/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8423 - val_loss: 0.8236 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.85445\n",
      "Epoch 185/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5139 - acc: 0.8343 - val_loss: 0.8287 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.85445\n",
      "Epoch 186/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5121 - acc: 0.8284 - val_loss: 0.8175 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.85445\n",
      "Epoch 187/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5117 - acc: 0.8328 - val_loss: 0.8200 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.85445\n",
      "Epoch 188/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5016 - acc: 0.8175 - val_loss: 0.8188 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.85445\n",
      "Epoch 189/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4916 - acc: 0.8446 - val_loss: 0.8248 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.85445\n",
      "Epoch 190/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5215 - acc: 0.8278 - val_loss: 0.8230 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.85445\n",
      "Epoch 191/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5040 - acc: 0.8264 - val_loss: 0.8233 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.85445\n",
      "Epoch 192/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5181 - acc: 0.8299 - val_loss: 0.8181 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.85445\n",
      "Epoch 193/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4975 - acc: 0.8349 - val_loss: 0.8207 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.85445\n",
      "Epoch 194/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5300 - acc: 0.8311 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.85445\n",
      "Epoch 195/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5327 - acc: 0.8258 - val_loss: 0.8171 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.85445\n",
      "Epoch 196/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5139 - acc: 0.8269 - val_loss: 0.8185 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.85445\n",
      "Epoch 197/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4890 - acc: 0.8417 - val_loss: 0.8206 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.85445\n",
      "Epoch 198/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4980 - acc: 0.8281 - val_loss: 0.8185 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.85445\n",
      "Epoch 199/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5231 - acc: 0.8149 - val_loss: 0.8160 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.85445\n",
      "Epoch 200/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5110 - acc: 0.8269 - val_loss: 0.8161 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.85445\n",
      "Epoch 201/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4968 - acc: 0.8449 - val_loss: 0.8073 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.85445\n",
      "Epoch 202/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4870 - acc: 0.8346 - val_loss: 0.8104 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.85445\n",
      "Epoch 203/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4853 - acc: 0.8396 - val_loss: 0.8090 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.85445\n",
      "Epoch 204/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5271 - acc: 0.8249 - val_loss: 0.8112 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.85445\n",
      "Epoch 205/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5170 - acc: 0.8246 - val_loss: 0.8144 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.85445\n",
      "Epoch 206/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5335 - acc: 0.8237 - val_loss: 0.8177 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.85445\n",
      "Epoch 207/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5029 - acc: 0.8402 - val_loss: 0.8266 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.85445\n",
      "Epoch 208/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8237 - val_loss: 0.8264 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.85445\n",
      "Epoch 209/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4998 - acc: 0.8373 - val_loss: 0.8225 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.85445\n",
      "Epoch 210/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4929 - acc: 0.8390 - val_loss: 0.8175 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.85445\n",
      "Epoch 211/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5077 - acc: 0.8325 - val_loss: 0.8152 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.85445\n",
      "Epoch 212/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5066 - acc: 0.8396 - val_loss: 0.8169 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.85445\n",
      "Epoch 213/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5304 - acc: 0.8296 - val_loss: 0.8208 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.85445\n",
      "Epoch 214/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4722 - acc: 0.8358 - val_loss: 0.8171 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.85445\n",
      "Epoch 215/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4969 - acc: 0.8361 - val_loss: 0.8193 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.85445\n",
      "Epoch 216/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4933 - acc: 0.8305 - val_loss: 0.8157 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.85445\n",
      "Epoch 217/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5042 - acc: 0.8311 - val_loss: 0.8092 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.85445\n",
      "Epoch 218/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5227 - acc: 0.8210 - val_loss: 0.8096 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.85445\n",
      "Epoch 219/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5151 - acc: 0.8284 - val_loss: 0.8106 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.85445\n",
      "Epoch 220/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5098 - acc: 0.8237 - val_loss: 0.8144 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.85445\n",
      "Epoch 221/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4979 - acc: 0.8390 - val_loss: 0.8126 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.85445\n",
      "Epoch 222/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4913 - acc: 0.8420 - val_loss: 0.8181 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.85445\n",
      "Epoch 223/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5028 - acc: 0.8417 - val_loss: 0.8194 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.85445\n",
      "Epoch 224/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5212 - acc: 0.8334 - val_loss: 0.8179 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.85445\n",
      "Epoch 225/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5200 - acc: 0.8302 - val_loss: 0.8163 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.85445\n",
      "Epoch 226/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5129 - acc: 0.8314 - val_loss: 0.8132 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.85445\n",
      "Epoch 227/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5223 - acc: 0.8275 - val_loss: 0.8133 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.85445\n",
      "Epoch 228/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4977 - acc: 0.8364 - val_loss: 0.8146 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.85445\n",
      "Epoch 229/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5109 - acc: 0.8317 - val_loss: 0.8101 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.85445\n",
      "Epoch 230/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5073 - acc: 0.8361 - val_loss: 0.8163 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.85445\n",
      "Epoch 231/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5091 - acc: 0.8275 - val_loss: 0.8166 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.85445\n",
      "Epoch 232/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5078 - acc: 0.8376 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.85445\n",
      "Epoch 233/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5234 - acc: 0.8370 - val_loss: 0.8218 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.85445\n",
      "Epoch 234/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8343 - val_loss: 0.8177 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.85445\n",
      "Epoch 235/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4746 - acc: 0.8390 - val_loss: 0.8185 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.85445\n",
      "Epoch 236/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5325 - acc: 0.8275 - val_loss: 0.8124 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.85445\n",
      "Epoch 237/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4826 - acc: 0.8429 - val_loss: 0.8110 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.85445\n",
      "Epoch 238/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5015 - acc: 0.8390 - val_loss: 0.8133 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.85445\n",
      "Epoch 239/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8331 - val_loss: 0.8106 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.85445\n",
      "Epoch 240/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8364 - val_loss: 0.8170 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.85445\n",
      "Epoch 241/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4936 - acc: 0.8328 - val_loss: 0.8111 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.85445\n",
      "Epoch 242/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4862 - acc: 0.8323 - val_loss: 0.8072 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.85445\n",
      "Epoch 243/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4964 - acc: 0.8320 - val_loss: 0.8089 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.85445\n",
      "Epoch 244/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5084 - acc: 0.8323 - val_loss: 0.8147 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.85445\n",
      "Epoch 245/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4983 - acc: 0.8473 - val_loss: 0.8160 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.85445\n",
      "Epoch 246/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5222 - acc: 0.8258 - val_loss: 0.8090 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.85445\n",
      "Epoch 247/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5157 - acc: 0.8305 - val_loss: 0.8114 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.85445\n",
      "Epoch 248/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5015 - acc: 0.8299 - val_loss: 0.8143 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.85445\n",
      "Epoch 249/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5189 - acc: 0.8373 - val_loss: 0.8082 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00249: val_acc improved from 0.85445 to 0.85445, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 250/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5040 - acc: 0.8399 - val_loss: 0.8142 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.85445\n",
      "Epoch 251/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4974 - acc: 0.8314 - val_loss: 0.8120 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.85445\n",
      "Epoch 252/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5180 - acc: 0.8275 - val_loss: 0.8078 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.85445\n",
      "Epoch 253/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5058 - acc: 0.8367 - val_loss: 0.8132 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.85445\n",
      "Epoch 254/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5164 - acc: 0.8290 - val_loss: 0.8184 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.85445\n",
      "Epoch 255/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4977 - acc: 0.8370 - val_loss: 0.8161 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.85445\n",
      "Epoch 256/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4842 - acc: 0.8358 - val_loss: 0.8191 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.85445\n",
      "Epoch 257/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5013 - acc: 0.8361 - val_loss: 0.8236 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.85445\n",
      "Epoch 258/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5058 - acc: 0.8379 - val_loss: 0.8182 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.85445\n",
      "Epoch 259/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4971 - acc: 0.8399 - val_loss: 0.8171 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.85445\n",
      "Epoch 260/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5046 - acc: 0.8370 - val_loss: 0.8168 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.85445\n",
      "Epoch 261/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5162 - acc: 0.8311 - val_loss: 0.8182 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.85445\n",
      "Epoch 262/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5065 - acc: 0.8258 - val_loss: 0.8178 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.85445\n",
      "Epoch 263/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4952 - acc: 0.8287 - val_loss: 0.8175 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.85445\n",
      "Epoch 264/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5086 - acc: 0.8302 - val_loss: 0.8108 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.85445\n",
      "Epoch 265/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4823 - acc: 0.8358 - val_loss: 0.8169 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.85445\n",
      "Epoch 266/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4907 - acc: 0.8349 - val_loss: 0.8120 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.85445\n",
      "Epoch 267/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5152 - acc: 0.8314 - val_loss: 0.8132 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.85445\n",
      "Epoch 268/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4992 - acc: 0.8346 - val_loss: 0.8127 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.85445\n",
      "Epoch 269/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4962 - acc: 0.8290 - val_loss: 0.8116 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.85445\n",
      "Epoch 270/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4906 - acc: 0.8311 - val_loss: 0.8099 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.85445\n",
      "Epoch 271/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4947 - acc: 0.8420 - val_loss: 0.8158 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.85445\n",
      "Epoch 272/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4924 - acc: 0.8411 - val_loss: 0.8142 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.85445\n",
      "Epoch 273/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5012 - acc: 0.8331 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.85445\n",
      "Epoch 274/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4950 - acc: 0.8320 - val_loss: 0.8126 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.85445\n",
      "Epoch 275/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4794 - acc: 0.8352 - val_loss: 0.8102 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.85445\n",
      "Epoch 276/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5010 - acc: 0.8328 - val_loss: 0.8113 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.85445\n",
      "Epoch 277/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5276 - acc: 0.8252 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.85445\n",
      "Epoch 278/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5161 - acc: 0.8190 - val_loss: 0.8105 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.85445\n",
      "Epoch 279/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5036 - acc: 0.8246 - val_loss: 0.8110 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.85445\n",
      "Epoch 280/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5165 - acc: 0.8228 - val_loss: 0.8097 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.85445\n",
      "Epoch 281/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8284 - val_loss: 0.8120 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.85445\n",
      "Epoch 282/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5060 - acc: 0.8275 - val_loss: 0.8101 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.85445\n",
      "Epoch 283/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8367 - val_loss: 0.8145 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.85445\n",
      "Epoch 284/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5039 - acc: 0.8281 - val_loss: 0.8072 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.85445\n",
      "Epoch 285/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5009 - acc: 0.8281 - val_loss: 0.8067 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.85445\n",
      "Epoch 286/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4932 - acc: 0.8361 - val_loss: 0.8083 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.85445\n",
      "Epoch 287/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5094 - acc: 0.8352 - val_loss: 0.8118 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.85445\n",
      "Epoch 288/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4743 - acc: 0.8408 - val_loss: 0.8090 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.85445\n",
      "Epoch 289/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4773 - acc: 0.8482 - val_loss: 0.8094 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.85445\n",
      "Epoch 290/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4995 - acc: 0.8317 - val_loss: 0.8095 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.85445\n",
      "Epoch 291/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5083 - acc: 0.8340 - val_loss: 0.8130 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.85445\n",
      "Epoch 292/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5038 - acc: 0.8281 - val_loss: 0.8104 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.85445\n",
      "Epoch 293/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4935 - acc: 0.8387 - val_loss: 0.8114 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.85445\n",
      "Epoch 294/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8302 - val_loss: 0.8151 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.85445\n",
      "Epoch 295/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4927 - acc: 0.8381 - val_loss: 0.8138 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.85445\n",
      "Epoch 296/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4838 - acc: 0.8358 - val_loss: 0.8137 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.85445\n",
      "Epoch 297/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4947 - acc: 0.8381 - val_loss: 0.8118 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.85445\n",
      "Epoch 298/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8323 - val_loss: 0.8118 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.85445\n",
      "Epoch 299/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5034 - acc: 0.8305 - val_loss: 0.8121 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.85445\n",
      "Epoch 300/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4832 - acc: 0.8393 - val_loss: 0.8121 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.85445\n",
      "Epoch 301/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4981 - acc: 0.8284 - val_loss: 0.8150 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.85445\n",
      "Epoch 302/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4847 - acc: 0.8317 - val_loss: 0.8119 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.85445\n",
      "Epoch 303/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5014 - acc: 0.8325 - val_loss: 0.8066 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.85445\n",
      "Epoch 304/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5332 - acc: 0.8267 - val_loss: 0.8082 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.85445\n",
      "Epoch 305/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4978 - acc: 0.8320 - val_loss: 0.8099 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.85445\n",
      "Epoch 306/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4953 - acc: 0.8396 - val_loss: 0.8202 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.85445\n",
      "Epoch 307/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5144 - acc: 0.8349 - val_loss: 0.8135 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.85445\n",
      "Epoch 308/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4962 - acc: 0.8343 - val_loss: 0.8136 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.85445\n",
      "Epoch 309/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4929 - acc: 0.8355 - val_loss: 0.8132 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.85445\n",
      "Epoch 310/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5007 - acc: 0.8414 - val_loss: 0.8105 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.85445\n",
      "Epoch 311/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5204 - acc: 0.8249 - val_loss: 0.8136 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.85445\n",
      "Epoch 312/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5171 - acc: 0.8228 - val_loss: 0.8130 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.85445\n",
      "Epoch 313/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5041 - acc: 0.8343 - val_loss: 0.8188 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.85445\n",
      "Epoch 314/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4986 - acc: 0.8290 - val_loss: 0.8156 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.85445\n",
      "Epoch 315/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5199 - acc: 0.8287 - val_loss: 0.8158 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.85445\n",
      "Epoch 316/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5029 - acc: 0.8264 - val_loss: 0.8148 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.85445\n",
      "Epoch 317/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5184 - acc: 0.8396 - val_loss: 0.8145 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.85445\n",
      "Epoch 318/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4892 - acc: 0.8396 - val_loss: 0.8151 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.85445\n",
      "Epoch 319/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4869 - acc: 0.8376 - val_loss: 0.8157 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.85445\n",
      "Epoch 320/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4926 - acc: 0.8443 - val_loss: 0.8105 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.85445\n",
      "Epoch 321/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4772 - acc: 0.8317 - val_loss: 0.8163 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.85445\n",
      "Epoch 322/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4757 - acc: 0.8340 - val_loss: 0.8138 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.85445\n",
      "Epoch 323/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5112 - acc: 0.8387 - val_loss: 0.8141 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.85445\n",
      "Epoch 324/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4883 - acc: 0.8358 - val_loss: 0.8118 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.85445\n",
      "Epoch 325/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4741 - acc: 0.8390 - val_loss: 0.8147 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.85445\n",
      "Epoch 326/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5240 - acc: 0.8367 - val_loss: 0.8180 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.85445\n",
      "Epoch 327/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4927 - acc: 0.8370 - val_loss: 0.8147 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.85445\n",
      "Epoch 328/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4863 - acc: 0.8284 - val_loss: 0.8167 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.85445\n",
      "Epoch 329/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5012 - acc: 0.8343 - val_loss: 0.8128 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.85445\n",
      "Epoch 330/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5116 - acc: 0.8393 - val_loss: 0.8127 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.85445\n",
      "Epoch 331/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4851 - acc: 0.8420 - val_loss: 0.8133 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.85445\n",
      "Epoch 332/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4812 - acc: 0.8420 - val_loss: 0.8128 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.85445\n",
      "Epoch 333/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5152 - acc: 0.8349 - val_loss: 0.8118 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.85445\n",
      "Epoch 334/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4966 - acc: 0.8408 - val_loss: 0.8116 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.85445\n",
      "Epoch 335/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4889 - acc: 0.8387 - val_loss: 0.8161 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.85445\n",
      "Epoch 336/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5180 - acc: 0.8193 - val_loss: 0.8149 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.85445\n",
      "Epoch 337/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5106 - acc: 0.8296 - val_loss: 0.8138 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.85445\n",
      "Epoch 338/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5143 - acc: 0.8352 - val_loss: 0.8162 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.85445\n",
      "Epoch 339/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5143 - acc: 0.8281 - val_loss: 0.8126 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.85445\n",
      "Epoch 340/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4981 - acc: 0.8387 - val_loss: 0.8156 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.85445\n",
      "Epoch 341/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4970 - acc: 0.8346 - val_loss: 0.8131 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.85445\n",
      "Epoch 342/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4677 - acc: 0.8373 - val_loss: 0.8145 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.85445\n",
      "Epoch 343/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5342 - acc: 0.8287 - val_loss: 0.8166 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.85445\n",
      "Epoch 344/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4547 - acc: 0.8414 - val_loss: 0.8141 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.85445\n",
      "Epoch 345/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4948 - acc: 0.8370 - val_loss: 0.8173 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.85445\n",
      "Epoch 346/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4853 - acc: 0.8352 - val_loss: 0.8153 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.85445\n",
      "Epoch 347/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4957 - acc: 0.8532 - val_loss: 0.8181 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.85445\n",
      "Epoch 348/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4950 - acc: 0.8290 - val_loss: 0.8157 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.85445\n",
      "Epoch 349/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4917 - acc: 0.8479 - val_loss: 0.8161 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.85445\n",
      "Epoch 350/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8381 - val_loss: 0.8182 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.85445\n",
      "Epoch 351/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4680 - acc: 0.8470 - val_loss: 0.8162 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.85445\n",
      "Epoch 352/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5024 - acc: 0.8252 - val_loss: 0.8195 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.85445\n",
      "Epoch 353/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4864 - acc: 0.8429 - val_loss: 0.8162 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.85445\n",
      "Epoch 354/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4821 - acc: 0.8381 - val_loss: 0.8155 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.85445\n",
      "Epoch 355/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5151 - acc: 0.8349 - val_loss: 0.8136 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.85445\n",
      "Epoch 356/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4817 - acc: 0.8287 - val_loss: 0.8149 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.85445\n",
      "Epoch 357/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4932 - acc: 0.8331 - val_loss: 0.8148 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.85445\n",
      "Epoch 358/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4661 - acc: 0.8367 - val_loss: 0.8197 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.85445\n",
      "Epoch 359/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4898 - acc: 0.8390 - val_loss: 0.8174 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.85445\n",
      "Epoch 360/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4714 - acc: 0.8461 - val_loss: 0.8199 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.85445\n",
      "Epoch 361/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5004 - acc: 0.8381 - val_loss: 0.8195 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.85445\n",
      "Epoch 362/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4915 - acc: 0.8296 - val_loss: 0.8198 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.85445\n",
      "Epoch 363/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4769 - acc: 0.8364 - val_loss: 0.8201 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.85445\n",
      "Epoch 364/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4905 - acc: 0.8361 - val_loss: 0.8194 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.85445\n",
      "Epoch 365/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5106 - acc: 0.8373 - val_loss: 0.8139 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.85445\n",
      "Epoch 366/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4840 - acc: 0.8352 - val_loss: 0.8162 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.85445\n",
      "Epoch 367/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4942 - acc: 0.8331 - val_loss: 0.8184 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.85445\n",
      "Epoch 368/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8340 - val_loss: 0.8175 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.85445\n",
      "Epoch 369/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4753 - acc: 0.8305 - val_loss: 0.8171 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.85445\n",
      "Epoch 370/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5043 - acc: 0.8343 - val_loss: 0.8192 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.85445\n",
      "Epoch 371/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5166 - acc: 0.8255 - val_loss: 0.8197 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.85445\n",
      "Epoch 372/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8267 - val_loss: 0.8217 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.85445\n",
      "Epoch 373/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4957 - acc: 0.8355 - val_loss: 0.8188 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.85445\n",
      "Epoch 374/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4960 - acc: 0.8393 - val_loss: 0.8150 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.85445\n",
      "Epoch 375/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4839 - acc: 0.8399 - val_loss: 0.8165 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.85445\n",
      "Epoch 376/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4871 - acc: 0.8370 - val_loss: 0.8160 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.85445\n",
      "Epoch 377/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4616 - acc: 0.8470 - val_loss: 0.8152 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.85445\n",
      "Epoch 378/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4763 - acc: 0.8432 - val_loss: 0.8178 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.85445\n",
      "Epoch 379/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4751 - acc: 0.8361 - val_loss: 0.8124 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.85445\n",
      "Epoch 380/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4769 - acc: 0.8384 - val_loss: 0.8129 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.85445\n",
      "Epoch 381/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4920 - acc: 0.8328 - val_loss: 0.8129 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.85445\n",
      "Epoch 382/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5108 - acc: 0.8305 - val_loss: 0.8112 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.85445\n",
      "Epoch 383/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4892 - acc: 0.8323 - val_loss: 0.8123 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.85445\n",
      "Epoch 384/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4869 - acc: 0.8405 - val_loss: 0.8151 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.85445\n",
      "Epoch 385/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5003 - acc: 0.8272 - val_loss: 0.8131 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.85445\n",
      "Epoch 386/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4999 - acc: 0.8417 - val_loss: 0.8174 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.85445\n",
      "Epoch 387/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4979 - acc: 0.8340 - val_loss: 0.8158 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.85445\n",
      "Epoch 388/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4748 - acc: 0.8452 - val_loss: 0.8090 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.85445\n",
      "Epoch 389/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4871 - acc: 0.8278 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.85445\n",
      "Epoch 390/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5047 - acc: 0.8334 - val_loss: 0.8143 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.85445\n",
      "Epoch 391/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4562 - acc: 0.8367 - val_loss: 0.8192 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.85445\n",
      "Epoch 392/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4790 - acc: 0.8346 - val_loss: 0.8159 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.85445\n",
      "Epoch 393/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4935 - acc: 0.8278 - val_loss: 0.8120 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.85445\n",
      "Epoch 394/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4966 - acc: 0.8414 - val_loss: 0.8121 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.85445\n",
      "Epoch 395/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4989 - acc: 0.8349 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.85445\n",
      "Epoch 396/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4792 - acc: 0.8384 - val_loss: 0.8101 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.85445\n",
      "Epoch 397/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4743 - acc: 0.8370 - val_loss: 0.8174 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.85445\n",
      "Epoch 398/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4834 - acc: 0.8296 - val_loss: 0.8178 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.85445\n",
      "Epoch 399/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4806 - acc: 0.8328 - val_loss: 0.8137 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.85445\n",
      "Epoch 400/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4954 - acc: 0.8402 - val_loss: 0.8123 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.85445\n",
      "Epoch 401/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4965 - acc: 0.8252 - val_loss: 0.8127 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.85445\n",
      "Epoch 402/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4890 - acc: 0.8408 - val_loss: 0.8169 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.85445\n",
      "Epoch 403/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5033 - acc: 0.8213 - val_loss: 0.8140 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.85445\n",
      "Epoch 00403: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold7_mfcc7===\n",
      "using resnet model: 3\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 30, 130, 64)  3200        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 30, 130, 64)  256         conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 30, 130, 64)  0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 15, 65, 64)   0           activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 15, 65, 64)   256         conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 15, 65, 64)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 15, 65, 64)   36928       activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 15, 65, 64)   256         conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 15, 65, 64)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 15, 65, 256)  16640       activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 15, 65, 256)  0           conv2d_327[0][0]                 \n",
      "                                                                 conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 15, 65, 256)  1024        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 15, 65, 256)  0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 15, 65, 64)   16448       activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 15, 65, 64)   256         conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 15, 65, 64)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 15, 65, 64)   36928       activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 15, 65, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 15, 65, 64)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 15, 65, 256)  16640       activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 15, 65, 256)  0           add_114[0][0]                    \n",
      "                                                                 conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 15, 65, 256)  1024        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 15, 65, 256)  0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 15, 65, 64)   16448       activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 15, 65, 64)   256         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 15, 65, 64)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 15, 65, 64)   36928       activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 15, 65, 64)   256         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 15, 65, 64)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 15, 65, 256)  16640       activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 15, 65, 256)  0           add_115[0][0]                    \n",
      "                                                                 conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 15, 65, 256)  1024        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 15, 65, 256)  0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 8, 33, 128)   32896       activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 8, 33, 128)   512         conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 8, 33, 128)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 8, 33, 128)   147584      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 8, 33, 128)   512         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 8, 33, 128)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 8, 33, 512)   131584      add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 8, 33, 512)   66048       activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 8, 33, 512)   0           conv2d_337[0][0]                 \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 8, 33, 512)   2048        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 8, 33, 512)   0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 8, 33, 128)   65664       activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 8, 33, 128)   512         conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 8, 33, 128)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 8, 33, 128)   147584      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 8, 33, 128)   512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 8, 33, 128)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 8, 33, 512)   66048       activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 8, 33, 512)   0           add_117[0][0]                    \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 8, 33, 512)   2048        add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 8, 33, 512)   0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 8, 33, 128)   65664       activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 8, 33, 128)   512         conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 8, 33, 128)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 8, 33, 128)   147584      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 8, 33, 128)   512         conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 8, 33, 128)   0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 8, 33, 512)   66048       activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 8, 33, 512)   0           add_118[0][0]                    \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 8, 33, 512)   2048        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 8, 33, 512)   0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 8, 33, 128)   65664       activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 8, 33, 128)   512         conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 8, 33, 128)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 8, 33, 128)   147584      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 8, 33, 128)   512         conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 8, 33, 128)   0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 8, 33, 512)   66048       activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 8, 33, 512)   0           add_119[0][0]                    \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 8, 33, 512)   2048        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 8, 33, 512)   0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 17, 256)   131328      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 17, 256)   1024        conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 17, 256)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 17, 256)   590080      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 17, 256)   1024        conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 17, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 17, 1024)  525312      add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 17, 1024)  263168      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 4, 17, 1024)  0           conv2d_350[0][0]                 \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 17, 1024)  4096        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 17, 256)   262400      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 17, 256)   1024        conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 17, 256)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 17, 256)   590080      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 17, 256)   1024        conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 17, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 17, 1024)  263168      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 4, 17, 1024)  0           add_121[0][0]                    \n",
      "                                                                 conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 17, 1024)  4096        add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 17, 256)   262400      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 17, 256)   1024        conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 17, 256)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 17, 256)   590080      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 17, 256)   1024        conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 17, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 17, 1024)  263168      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 4, 17, 1024)  0           add_122[0][0]                    \n",
      "                                                                 conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 17, 1024)  4096        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 17, 256)   262400      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 17, 256)   1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 17, 256)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 17, 256)   590080      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 17, 256)   1024        conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 17, 256)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 4, 17, 1024)  263168      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_124 (Add)                   (None, 4, 17, 1024)  0           add_123[0][0]                    \n",
      "                                                                 conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 17, 1024)  4096        add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 4, 17, 256)   262400      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 17, 256)   1024        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 17, 256)   0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 4, 17, 256)   590080      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 17, 256)   1024        conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 17, 256)   0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 4, 17, 1024)  263168      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_125 (Add)                   (None, 4, 17, 1024)  0           add_124[0][0]                    \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 17, 1024)  4096        add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 4, 17, 256)   262400      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 17, 256)   1024        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 17, 256)   0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 4, 17, 256)   590080      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 17, 256)   1024        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 17, 256)   0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 4, 17, 1024)  263168      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_126 (Add)                   (None, 4, 17, 1024)  0           add_125[0][0]                    \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 17, 1024)  4096        add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 4, 17, 256)   262400      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 4, 17, 256)   1024        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 17, 256)   0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 4, 17, 256)   590080      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 4, 17, 256)   1024        conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 17, 256)   0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 4, 17, 1024)  263168      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_127 (Add)                   (None, 4, 17, 1024)  0           add_126[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 4, 17, 1024)  4096        add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 4, 17, 256)   262400      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 4, 17, 256)   1024        conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 4, 17, 256)   0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 4, 17, 256)   590080      activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 4, 17, 256)   1024        conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 4, 17, 256)   0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 4, 17, 1024)  263168      activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_128 (Add)                   (None, 4, 17, 1024)  0           add_127[0][0]                    \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 4, 17, 1024)  4096        add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 4, 17, 256)   262400      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 4, 17, 256)   1024        conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 4, 17, 256)   0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 4, 17, 256)   590080      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 4, 17, 256)   1024        conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 4, 17, 256)   0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 4, 17, 1024)  263168      activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_129 (Add)                   (None, 4, 17, 1024)  0           add_128[0][0]                    \n",
      "                                                                 conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 4, 17, 1024)  4096        add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 4, 17, 256)   262400      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 4, 17, 256)   1024        conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 4, 17, 256)   0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_376 (Conv2D)             (None, 4, 17, 256)   590080      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 4, 17, 256)   1024        conv2d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 4, 17, 256)   0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_377 (Conv2D)             (None, 4, 17, 1024)  263168      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_130 (Add)                   (None, 4, 17, 1024)  0           add_129[0][0]                    \n",
      "                                                                 conv2d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 4, 17, 1024)  4096        add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_378 (Conv2D)             (None, 4, 17, 256)   262400      activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 4, 17, 256)   1024        conv2d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 4, 17, 256)   0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_379 (Conv2D)             (None, 4, 17, 256)   590080      activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 4, 17, 256)   1024        conv2d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 4, 17, 256)   0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_380 (Conv2D)             (None, 4, 17, 1024)  263168      activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_131 (Add)                   (None, 4, 17, 1024)  0           add_130[0][0]                    \n",
      "                                                                 conv2d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 4, 17, 1024)  4096        add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_381 (Conv2D)             (None, 4, 17, 256)   262400      activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 4, 17, 256)   1024        conv2d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 4, 17, 256)   0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_382 (Conv2D)             (None, 4, 17, 256)   590080      activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 4, 17, 256)   1024        conv2d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 4, 17, 256)   0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_383 (Conv2D)             (None, 4, 17, 1024)  263168      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 4, 17, 1024)  0           add_131[0][0]                    \n",
      "                                                                 conv2d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 4, 17, 1024)  4096        add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_384 (Conv2D)             (None, 4, 17, 256)   262400      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 4, 17, 256)   1024        conv2d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 4, 17, 256)   0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_385 (Conv2D)             (None, 4, 17, 256)   590080      activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 4, 17, 256)   1024        conv2d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 4, 17, 256)   0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_386 (Conv2D)             (None, 4, 17, 1024)  263168      activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 4, 17, 1024)  0           add_132[0][0]                    \n",
      "                                                                 conv2d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 4, 17, 1024)  4096        add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_387 (Conv2D)             (None, 4, 17, 256)   262400      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 4, 17, 256)   1024        conv2d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 4, 17, 256)   0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_388 (Conv2D)             (None, 4, 17, 256)   590080      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 4, 17, 256)   1024        conv2d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 4, 17, 256)   0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 4, 17, 1024)  263168      activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 4, 17, 1024)  0           add_133[0][0]                    \n",
      "                                                                 conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 4, 17, 1024)  4096        add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 4, 17, 256)   262400      activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 4, 17, 256)   1024        conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 4, 17, 256)   0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 4, 17, 256)   590080      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 4, 17, 256)   1024        conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 4, 17, 256)   0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 4, 17, 1024)  263168      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 4, 17, 1024)  0           add_134[0][0]                    \n",
      "                                                                 conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 4, 17, 1024)  4096        add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 4, 17, 256)   262400      activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 4, 17, 256)   1024        conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 4, 17, 256)   0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 4, 17, 256)   590080      activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 4, 17, 256)   1024        conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 4, 17, 256)   0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 4, 17, 1024)  263168      activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 4, 17, 1024)  0           add_135[0][0]                    \n",
      "                                                                 conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 4, 17, 1024)  4096        add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 4, 17, 256)   262400      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 4, 17, 256)   1024        conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 4, 17, 256)   0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 4, 17, 256)   590080      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 4, 17, 256)   1024        conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 4, 17, 256)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 4, 17, 1024)  263168      activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 4, 17, 1024)  0           add_136[0][0]                    \n",
      "                                                                 conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 4, 17, 1024)  4096        add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 4, 17, 256)   262400      activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 4, 17, 256)   1024        conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 4, 17, 256)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 4, 17, 256)   590080      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 4, 17, 256)   1024        conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 4, 17, 256)   0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 4, 17, 1024)  263168      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 4, 17, 1024)  0           add_137[0][0]                    \n",
      "                                                                 conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 4, 17, 1024)  4096        add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 4, 17, 256)   262400      activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 4, 17, 256)   1024        conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 4, 17, 256)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 4, 17, 256)   590080      activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 4, 17, 256)   1024        conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 4, 17, 256)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 4, 17, 1024)  263168      activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 4, 17, 1024)  0           add_138[0][0]                    \n",
      "                                                                 conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 4, 17, 1024)  4096        add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 4, 17, 256)   262400      activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 4, 17, 256)   1024        conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 4, 17, 256)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 4, 17, 256)   590080      activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 4, 17, 256)   1024        conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 4, 17, 256)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 4, 17, 1024)  263168      activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 4, 17, 1024)  0           add_139[0][0]                    \n",
      "                                                                 conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 4, 17, 1024)  4096        add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 4, 17, 256)   262400      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 4, 17, 256)   1024        conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 4, 17, 256)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 4, 17, 256)   590080      activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 4, 17, 256)   1024        conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 4, 17, 256)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 4, 17, 1024)  263168      activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 4, 17, 1024)  0           add_140[0][0]                    \n",
      "                                                                 conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 4, 17, 1024)  4096        add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 4, 17, 256)   262400      activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 4, 17, 256)   1024        conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 4, 17, 256)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 4, 17, 256)   590080      activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 4, 17, 256)   1024        conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 4, 17, 256)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 4, 17, 1024)  263168      activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 4, 17, 1024)  0           add_141[0][0]                    \n",
      "                                                                 conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 4, 17, 1024)  4096        add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 4, 17, 256)   262400      activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 4, 17, 256)   1024        conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 4, 17, 256)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 4, 17, 256)   590080      activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 4, 17, 256)   1024        conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 4, 17, 256)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 4, 17, 1024)  263168      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 4, 17, 1024)  0           add_142[0][0]                    \n",
      "                                                                 conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 4, 17, 1024)  4096        add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 2, 9, 512)    524800      activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 2, 9, 512)    2048        conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 2, 9, 512)    0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 2, 9, 512)    2359808     activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 2, 9, 512)    2048        conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 2, 9, 512)    0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 2, 9, 2048)   2099200     add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 2, 9, 2048)   0           conv2d_420[0][0]                 \n",
      "                                                                 conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 2, 9, 2048)   8192        add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 2, 9, 512)    1049088     activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 2, 9, 512)    2048        conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 2, 9, 512)    0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 2, 9, 512)    2359808     activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 2, 9, 512)    2048        conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 2, 9, 512)    0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 2, 9, 2048)   0           add_144[0][0]                    \n",
      "                                                                 conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 2, 9, 2048)   8192        add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 2, 9, 512)    1049088     activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 2, 9, 512)    2048        conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 2, 9, 512)    0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 2, 9, 512)    2359808     activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 2, 9, 512)    2048        conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 2, 9, 512)    0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 2, 9, 2048)   0           add_145[0][0]                    \n",
      "                                                                 conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 2, 9, 2048)   8192        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1, 1, 2048)   0           activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 44)           90156       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 44)           176         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 44)           0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 41)           1845        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,728,721\n",
      "Trainable params: 42,630,969\n",
      "Non-trainable params: 97,752\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "104/104 [==============================] - 31s 298ms/step - loss: 13.1943 - acc: 0.0484 - val_loss: 11.3525 - val_acc: 0.0431\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.04313, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 2/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 9.7494 - acc: 0.0754 - val_loss: 8.6106 - val_acc: 0.0997\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.04313 to 0.09973, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 3/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 7.4657 - acc: 0.0916 - val_loss: 6.6357 - val_acc: 0.0916\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.09973\n",
      "Epoch 4/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 6.0615 - acc: 0.0944 - val_loss: 5.5426 - val_acc: 0.1105\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.09973 to 0.11051, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 5.2186 - acc: 0.1220 - val_loss: 5.3015 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.11051 to 0.12938, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 6/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.6955 - acc: 0.1268 - val_loss: 4.6258 - val_acc: 0.1078\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.12938\n",
      "Epoch 7/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.2841 - acc: 0.1550 - val_loss: 4.5833 - val_acc: 0.1051\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.12938\n",
      "Epoch 8/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.0487 - acc: 0.1617 - val_loss: 3.6932 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.12938 to 0.16442, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 9/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.8751 - acc: 0.1617 - val_loss: 3.9628 - val_acc: 0.1806\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.16442 to 0.18059, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 10/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.7542 - acc: 0.1797 - val_loss: 3.0725 - val_acc: 0.2318\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.18059 to 0.23181, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 11/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.5998 - acc: 0.1914 - val_loss: 4.1241 - val_acc: 0.1590\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.23181\n",
      "Epoch 12/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.5287 - acc: 0.1977 - val_loss: 2.9334 - val_acc: 0.3019\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.23181 to 0.30189, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 13/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.4585 - acc: 0.2073 - val_loss: 3.5923 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.30189\n",
      "Epoch 14/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.4016 - acc: 0.2163 - val_loss: 4.3131 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.30189\n",
      "Epoch 15/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.3480 - acc: 0.2344 - val_loss: 3.2426 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.30189\n",
      "Epoch 16/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.3182 - acc: 0.2200 - val_loss: 3.4447 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.30189\n",
      "Epoch 17/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.2691 - acc: 0.2434 - val_loss: 2.9332 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.30189\n",
      "Epoch 18/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.2235 - acc: 0.2584 - val_loss: 3.0088 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.30189\n",
      "Epoch 19/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 3.1939 - acc: 0.2533 - val_loss: 3.1747 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.30189\n",
      "Epoch 20/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1735 - acc: 0.2647 - val_loss: 2.8285 - val_acc: 0.3046\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.30189 to 0.30458, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 21/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1450 - acc: 0.2803 - val_loss: 2.5112 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.30458 to 0.32615, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 22/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1205 - acc: 0.2891 - val_loss: 3.3427 - val_acc: 0.2237\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.32615\n",
      "Epoch 23/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0949 - acc: 0.2825 - val_loss: 2.4923 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.32615 to 0.32884, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 24/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0751 - acc: 0.2897 - val_loss: 3.5520 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.32884\n",
      "Epoch 25/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0454 - acc: 0.3032 - val_loss: 2.4403 - val_acc: 0.3693\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.32884 to 0.36927, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 26/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0477 - acc: 0.2930 - val_loss: 2.4589 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.36927\n",
      "Epoch 27/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0407 - acc: 0.3011 - val_loss: 2.3740 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.36927 to 0.38814, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 28/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0432 - acc: 0.3032 - val_loss: 2.7198 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.38814\n",
      "Epoch 29/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9989 - acc: 0.3200 - val_loss: 2.2128 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.38814 to 0.42318, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 30/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9572 - acc: 0.3272 - val_loss: 3.0081 - val_acc: 0.2642\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.42318\n",
      "Epoch 31/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9482 - acc: 0.3459 - val_loss: 2.2337 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.42318\n",
      "Epoch 32/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9767 - acc: 0.3266 - val_loss: 2.5531 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.42318\n",
      "Epoch 33/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9355 - acc: 0.3314 - val_loss: 2.5346 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.42318\n",
      "Epoch 34/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9650 - acc: 0.3287 - val_loss: 2.5664 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.42318\n",
      "Epoch 35/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9378 - acc: 0.3438 - val_loss: 2.5534 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.42318\n",
      "Epoch 36/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8932 - acc: 0.3477 - val_loss: 2.4106 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.42318\n",
      "Epoch 37/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9197 - acc: 0.3398 - val_loss: 2.4898 - val_acc: 0.3396\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.42318\n",
      "Epoch 38/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8974 - acc: 0.3480 - val_loss: 3.3308 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.42318\n",
      "Epoch 39/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8935 - acc: 0.3498 - val_loss: 2.6245 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.42318\n",
      "Epoch 40/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8556 - acc: 0.3666 - val_loss: 2.2348 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.42318 to 0.43396, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 41/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8504 - acc: 0.3609 - val_loss: 2.4649 - val_acc: 0.3181\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43396\n",
      "Epoch 42/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8432 - acc: 0.3750 - val_loss: 2.2757 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.43396\n",
      "Epoch 43/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.8793 - acc: 0.3579 - val_loss: 2.3172 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.43396\n",
      "Epoch 44/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8708 - acc: 0.3675 - val_loss: 2.1280 - val_acc: 0.4178\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.43396\n",
      "Epoch 45/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8275 - acc: 0.3792 - val_loss: 2.2173 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.43396\n",
      "Epoch 46/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8328 - acc: 0.3768 - val_loss: 2.3854 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.43396\n",
      "Epoch 47/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7927 - acc: 0.3867 - val_loss: 2.2213 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.43396\n",
      "Epoch 48/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7837 - acc: 0.3888 - val_loss: 2.4305 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.43396\n",
      "Epoch 49/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.8051 - acc: 0.3909 - val_loss: 2.3590 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.43396\n",
      "Epoch 50/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7928 - acc: 0.3912 - val_loss: 2.3144 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.43396\n",
      "Epoch 51/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7991 - acc: 0.3915 - val_loss: 2.3940 - val_acc: 0.3666\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.43396\n",
      "Epoch 52/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7505 - acc: 0.4099 - val_loss: 1.9444 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.43396 to 0.50404, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 53/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7577 - acc: 0.3966 - val_loss: 2.4931 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.50404\n",
      "Epoch 54/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7224 - acc: 0.4207 - val_loss: 2.0324 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.50404 to 0.52291, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 55/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7621 - acc: 0.3975 - val_loss: 1.9603 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.52291\n",
      "Epoch 56/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7663 - acc: 0.4041 - val_loss: 2.2100 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.52291\n",
      "Epoch 57/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7498 - acc: 0.4090 - val_loss: 2.2188 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.52291\n",
      "Epoch 58/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7370 - acc: 0.4216 - val_loss: 2.2137 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.52291\n",
      "Epoch 59/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7362 - acc: 0.4306 - val_loss: 2.2040 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.52291\n",
      "Epoch 60/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7067 - acc: 0.4213 - val_loss: 5.2504 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.52291\n",
      "Epoch 61/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7194 - acc: 0.4105 - val_loss: 1.8890 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.52291\n",
      "Epoch 62/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7299 - acc: 0.4141 - val_loss: 2.5465 - val_acc: 0.3774\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.52291\n",
      "Epoch 63/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6966 - acc: 0.4273 - val_loss: 3.2679 - val_acc: 0.2399\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.52291\n",
      "Epoch 64/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6850 - acc: 0.4336 - val_loss: 1.7087 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.52291 to 0.55526, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 65/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6783 - acc: 0.4279 - val_loss: 2.4578 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.55526\n",
      "Epoch 66/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6587 - acc: 0.4375 - val_loss: 2.1841 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.55526\n",
      "Epoch 67/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7005 - acc: 0.4264 - val_loss: 3.0916 - val_acc: 0.2507\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.55526\n",
      "Epoch 68/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7099 - acc: 0.4300 - val_loss: 2.0903 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.55526\n",
      "Epoch 69/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6702 - acc: 0.4414 - val_loss: 2.3438 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.55526\n",
      "Epoch 70/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6685 - acc: 0.4483 - val_loss: 2.8562 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.55526\n",
      "Epoch 71/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6578 - acc: 0.4399 - val_loss: 1.9720 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.55526\n",
      "Epoch 72/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6730 - acc: 0.4429 - val_loss: 2.6241 - val_acc: 0.3774\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.55526\n",
      "Epoch 73/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6528 - acc: 0.4327 - val_loss: 2.0396 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.55526\n",
      "Epoch 74/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6283 - acc: 0.4597 - val_loss: 1.9986 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.55526\n",
      "Epoch 75/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6365 - acc: 0.4525 - val_loss: 1.8476 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.55526\n",
      "Epoch 76/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6087 - acc: 0.4567 - val_loss: 2.4647 - val_acc: 0.4097\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.55526\n",
      "Epoch 77/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6255 - acc: 0.4567 - val_loss: 1.8684 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.55526\n",
      "Epoch 78/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6284 - acc: 0.4606 - val_loss: 2.1464 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.55526\n",
      "Epoch 79/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5915 - acc: 0.4606 - val_loss: 1.9847 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.55526\n",
      "Epoch 80/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5715 - acc: 0.4724 - val_loss: 2.0777 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.55526\n",
      "Epoch 81/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6160 - acc: 0.4426 - val_loss: 1.7370 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.55526 to 0.56065, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6056 - acc: 0.4678 - val_loss: 2.1143 - val_acc: 0.4636\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.56065\n",
      "Epoch 83/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5753 - acc: 0.4663 - val_loss: 2.0739 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.56065\n",
      "Epoch 84/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6193 - acc: 0.4627 - val_loss: 1.8494 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.56065\n",
      "Epoch 85/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5895 - acc: 0.4709 - val_loss: 1.8383 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.56065\n",
      "Epoch 86/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6096 - acc: 0.4748 - val_loss: 1.7248 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.56065 to 0.57143, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 87/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5781 - acc: 0.4748 - val_loss: 1.7320 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.57143\n",
      "Epoch 88/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5714 - acc: 0.4856 - val_loss: 2.2558 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.57143\n",
      "Epoch 89/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5453 - acc: 0.4919 - val_loss: 2.2484 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.57143\n",
      "Epoch 90/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5316 - acc: 0.4910 - val_loss: 1.8279 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.57143\n",
      "Epoch 91/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5520 - acc: 0.4811 - val_loss: 1.9488 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.57143\n",
      "Epoch 92/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5546 - acc: 0.4775 - val_loss: 2.1518 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.57143\n",
      "Epoch 93/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5626 - acc: 0.4787 - val_loss: 2.0986 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.57143\n",
      "Epoch 94/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5196 - acc: 0.5027 - val_loss: 1.8898 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.57143\n",
      "Epoch 95/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5557 - acc: 0.4736 - val_loss: 1.7539 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.57143\n",
      "Epoch 96/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5059 - acc: 0.4940 - val_loss: 1.6600 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.57143 to 0.59299, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 97/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5025 - acc: 0.5075 - val_loss: 1.9041 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.59299\n",
      "Epoch 98/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5244 - acc: 0.5090 - val_loss: 1.7818 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.59299\n",
      "Epoch 99/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5144 - acc: 0.4790 - val_loss: 1.9934 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.59299\n",
      "Epoch 100/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4991 - acc: 0.5036 - val_loss: 1.7473 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.59299\n",
      "Epoch 101/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4840 - acc: 0.5012 - val_loss: 2.0101 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.59299\n",
      "Epoch 102/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4865 - acc: 0.5153 - val_loss: 1.9734 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.59299\n",
      "Epoch 103/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4983 - acc: 0.4976 - val_loss: 1.7409 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.59299\n",
      "Epoch 104/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5393 - acc: 0.4955 - val_loss: 1.7462 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.59299\n",
      "Epoch 105/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4853 - acc: 0.4958 - val_loss: 1.7381 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.59299\n",
      "Epoch 106/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4769 - acc: 0.5144 - val_loss: 2.1086 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.59299\n",
      "Epoch 107/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4683 - acc: 0.5090 - val_loss: 1.6933 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.59299\n",
      "Epoch 108/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4817 - acc: 0.5072 - val_loss: 1.9889 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.59299\n",
      "Epoch 109/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4998 - acc: 0.4946 - val_loss: 1.9330 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.59299\n",
      "Epoch 110/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4809 - acc: 0.5192 - val_loss: 1.6030 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00110: val_acc improved from 0.59299 to 0.60108, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 111/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4934 - acc: 0.5042 - val_loss: 2.1887 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.60108\n",
      "Epoch 112/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4638 - acc: 0.5084 - val_loss: 1.7815 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.60108\n",
      "Epoch 113/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4748 - acc: 0.5186 - val_loss: 1.5878 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.60108\n",
      "Epoch 114/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4155 - acc: 0.5331 - val_loss: 1.6846 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.60108\n",
      "Epoch 115/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4660 - acc: 0.5231 - val_loss: 2.2268 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.60108\n",
      "Epoch 116/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4300 - acc: 0.5204 - val_loss: 1.5104 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.60108 to 0.62534, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 117/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4558 - acc: 0.5171 - val_loss: 1.5849 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.62534\n",
      "Epoch 118/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4350 - acc: 0.5204 - val_loss: 2.2850 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.62534\n",
      "Epoch 119/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4700 - acc: 0.5171 - val_loss: 1.6191 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.62534\n",
      "Epoch 120/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4620 - acc: 0.5081 - val_loss: 1.7853 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.62534\n",
      "Epoch 121/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4300 - acc: 0.5418 - val_loss: 1.8643 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.62534\n",
      "Epoch 122/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4117 - acc: 0.5288 - val_loss: 1.3970 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00122: val_acc improved from 0.62534 to 0.66038, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 123/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4082 - acc: 0.5303 - val_loss: 1.8739 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.66038\n",
      "Epoch 124/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4262 - acc: 0.5189 - val_loss: 1.7739 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.66038\n",
      "Epoch 125/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4064 - acc: 0.5319 - val_loss: 1.7581 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.66038\n",
      "Epoch 126/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4287 - acc: 0.5346 - val_loss: 1.7310 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.66038\n",
      "Epoch 127/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4083 - acc: 0.5337 - val_loss: 1.6133 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.66038\n",
      "Epoch 128/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3811 - acc: 0.5439 - val_loss: 1.6363 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.66038\n",
      "Epoch 129/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4202 - acc: 0.5334 - val_loss: 1.8329 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.66038\n",
      "Epoch 130/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4129 - acc: 0.5319 - val_loss: 2.0886 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.66038\n",
      "Epoch 131/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4248 - acc: 0.5343 - val_loss: 1.8967 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.66038\n",
      "Epoch 132/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4083 - acc: 0.5346 - val_loss: 1.7967 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.66038\n",
      "Epoch 133/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3871 - acc: 0.5385 - val_loss: 1.5420 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.66038\n",
      "Epoch 134/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3852 - acc: 0.5466 - val_loss: 1.4599 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.66038\n",
      "Epoch 135/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3939 - acc: 0.5457 - val_loss: 1.8613 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.66038\n",
      "Epoch 136/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3714 - acc: 0.5550 - val_loss: 1.6268 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.66038\n",
      "Epoch 137/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4009 - acc: 0.5355 - val_loss: 1.4932 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.66038\n",
      "Epoch 138/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3729 - acc: 0.5616 - val_loss: 1.4487 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.66038\n",
      "Epoch 139/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3796 - acc: 0.5607 - val_loss: 1.8176 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.66038\n",
      "Epoch 140/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3863 - acc: 0.5343 - val_loss: 1.8302 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.66038\n",
      "Epoch 141/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3615 - acc: 0.5490 - val_loss: 1.5666 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.66038\n",
      "Epoch 142/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3443 - acc: 0.5619 - val_loss: 1.4744 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.66038\n",
      "Epoch 143/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3501 - acc: 0.5640 - val_loss: 1.4659 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.66038\n",
      "Epoch 144/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3705 - acc: 0.5577 - val_loss: 1.6552 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.66038\n",
      "Epoch 145/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4220 - acc: 0.5427 - val_loss: 1.4871 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.66038\n",
      "Epoch 146/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3787 - acc: 0.5376 - val_loss: 1.6268 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.66038\n",
      "Epoch 147/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3913 - acc: 0.5499 - val_loss: 2.0666 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.66038\n",
      "Epoch 148/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3403 - acc: 0.5646 - val_loss: 1.4627 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.66038\n",
      "Epoch 149/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3638 - acc: 0.5541 - val_loss: 1.5313 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.66038\n",
      "Epoch 150/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3268 - acc: 0.5634 - val_loss: 1.4008 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.66038\n",
      "Epoch 151/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3591 - acc: 0.5544 - val_loss: 1.7607 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.66038\n",
      "Epoch 152/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3423 - acc: 0.5739 - val_loss: 1.4763 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.66038\n",
      "Epoch 153/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3469 - acc: 0.5529 - val_loss: 1.4805 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.66038\n",
      "Epoch 154/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3717 - acc: 0.5514 - val_loss: 1.6208 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.66038\n",
      "Epoch 155/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3445 - acc: 0.5658 - val_loss: 1.5749 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.66038\n",
      "Epoch 156/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3205 - acc: 0.5667 - val_loss: 1.8703 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.66038\n",
      "Epoch 157/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3438 - acc: 0.5637 - val_loss: 1.7493 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.66038\n",
      "Epoch 158/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3480 - acc: 0.5565 - val_loss: 1.4041 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.66038\n",
      "Epoch 159/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3457 - acc: 0.5640 - val_loss: 1.4501 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.66038\n",
      "Epoch 160/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3181 - acc: 0.5895 - val_loss: 2.0663 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.66038\n",
      "Epoch 161/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3296 - acc: 0.5688 - val_loss: 1.6512 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.66038\n",
      "Epoch 162/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3306 - acc: 0.5763 - val_loss: 2.1039 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.66038\n",
      "Epoch 163/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3383 - acc: 0.5703 - val_loss: 1.7051 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.66038\n",
      "Epoch 164/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3342 - acc: 0.5808 - val_loss: 1.9652 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.66038\n",
      "Epoch 165/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3207 - acc: 0.5865 - val_loss: 1.9980 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.66038\n",
      "Epoch 166/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3088 - acc: 0.5676 - val_loss: 1.6239 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.66038\n",
      "Epoch 167/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3280 - acc: 0.5739 - val_loss: 1.4356 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.66038\n",
      "Epoch 168/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2756 - acc: 0.5871 - val_loss: 1.5352 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.66038\n",
      "Epoch 169/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2905 - acc: 0.5980 - val_loss: 1.6908 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.66038\n",
      "Epoch 170/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2982 - acc: 0.5754 - val_loss: 1.6403 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.66038\n",
      "Epoch 171/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3172 - acc: 0.5778 - val_loss: 1.6655 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.66038\n",
      "Epoch 172/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2956 - acc: 0.5871 - val_loss: 1.6640 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.66038\n",
      "Epoch 173/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2889 - acc: 0.5838 - val_loss: 1.4811 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.66038\n",
      "Epoch 174/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3274 - acc: 0.5730 - val_loss: 1.7159 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.66038\n",
      "Epoch 175/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2742 - acc: 0.5968 - val_loss: 1.6568 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.66038\n",
      "Epoch 176/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2929 - acc: 0.5766 - val_loss: 1.4831 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.66038\n",
      "Epoch 177/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2741 - acc: 0.5814 - val_loss: 1.6636 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.66038\n",
      "Epoch 178/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2946 - acc: 0.5895 - val_loss: 1.6160 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.66038\n",
      "Epoch 179/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3148 - acc: 0.5754 - val_loss: 1.5928 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.66038\n",
      "Epoch 180/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2807 - acc: 0.5850 - val_loss: 1.5013 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.66038\n",
      "Epoch 181/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2938 - acc: 0.5841 - val_loss: 1.8515 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.66038\n",
      "Epoch 182/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2746 - acc: 0.5838 - val_loss: 1.6513 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.66038\n",
      "Epoch 183/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2887 - acc: 0.5974 - val_loss: 1.6139 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.66038\n",
      "Epoch 184/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2796 - acc: 0.5886 - val_loss: 1.4487 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.66038\n",
      "Epoch 185/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2826 - acc: 0.5793 - val_loss: 1.6489 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.66038\n",
      "Epoch 186/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2666 - acc: 0.5814 - val_loss: 1.9146 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.66038\n",
      "Epoch 187/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2733 - acc: 0.5772 - val_loss: 1.6006 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.66038\n",
      "Epoch 188/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3081 - acc: 0.5748 - val_loss: 1.3779 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00188: val_acc improved from 0.66038 to 0.69272, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 189/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2427 - acc: 0.6001 - val_loss: 1.7476 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.69272\n",
      "Epoch 190/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2700 - acc: 0.5895 - val_loss: 1.3794 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.69272\n",
      "Epoch 191/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2885 - acc: 0.5931 - val_loss: 1.5375 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.69272\n",
      "Epoch 192/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2559 - acc: 0.5938 - val_loss: 1.9932 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.69272\n",
      "Epoch 193/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2813 - acc: 0.5925 - val_loss: 2.2800 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.69272\n",
      "Epoch 194/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2689 - acc: 0.5856 - val_loss: 1.5674 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.69272\n",
      "Epoch 195/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2293 - acc: 0.6124 - val_loss: 1.4515 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.69272\n",
      "Epoch 196/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2456 - acc: 0.5934 - val_loss: 1.4905 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.69272\n",
      "Epoch 197/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2581 - acc: 0.5983 - val_loss: 1.4136 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.69272\n",
      "Epoch 198/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2582 - acc: 0.5862 - val_loss: 1.4729 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.69272\n",
      "Epoch 199/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2444 - acc: 0.6001 - val_loss: 1.5930 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.69272\n",
      "Epoch 200/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2151 - acc: 0.6133 - val_loss: 1.4607 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.69272\n",
      "Epoch 201/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2524 - acc: 0.6028 - val_loss: 1.7673 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.69272\n",
      "Epoch 202/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2185 - acc: 0.6088 - val_loss: 1.7777 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.69272\n",
      "Epoch 203/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2624 - acc: 0.5959 - val_loss: 1.5084 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.69272\n",
      "Epoch 204/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2716 - acc: 0.5874 - val_loss: 1.6867 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.69272\n",
      "Epoch 205/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2450 - acc: 0.6040 - val_loss: 1.7222 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.69272\n",
      "Epoch 206/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2482 - acc: 0.6055 - val_loss: 2.4478 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.69272\n",
      "Epoch 207/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2531 - acc: 0.6061 - val_loss: 2.2285 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.69272\n",
      "Epoch 208/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2344 - acc: 0.6109 - val_loss: 1.4795 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.69272\n",
      "Epoch 209/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2488 - acc: 0.6040 - val_loss: 1.7277 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.69272\n",
      "Epoch 210/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2319 - acc: 0.6073 - val_loss: 1.3944 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.69272\n",
      "Epoch 211/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2177 - acc: 0.6085 - val_loss: 1.3874 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.69272\n",
      "Epoch 212/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2306 - acc: 0.6133 - val_loss: 1.6480 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.69272\n",
      "Epoch 213/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2202 - acc: 0.6040 - val_loss: 1.5637 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.69272\n",
      "Epoch 214/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2229 - acc: 0.6130 - val_loss: 1.8806 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.69272\n",
      "Epoch 215/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2370 - acc: 0.6073 - val_loss: 1.4240 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.69272\n",
      "Epoch 216/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2225 - acc: 0.6217 - val_loss: 1.5907 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.69272\n",
      "Epoch 217/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2479 - acc: 0.6037 - val_loss: 1.9466 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.69272\n",
      "Epoch 218/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2303 - acc: 0.6172 - val_loss: 1.4360 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.69272\n",
      "Epoch 219/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2380 - acc: 0.6142 - val_loss: 1.7792 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.69272\n",
      "Epoch 220/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2356 - acc: 0.6028 - val_loss: 1.3846 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.69272\n",
      "Epoch 221/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1890 - acc: 0.6244 - val_loss: 1.4459 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.69272\n",
      "Epoch 222/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1942 - acc: 0.6154 - val_loss: 1.6171 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.69272\n",
      "Epoch 223/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1902 - acc: 0.6190 - val_loss: 1.5379 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.69272\n",
      "Epoch 224/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1716 - acc: 0.6307 - val_loss: 3.5196 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.69272\n",
      "Epoch 225/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1891 - acc: 0.6232 - val_loss: 1.5401 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.69272\n",
      "Epoch 226/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2267 - acc: 0.6187 - val_loss: 1.9193 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.69272\n",
      "Epoch 227/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1918 - acc: 0.6265 - val_loss: 1.5637 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.69272\n",
      "Epoch 228/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1940 - acc: 0.6226 - val_loss: 1.2827 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00228: val_acc improved from 0.69272 to 0.70620, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 229/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1953 - acc: 0.6217 - val_loss: 1.5901 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.70620\n",
      "Epoch 230/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2013 - acc: 0.6301 - val_loss: 1.7231 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.70620\n",
      "Epoch 231/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2142 - acc: 0.6247 - val_loss: 1.4122 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.70620\n",
      "Epoch 232/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1904 - acc: 0.6334 - val_loss: 1.4363 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.70620\n",
      "Epoch 233/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2003 - acc: 0.6358 - val_loss: 1.5008 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.70620\n",
      "Epoch 234/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1920 - acc: 0.6277 - val_loss: 1.6115 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.70620\n",
      "Epoch 235/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1700 - acc: 0.6277 - val_loss: 1.3831 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.70620\n",
      "Epoch 236/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1889 - acc: 0.6265 - val_loss: 1.8352 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.70620\n",
      "Epoch 237/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1783 - acc: 0.6280 - val_loss: 1.3895 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.70620\n",
      "Epoch 238/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1624 - acc: 0.6238 - val_loss: 1.6835 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.70620\n",
      "Epoch 239/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1509 - acc: 0.6454 - val_loss: 1.4763 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.70620\n",
      "Epoch 240/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2083 - acc: 0.6154 - val_loss: 1.3973 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.70620\n",
      "Epoch 241/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2013 - acc: 0.6214 - val_loss: 1.8112 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.70620\n",
      "Epoch 242/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1609 - acc: 0.6349 - val_loss: 1.4818 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.70620\n",
      "Epoch 243/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1779 - acc: 0.6277 - val_loss: 1.6564 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.70620\n",
      "Epoch 244/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1617 - acc: 0.6388 - val_loss: 1.4577 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.70620\n",
      "Epoch 245/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1935 - acc: 0.6280 - val_loss: 1.4020 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.70620\n",
      "Epoch 246/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1505 - acc: 0.6400 - val_loss: 2.1682 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.70620\n",
      "Epoch 247/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1584 - acc: 0.6352 - val_loss: 1.4912 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.70620\n",
      "Epoch 248/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1601 - acc: 0.6427 - val_loss: 1.2765 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.70620\n",
      "Epoch 249/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1818 - acc: 0.6241 - val_loss: 2.1700 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.70620\n",
      "Epoch 250/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1879 - acc: 0.6304 - val_loss: 1.7386 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.70620\n",
      "Epoch 251/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1453 - acc: 0.6361 - val_loss: 1.4749 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.70620\n",
      "Epoch 252/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1769 - acc: 0.6301 - val_loss: 1.2565 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.70620\n",
      "Epoch 253/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1739 - acc: 0.6439 - val_loss: 1.2846 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.70620\n",
      "Epoch 254/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1403 - acc: 0.6463 - val_loss: 1.5311 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.70620\n",
      "Epoch 255/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1688 - acc: 0.6292 - val_loss: 1.8313 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.70620\n",
      "Epoch 256/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1594 - acc: 0.6394 - val_loss: 1.4588 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.70620\n",
      "Epoch 257/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1392 - acc: 0.6544 - val_loss: 1.6887 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.70620\n",
      "Epoch 258/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1995 - acc: 0.6172 - val_loss: 1.5550 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.70620\n",
      "Epoch 259/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1702 - acc: 0.6418 - val_loss: 1.4449 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.70620\n",
      "Epoch 260/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1514 - acc: 0.6478 - val_loss: 1.4496 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.70620\n",
      "Epoch 261/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1435 - acc: 0.6502 - val_loss: 1.7390 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.70620\n",
      "Epoch 262/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1451 - acc: 0.6469 - val_loss: 1.3273 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.70620\n",
      "Epoch 263/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1325 - acc: 0.6304 - val_loss: 1.3496 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.70620\n",
      "Epoch 264/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1498 - acc: 0.6529 - val_loss: 1.2649 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00264: val_acc improved from 0.70620 to 0.70889, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 265/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1554 - acc: 0.6316 - val_loss: 1.7085 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.70889\n",
      "Epoch 266/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1192 - acc: 0.6418 - val_loss: 1.3505 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.70889\n",
      "Epoch 267/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1516 - acc: 0.6280 - val_loss: 1.1583 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00267: val_acc improved from 0.70889 to 0.73046, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 268/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1240 - acc: 0.6559 - val_loss: 1.3674 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.73046\n",
      "Epoch 269/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1589 - acc: 0.6514 - val_loss: 1.3288 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.73046\n",
      "Epoch 270/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1436 - acc: 0.6499 - val_loss: 1.8256 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.73046\n",
      "Epoch 271/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1430 - acc: 0.6457 - val_loss: 2.0530 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.73046\n",
      "Epoch 272/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1379 - acc: 0.6487 - val_loss: 1.8271 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.73046\n",
      "Epoch 273/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1226 - acc: 0.6535 - val_loss: 1.2508 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.73046\n",
      "Epoch 274/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0993 - acc: 0.6668 - val_loss: 1.9990 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.73046\n",
      "Epoch 275/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1061 - acc: 0.6617 - val_loss: 1.6615 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.73046\n",
      "Epoch 276/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1511 - acc: 0.6502 - val_loss: 1.3801 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.73046\n",
      "Epoch 277/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1548 - acc: 0.6442 - val_loss: 2.0500 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.73046\n",
      "Epoch 278/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1378 - acc: 0.6475 - val_loss: 1.6358 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.73046\n",
      "Epoch 279/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1155 - acc: 0.6490 - val_loss: 1.7490 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.73046\n",
      "Epoch 280/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1326 - acc: 0.6493 - val_loss: 1.9097 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.73046\n",
      "Epoch 281/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1302 - acc: 0.6538 - val_loss: 1.3758 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.73046\n",
      "Epoch 282/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1438 - acc: 0.6472 - val_loss: 1.2652 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.73046\n",
      "Epoch 283/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1181 - acc: 0.6466 - val_loss: 1.4530 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.73046\n",
      "Epoch 284/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1399 - acc: 0.6451 - val_loss: 1.2450 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.73046\n",
      "Epoch 285/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1342 - acc: 0.6457 - val_loss: 1.6006 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.73046\n",
      "Epoch 286/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0976 - acc: 0.6584 - val_loss: 1.3307 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.73046\n",
      "Epoch 287/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1437 - acc: 0.6478 - val_loss: 1.3776 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.73046\n",
      "Epoch 288/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1112 - acc: 0.6532 - val_loss: 1.3052 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.73046\n",
      "Epoch 289/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1294 - acc: 0.6460 - val_loss: 1.8036 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.73046\n",
      "Epoch 290/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1170 - acc: 0.6538 - val_loss: 1.9183 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.73046\n",
      "Epoch 291/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0984 - acc: 0.6677 - val_loss: 1.3066 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.73046\n",
      "Epoch 292/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0899 - acc: 0.6695 - val_loss: 1.4468 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.73046\n",
      "Epoch 293/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1015 - acc: 0.6505 - val_loss: 1.2162 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.73046\n",
      "Epoch 294/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1128 - acc: 0.6472 - val_loss: 1.6189 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.73046\n",
      "Epoch 295/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1262 - acc: 0.6593 - val_loss: 1.4420 - val_acc: 0.6496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_acc did not improve from 0.73046\n",
      "Epoch 296/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0969 - acc: 0.6608 - val_loss: 1.4881 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.73046\n",
      "Epoch 297/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1384 - acc: 0.6505 - val_loss: 1.7873 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.73046\n",
      "Epoch 298/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0971 - acc: 0.6641 - val_loss: 1.2305 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.73046\n",
      "Epoch 299/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1311 - acc: 0.6505 - val_loss: 1.5933 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.73046\n",
      "Epoch 300/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0863 - acc: 0.6593 - val_loss: 1.9527 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.73046\n",
      "Epoch 301/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0852 - acc: 0.6701 - val_loss: 1.5428 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.73046\n",
      "Epoch 302/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0799 - acc: 0.6650 - val_loss: 1.8000 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.73046\n",
      "Epoch 303/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1111 - acc: 0.6578 - val_loss: 1.5382 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.73046\n",
      "Epoch 304/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1067 - acc: 0.6502 - val_loss: 1.2247 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.73046\n",
      "Epoch 305/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0869 - acc: 0.6695 - val_loss: 1.2794 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.73046\n",
      "Epoch 306/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1017 - acc: 0.6584 - val_loss: 1.3736 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.73046\n",
      "Epoch 307/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0970 - acc: 0.6701 - val_loss: 1.5430 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.73046\n",
      "Epoch 308/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1024 - acc: 0.6704 - val_loss: 1.2744 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.73046\n",
      "Epoch 309/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0965 - acc: 0.6737 - val_loss: 1.6512 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.73046\n",
      "Epoch 310/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0976 - acc: 0.6671 - val_loss: 1.3275 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.73046\n",
      "Epoch 311/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1008 - acc: 0.6635 - val_loss: 1.5241 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.73046\n",
      "Epoch 312/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0876 - acc: 0.6725 - val_loss: 1.5877 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.73046\n",
      "Epoch 313/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1148 - acc: 0.6575 - val_loss: 1.5968 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.73046\n",
      "Epoch 314/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1077 - acc: 0.6662 - val_loss: 1.5206 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.73046\n",
      "Epoch 315/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0836 - acc: 0.6650 - val_loss: 1.2869 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.73046\n",
      "Epoch 316/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0789 - acc: 0.6791 - val_loss: 1.9751 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.73046\n",
      "Epoch 317/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0768 - acc: 0.6629 - val_loss: 1.9617 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.73046\n",
      "Epoch 318/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0824 - acc: 0.6764 - val_loss: 1.7685 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.73046\n",
      "Epoch 319/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0949 - acc: 0.6653 - val_loss: 1.6227 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.73046\n",
      "Epoch 320/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0666 - acc: 0.6740 - val_loss: 1.3160 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.73046\n",
      "Epoch 321/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0944 - acc: 0.6605 - val_loss: 1.4872 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.73046\n",
      "Epoch 322/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0634 - acc: 0.6842 - val_loss: 1.3591 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.73046\n",
      "Epoch 323/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0950 - acc: 0.6695 - val_loss: 1.6384 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.73046\n",
      "Epoch 324/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0762 - acc: 0.6761 - val_loss: 1.5512 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.73046\n",
      "Epoch 325/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0846 - acc: 0.6734 - val_loss: 1.3904 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.73046\n",
      "Epoch 326/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1052 - acc: 0.6644 - val_loss: 2.1830 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.73046\n",
      "Epoch 327/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0747 - acc: 0.6764 - val_loss: 1.2423 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.73046\n",
      "Epoch 328/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1100 - acc: 0.6632 - val_loss: 1.1067 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00328: val_acc improved from 0.73046 to 0.76280, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 329/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0507 - acc: 0.6791 - val_loss: 1.5593 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.76280\n",
      "Epoch 330/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0406 - acc: 0.6845 - val_loss: 1.6521 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.76280\n",
      "Epoch 331/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0666 - acc: 0.6779 - val_loss: 1.2414 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.76280\n",
      "Epoch 332/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0566 - acc: 0.6776 - val_loss: 1.9775 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.76280\n",
      "Epoch 333/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1043 - acc: 0.6629 - val_loss: 1.3874 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.76280\n",
      "Epoch 334/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1148 - acc: 0.6596 - val_loss: 1.6611 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.76280\n",
      "Epoch 335/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1333 - acc: 0.6529 - val_loss: 1.2480 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.76280\n",
      "Epoch 336/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0914 - acc: 0.6809 - val_loss: 1.2815 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.76280\n",
      "Epoch 337/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0578 - acc: 0.6896 - val_loss: 1.2221 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.76280\n",
      "Epoch 338/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0615 - acc: 0.6722 - val_loss: 2.0541 - val_acc: 0.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00338: val_acc did not improve from 0.76280\n",
      "Epoch 339/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0562 - acc: 0.6779 - val_loss: 1.9267 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.76280\n",
      "Epoch 340/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0765 - acc: 0.6707 - val_loss: 1.4584 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.76280\n",
      "Epoch 341/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0276 - acc: 0.6854 - val_loss: 1.2995 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.76280\n",
      "Epoch 342/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0683 - acc: 0.6653 - val_loss: 1.2638 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.76280\n",
      "Epoch 343/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0526 - acc: 0.6737 - val_loss: 1.2126 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.76280\n",
      "Epoch 344/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0472 - acc: 0.6848 - val_loss: 1.3322 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.76280\n",
      "Epoch 345/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0803 - acc: 0.6671 - val_loss: 1.7558 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.76280\n",
      "Epoch 346/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0570 - acc: 0.6866 - val_loss: 1.3614 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.76280\n",
      "Epoch 347/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0917 - acc: 0.6632 - val_loss: 1.7740 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.76280\n",
      "Epoch 348/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0666 - acc: 0.6809 - val_loss: 1.4497 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.76280\n",
      "Epoch 349/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0694 - acc: 0.6788 - val_loss: 1.4759 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.76280\n",
      "Epoch 350/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0183 - acc: 0.6887 - val_loss: 1.1892 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.76280\n",
      "Epoch 351/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0783 - acc: 0.6755 - val_loss: 1.6690 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.76280\n",
      "Epoch 352/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0547 - acc: 0.6797 - val_loss: 1.5544 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.76280\n",
      "Epoch 353/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0253 - acc: 0.6974 - val_loss: 1.3568 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.76280\n",
      "Epoch 354/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0621 - acc: 0.6719 - val_loss: 1.5162 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.76280\n",
      "Epoch 355/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0960 - acc: 0.6626 - val_loss: 1.7540 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.76280\n",
      "Epoch 356/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0229 - acc: 0.6875 - val_loss: 1.5074 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.76280\n",
      "Epoch 357/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0503 - acc: 0.6887 - val_loss: 1.5401 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.76280\n",
      "Epoch 358/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0580 - acc: 0.6761 - val_loss: 1.7477 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.76280\n",
      "Epoch 359/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0660 - acc: 0.6701 - val_loss: 1.2417 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.76280\n",
      "Epoch 360/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0258 - acc: 0.6848 - val_loss: 1.4873 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.76280\n",
      "Epoch 361/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0513 - acc: 0.6806 - val_loss: 1.3388 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.76280\n",
      "Epoch 362/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0296 - acc: 0.6836 - val_loss: 1.2766 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.76280\n",
      "Epoch 363/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0206 - acc: 0.6950 - val_loss: 1.3027 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.76280\n",
      "Epoch 364/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0672 - acc: 0.6707 - val_loss: 1.6278 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.76280\n",
      "Epoch 365/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0274 - acc: 0.6932 - val_loss: 1.3898 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.76280\n",
      "Epoch 366/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0364 - acc: 0.6959 - val_loss: 1.5615 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.76280\n",
      "Epoch 367/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0569 - acc: 0.6932 - val_loss: 1.1882 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.76280\n",
      "Epoch 368/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0681 - acc: 0.6737 - val_loss: 1.2102 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.76280\n",
      "Epoch 369/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0436 - acc: 0.6926 - val_loss: 1.4074 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.76280\n",
      "Epoch 370/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0326 - acc: 0.6911 - val_loss: 1.5461 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.76280\n",
      "Epoch 371/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0361 - acc: 0.6818 - val_loss: 2.3040 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.76280\n",
      "Epoch 372/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0239 - acc: 0.6800 - val_loss: 1.6627 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.76280\n",
      "Epoch 373/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0638 - acc: 0.6854 - val_loss: 1.4694 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.76280\n",
      "Epoch 374/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0359 - acc: 0.6926 - val_loss: 1.2959 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.76280\n",
      "Epoch 375/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0066 - acc: 0.6899 - val_loss: 1.7822 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.76280\n",
      "Epoch 376/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0310 - acc: 0.6956 - val_loss: 1.4106 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.76280\n",
      "Epoch 377/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0430 - acc: 0.6899 - val_loss: 1.5118 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.76280\n",
      "Epoch 378/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0187 - acc: 0.6971 - val_loss: 1.1478 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.76280\n",
      "Epoch 379/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0194 - acc: 0.6929 - val_loss: 1.7097 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.76280\n",
      "Epoch 380/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0005 - acc: 0.7067 - val_loss: 1.3192 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.76280\n",
      "Epoch 381/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0558 - acc: 0.6803 - val_loss: 1.1931 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.76280\n",
      "Epoch 382/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0409 - acc: 0.6905 - val_loss: 1.3713 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.76280\n",
      "Epoch 383/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0143 - acc: 0.6881 - val_loss: 1.6202 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.76280\n",
      "Epoch 384/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0667 - acc: 0.6755 - val_loss: 1.5952 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.76280\n",
      "Epoch 385/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0221 - acc: 0.6926 - val_loss: 1.5277 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.76280\n",
      "Epoch 386/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0240 - acc: 0.6872 - val_loss: 1.6782 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.76280\n",
      "Epoch 387/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0677 - acc: 0.6866 - val_loss: 1.2091 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.76280\n",
      "Epoch 388/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9832 - acc: 0.7082 - val_loss: 1.3195 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.76280\n",
      "Epoch 389/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0153 - acc: 0.6929 - val_loss: 1.4587 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.76280\n",
      "Epoch 390/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0322 - acc: 0.7061 - val_loss: 1.3641 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.76280\n",
      "Epoch 391/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0392 - acc: 0.6839 - val_loss: 1.3595 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.76280\n",
      "Epoch 392/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0028 - acc: 0.6920 - val_loss: 1.3603 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.76280\n",
      "Epoch 393/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0314 - acc: 0.6914 - val_loss: 1.2972 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.76280\n",
      "Epoch 394/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9964 - acc: 0.7004 - val_loss: 1.1727 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.76280\n",
      "Epoch 395/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0029 - acc: 0.6857 - val_loss: 1.2581 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.76280\n",
      "Epoch 396/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0074 - acc: 0.7025 - val_loss: 1.8556 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.76280\n",
      "Epoch 397/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0127 - acc: 0.6869 - val_loss: 1.2960 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.76280\n",
      "Epoch 398/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0228 - acc: 0.6989 - val_loss: 3.3260 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.76280\n",
      "Epoch 399/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0373 - acc: 0.6965 - val_loss: 1.2126 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.76280\n",
      "Epoch 400/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0253 - acc: 0.6839 - val_loss: 1.3408 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.76280\n",
      "Epoch 401/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0212 - acc: 0.6995 - val_loss: 1.3816 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.76280\n",
      "Epoch 402/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0319 - acc: 0.6863 - val_loss: 1.5352 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.76280\n",
      "Epoch 403/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0225 - acc: 0.6887 - val_loss: 1.3060 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.76280\n",
      "Epoch 404/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0405 - acc: 0.6815 - val_loss: 2.1520 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.76280\n",
      "Epoch 405/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0066 - acc: 0.6887 - val_loss: 2.0563 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.76280\n",
      "Epoch 406/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9992 - acc: 0.6959 - val_loss: 1.1097 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.76280\n",
      "Epoch 407/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0010 - acc: 0.7007 - val_loss: 1.3002 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.76280\n",
      "Epoch 408/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0208 - acc: 0.6878 - val_loss: 1.4678 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.76280\n",
      "Epoch 409/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0261 - acc: 0.6839 - val_loss: 1.2757 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.76280\n",
      "Epoch 410/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0255 - acc: 0.6917 - val_loss: 1.4862 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.76280\n",
      "Epoch 411/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 1.9870 - acc: 0.7019 - val_loss: 1.2593 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.76280\n",
      "Epoch 412/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9973 - acc: 0.7058 - val_loss: 1.8414 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.76280\n",
      "Epoch 413/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0019 - acc: 0.6992 - val_loss: 1.3700 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.76280\n",
      "Epoch 414/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0498 - acc: 0.6929 - val_loss: 2.2297 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.76280\n",
      "Epoch 415/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0027 - acc: 0.6944 - val_loss: 1.3769 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.76280\n",
      "Epoch 416/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0175 - acc: 0.7019 - val_loss: 1.3795 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.76280\n",
      "Epoch 417/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0191 - acc: 0.7001 - val_loss: 1.3292 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.76280\n",
      "Epoch 418/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0060 - acc: 0.6881 - val_loss: 1.8261 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.76280\n",
      "Epoch 419/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9899 - acc: 0.7124 - val_loss: 1.3580 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.76280\n",
      "Epoch 420/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0024 - acc: 0.7121 - val_loss: 1.2240 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.76280\n",
      "Epoch 421/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0078 - acc: 0.7001 - val_loss: 1.2621 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.76280\n",
      "Epoch 422/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 1.9638 - acc: 0.7142 - val_loss: 1.1693 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.76280\n",
      "Epoch 423/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9892 - acc: 0.6980 - val_loss: 1.2424 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.76280\n",
      "Epoch 424/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0104 - acc: 0.7025 - val_loss: 1.7787 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.76280\n",
      "Epoch 425/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9943 - acc: 0.7049 - val_loss: 1.2392 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.76280\n",
      "Epoch 426/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0130 - acc: 0.6875 - val_loss: 1.1697 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.76280\n",
      "Epoch 427/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9907 - acc: 0.7082 - val_loss: 1.4116 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.76280\n",
      "Epoch 428/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0145 - acc: 0.6932 - val_loss: 1.3013 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.76280\n",
      "Epoch 00428: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_7===\n",
      "semi loading: model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "53/53 [==============================] - 27s 518ms/step - loss: 2.1267 - acc: 0.6197 - val_loss: 0.9797 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.77898, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 6/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 2.0533 - acc: 0.6418 - val_loss: 0.9591 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77898 to 0.79515, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 7/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 2.0226 - acc: 0.6524 - val_loss: 0.9689 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79515\n",
      "Epoch 8/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 2.0111 - acc: 0.6613 - val_loss: 0.9563 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79515\n",
      "Epoch 9/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.9717 - acc: 0.6689 - val_loss: 0.9635 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79515\n",
      "Epoch 10/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9547 - acc: 0.6798 - val_loss: 0.9561 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79515\n",
      "Epoch 11/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.9442 - acc: 0.6860 - val_loss: 0.9463 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79515\n",
      "Epoch 12/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8982 - acc: 0.7034 - val_loss: 0.9333 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79515\n",
      "Epoch 13/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9299 - acc: 0.6919 - val_loss: 0.9534 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79515\n",
      "Epoch 14/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9074 - acc: 0.7052 - val_loss: 0.9265 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79515\n",
      "Epoch 15/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8732 - acc: 0.7140 - val_loss: 0.9284 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.79515 to 0.79784, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 16/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8879 - acc: 0.7046 - val_loss: 0.9206 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.79784 to 0.80863, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 17/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8827 - acc: 0.7084 - val_loss: 0.9205 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80863\n",
      "Epoch 18/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8787 - acc: 0.7202 - val_loss: 0.9126 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80863\n",
      "Epoch 19/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8625 - acc: 0.7155 - val_loss: 0.9320 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80863\n",
      "Epoch 20/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8696 - acc: 0.7084 - val_loss: 0.9121 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80863\n",
      "Epoch 21/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8683 - acc: 0.7102 - val_loss: 0.9506 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80863\n",
      "Epoch 22/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8543 - acc: 0.7182 - val_loss: 0.9295 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80863\n",
      "Epoch 23/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8786 - acc: 0.7008 - val_loss: 0.9328 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.80863\n",
      "Epoch 24/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8791 - acc: 0.7073 - val_loss: 0.9266 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80863\n",
      "Epoch 25/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8395 - acc: 0.7196 - val_loss: 0.9637 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80863\n",
      "Epoch 26/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8671 - acc: 0.7043 - val_loss: 0.9240 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.80863 to 0.81941, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 27/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8706 - acc: 0.7052 - val_loss: 0.9043 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.81941 to 0.82210, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 28/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8336 - acc: 0.7155 - val_loss: 0.9014 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82210\n",
      "Epoch 29/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8749 - acc: 0.7134 - val_loss: 0.9198 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82210\n",
      "Epoch 30/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8312 - acc: 0.7273 - val_loss: 0.9273 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82210\n",
      "Epoch 31/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8747 - acc: 0.7117 - val_loss: 0.9221 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82210\n",
      "Epoch 32/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8548 - acc: 0.7146 - val_loss: 0.9167 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82210\n",
      "Epoch 33/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8495 - acc: 0.7120 - val_loss: 0.9234 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82210\n",
      "Epoch 34/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8338 - acc: 0.7391 - val_loss: 0.9011 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82210\n",
      "Epoch 35/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8183 - acc: 0.7311 - val_loss: 0.8972 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82210\n",
      "Epoch 36/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8191 - acc: 0.7350 - val_loss: 0.9013 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82210\n",
      "Epoch 37/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8392 - acc: 0.7196 - val_loss: 0.9105 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82210\n",
      "Epoch 38/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8022 - acc: 0.7258 - val_loss: 0.9208 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82210\n",
      "Epoch 39/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8240 - acc: 0.7220 - val_loss: 0.8721 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.82210 to 0.82749, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 40/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8247 - acc: 0.7285 - val_loss: 0.8775 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82749\n",
      "Epoch 41/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8048 - acc: 0.7409 - val_loss: 0.9070 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82749\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7992 - acc: 0.7308 - val_loss: 0.8754 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82749\n",
      "Epoch 43/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8072 - acc: 0.7220 - val_loss: 0.8747 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82749\n",
      "Epoch 44/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7823 - acc: 0.7388 - val_loss: 0.8848 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82749\n",
      "Epoch 45/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7956 - acc: 0.7397 - val_loss: 0.8953 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.82749\n",
      "Epoch 46/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7985 - acc: 0.7314 - val_loss: 0.9047 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.82749\n",
      "Epoch 47/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8168 - acc: 0.7258 - val_loss: 0.9202 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.82749\n",
      "Epoch 48/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7934 - acc: 0.7341 - val_loss: 0.8974 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.82749\n",
      "Epoch 49/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7828 - acc: 0.7317 - val_loss: 0.8953 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.82749\n",
      "Epoch 50/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8262 - acc: 0.7258 - val_loss: 0.8918 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.82749\n",
      "Epoch 51/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8025 - acc: 0.7335 - val_loss: 0.9125 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.82749\n",
      "Epoch 52/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7802 - acc: 0.7320 - val_loss: 0.9361 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.82749\n",
      "Epoch 53/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7886 - acc: 0.7441 - val_loss: 0.9342 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.82749\n",
      "Epoch 54/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8013 - acc: 0.7388 - val_loss: 0.9089 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.82749\n",
      "Epoch 55/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8017 - acc: 0.7406 - val_loss: 0.8821 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.82749\n",
      "Epoch 56/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7591 - acc: 0.7450 - val_loss: 0.8948 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.82749\n",
      "Epoch 57/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7717 - acc: 0.7426 - val_loss: 0.9002 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.82749\n",
      "Epoch 58/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7772 - acc: 0.7361 - val_loss: 0.8925 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.82749\n",
      "Epoch 59/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7975 - acc: 0.7456 - val_loss: 0.8906 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.82749\n",
      "Epoch 60/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7736 - acc: 0.7426 - val_loss: 0.8994 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.82749\n",
      "Epoch 61/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7849 - acc: 0.7314 - val_loss: 0.8979 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.82749\n",
      "Epoch 62/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7652 - acc: 0.7397 - val_loss: 0.9094 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.82749\n",
      "Epoch 63/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7659 - acc: 0.7509 - val_loss: 0.8837 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.82749\n",
      "Epoch 64/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7604 - acc: 0.7527 - val_loss: 0.9070 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.82749\n",
      "Epoch 65/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7802 - acc: 0.7544 - val_loss: 0.9242 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.82749\n",
      "Epoch 66/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7830 - acc: 0.7394 - val_loss: 0.9050 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.82749\n",
      "Epoch 67/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7783 - acc: 0.7435 - val_loss: 0.9113 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.82749\n",
      "Epoch 68/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7315 - acc: 0.7577 - val_loss: 0.8870 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.82749\n",
      "Epoch 69/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7891 - acc: 0.7432 - val_loss: 0.8649 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.82749\n",
      "Epoch 70/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7532 - val_loss: 0.8730 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.82749\n",
      "Epoch 71/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7621 - acc: 0.7373 - val_loss: 0.8774 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.82749\n",
      "Epoch 72/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7605 - acc: 0.7423 - val_loss: 0.8764 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.82749\n",
      "Epoch 73/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7480 - acc: 0.7597 - val_loss: 0.8726 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.82749\n",
      "Epoch 74/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7509 - val_loss: 0.8837 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.82749\n",
      "Epoch 75/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7397 - acc: 0.7524 - val_loss: 0.8683 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.82749\n",
      "Epoch 76/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7290 - acc: 0.7515 - val_loss: 0.8799 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.82749\n",
      "Epoch 77/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7486 - acc: 0.7639 - val_loss: 0.8685 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.82749\n",
      "Epoch 78/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7425 - acc: 0.7412 - val_loss: 0.8716 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.82749\n",
      "Epoch 79/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7402 - acc: 0.7577 - val_loss: 0.8635 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.82749\n",
      "Epoch 80/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7547 - acc: 0.7482 - val_loss: 0.8707 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.82749\n",
      "Epoch 81/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7502 - acc: 0.7583 - val_loss: 0.8829 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.82749\n",
      "Epoch 82/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7666 - acc: 0.7394 - val_loss: 0.8918 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.82749\n",
      "Epoch 83/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7589 - acc: 0.7435 - val_loss: 0.8734 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.82749\n",
      "Epoch 84/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7455 - acc: 0.7553 - val_loss: 0.8799 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.82749\n",
      "Epoch 85/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7451 - acc: 0.7488 - val_loss: 0.8698 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.82749\n",
      "Epoch 86/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7731 - acc: 0.7409 - val_loss: 0.8817 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.82749\n",
      "Epoch 87/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7213 - acc: 0.7633 - val_loss: 0.8700 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.82749\n",
      "Epoch 88/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7565 - acc: 0.7538 - val_loss: 0.8613 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.82749\n",
      "Epoch 89/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7643 - acc: 0.7497 - val_loss: 0.8534 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.82749\n",
      "Epoch 90/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7603 - acc: 0.7438 - val_loss: 0.8608 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.82749\n",
      "Epoch 91/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7474 - acc: 0.7544 - val_loss: 0.8742 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.82749\n",
      "Epoch 92/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7140 - acc: 0.7550 - val_loss: 0.8692 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.82749\n",
      "Epoch 93/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7097 - acc: 0.7633 - val_loss: 0.8680 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.82749\n",
      "Epoch 94/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7477 - acc: 0.7482 - val_loss: 0.8672 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.82749\n",
      "Epoch 95/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7354 - acc: 0.7615 - val_loss: 0.8644 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.82749\n",
      "Epoch 96/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7575 - acc: 0.7512 - val_loss: 0.8610 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.82749\n",
      "Epoch 97/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7405 - acc: 0.7656 - val_loss: 0.8547 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.82749\n",
      "Epoch 98/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7186 - acc: 0.7597 - val_loss: 0.8532 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.82749\n",
      "Epoch 99/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7636 - acc: 0.7479 - val_loss: 0.8451 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.82749\n",
      "Epoch 100/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7462 - acc: 0.7400 - val_loss: 0.8537 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.82749\n",
      "Epoch 101/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7219 - acc: 0.7556 - val_loss: 0.8491 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.82749\n",
      "Epoch 102/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7317 - acc: 0.7506 - val_loss: 0.8390 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.82749\n",
      "Epoch 103/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7258 - acc: 0.7544 - val_loss: 0.8477 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82749\n",
      "Epoch 104/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7686 - acc: 0.7473 - val_loss: 0.8492 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82749\n",
      "Epoch 105/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7382 - acc: 0.7656 - val_loss: 0.8547 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82749\n",
      "Epoch 106/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7740 - acc: 0.7400 - val_loss: 0.8569 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.82749\n",
      "Epoch 107/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7508 - acc: 0.7518 - val_loss: 0.8542 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.82749\n",
      "Epoch 108/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7187 - acc: 0.7577 - val_loss: 0.8574 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.82749\n",
      "Epoch 109/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7114 - acc: 0.7503 - val_loss: 0.8631 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.82749\n",
      "Epoch 110/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7470 - acc: 0.7541 - val_loss: 0.8583 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.82749\n",
      "Epoch 111/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7191 - acc: 0.7509 - val_loss: 0.8544 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.82749\n",
      "Epoch 112/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7296 - acc: 0.7624 - val_loss: 0.8634 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.82749\n",
      "Epoch 113/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7369 - acc: 0.7562 - val_loss: 0.8670 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.82749\n",
      "Epoch 114/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7049 - acc: 0.7703 - val_loss: 0.8681 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.82749\n",
      "Epoch 115/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7250 - acc: 0.7597 - val_loss: 0.8659 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.82749\n",
      "Epoch 116/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7207 - acc: 0.7588 - val_loss: 0.8652 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.82749\n",
      "Epoch 117/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7229 - acc: 0.7568 - val_loss: 0.8616 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.82749\n",
      "Epoch 118/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7551 - acc: 0.7538 - val_loss: 0.8629 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.82749\n",
      "Epoch 119/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7361 - acc: 0.7556 - val_loss: 0.8613 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.82749\n",
      "Epoch 120/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7397 - acc: 0.7494 - val_loss: 0.8619 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.82749\n",
      "Epoch 121/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7434 - acc: 0.7485 - val_loss: 0.8605 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.82749\n",
      "Epoch 122/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7152 - acc: 0.7706 - val_loss: 0.8585 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.82749\n",
      "Epoch 123/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7156 - acc: 0.7509 - val_loss: 0.8571 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.82749\n",
      "Epoch 124/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7598 - acc: 0.7488 - val_loss: 0.8591 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.82749\n",
      "Epoch 125/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7264 - acc: 0.7515 - val_loss: 0.8568 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.82749\n",
      "Epoch 126/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7095 - acc: 0.7715 - val_loss: 0.8573 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.82749\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7182 - acc: 0.7583 - val_loss: 0.8605 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.82749\n",
      "Epoch 128/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7418 - acc: 0.7479 - val_loss: 0.8585 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.82749\n",
      "Epoch 129/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7143 - acc: 0.7647 - val_loss: 0.8583 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.82749\n",
      "Epoch 130/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7164 - acc: 0.7618 - val_loss: 0.8593 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.82749\n",
      "Epoch 131/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7514 - acc: 0.7506 - val_loss: 0.8583 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.82749\n",
      "Epoch 132/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7162 - acc: 0.7568 - val_loss: 0.8581 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.82749\n",
      "Epoch 133/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7220 - acc: 0.7624 - val_loss: 0.8615 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.82749\n",
      "Epoch 134/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7222 - acc: 0.7541 - val_loss: 0.8627 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.82749\n",
      "Epoch 135/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7290 - acc: 0.7562 - val_loss: 0.8580 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.82749\n",
      "Epoch 136/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7169 - acc: 0.7591 - val_loss: 0.8561 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.82749\n",
      "Epoch 137/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7146 - acc: 0.7553 - val_loss: 0.8564 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.82749\n",
      "Epoch 138/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7397 - acc: 0.7609 - val_loss: 0.8559 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.82749\n",
      "Epoch 139/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7438 - acc: 0.7571 - val_loss: 0.8579 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.82749\n",
      "Epoch 140/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7157 - acc: 0.7500 - val_loss: 0.8594 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.82749\n",
      "Epoch 141/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7360 - acc: 0.7653 - val_loss: 0.8583 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.82749\n",
      "Epoch 142/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7048 - acc: 0.7792 - val_loss: 0.8548 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.82749\n",
      "Epoch 143/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7193 - acc: 0.7597 - val_loss: 0.8547 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.82749\n",
      "Epoch 144/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7245 - acc: 0.7633 - val_loss: 0.8569 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.82749\n",
      "Epoch 145/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7385 - acc: 0.7538 - val_loss: 0.8593 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.82749\n",
      "Epoch 146/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7257 - acc: 0.7644 - val_loss: 0.8607 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.82749\n",
      "Epoch 147/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7240 - acc: 0.7689 - val_loss: 0.8562 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.82749\n",
      "Epoch 148/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7277 - acc: 0.7612 - val_loss: 0.8550 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.82749\n",
      "Epoch 149/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7341 - acc: 0.7473 - val_loss: 0.8553 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.82749\n",
      "Epoch 150/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7089 - acc: 0.7565 - val_loss: 0.8563 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.82749\n",
      "Epoch 151/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7311 - acc: 0.7594 - val_loss: 0.8572 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.82749\n",
      "Epoch 152/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7138 - acc: 0.7609 - val_loss: 0.8547 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.82749\n",
      "Epoch 153/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7235 - acc: 0.7497 - val_loss: 0.8549 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.82749\n",
      "Epoch 154/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7284 - acc: 0.7541 - val_loss: 0.8532 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.82749\n",
      "Epoch 155/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7450 - val_loss: 0.8541 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.82749\n",
      "Epoch 156/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7254 - acc: 0.7529 - val_loss: 0.8555 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.82749\n",
      "Epoch 157/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7137 - acc: 0.7618 - val_loss: 0.8559 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.82749\n",
      "Epoch 158/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7627 - val_loss: 0.8571 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.82749\n",
      "Epoch 159/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7133 - acc: 0.7668 - val_loss: 0.8594 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.82749\n",
      "Epoch 160/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7352 - acc: 0.7423 - val_loss: 0.8583 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.82749\n",
      "Epoch 161/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7383 - acc: 0.7577 - val_loss: 0.8568 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.82749\n",
      "Epoch 162/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7216 - acc: 0.7524 - val_loss: 0.8567 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.82749\n",
      "Epoch 163/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7222 - acc: 0.7612 - val_loss: 0.8542 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.82749\n",
      "Epoch 164/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7195 - acc: 0.7644 - val_loss: 0.8554 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.82749\n",
      "Epoch 165/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7487 - acc: 0.7503 - val_loss: 0.8550 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.82749\n",
      "Epoch 166/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7362 - acc: 0.7577 - val_loss: 0.8532 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.82749\n",
      "Epoch 167/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7072 - acc: 0.7636 - val_loss: 0.8545 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.82749\n",
      "Epoch 168/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7164 - acc: 0.7615 - val_loss: 0.8540 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.82749\n",
      "Epoch 169/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7367 - acc: 0.7544 - val_loss: 0.8554 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.82749\n",
      "Epoch 170/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7223 - acc: 0.7583 - val_loss: 0.8558 - val_acc: 0.8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00170: val_acc did not improve from 0.82749\n",
      "Epoch 171/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7306 - acc: 0.7553 - val_loss: 0.8538 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.82749\n",
      "Epoch 172/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7110 - acc: 0.7529 - val_loss: 0.8537 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.82749\n",
      "Epoch 173/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7321 - acc: 0.7541 - val_loss: 0.8532 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.82749\n",
      "Epoch 174/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7309 - acc: 0.7488 - val_loss: 0.8533 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.82749\n",
      "Epoch 175/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7115 - acc: 0.7594 - val_loss: 0.8533 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.82749\n",
      "Epoch 176/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7116 - acc: 0.7553 - val_loss: 0.8540 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.82749\n",
      "Epoch 177/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7025 - acc: 0.7627 - val_loss: 0.8541 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.82749\n",
      "Epoch 178/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.6960 - acc: 0.7706 - val_loss: 0.8542 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.82749\n",
      "Epoch 179/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7556 - val_loss: 0.8527 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.82749\n",
      "Epoch 180/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7218 - acc: 0.7553 - val_loss: 0.8537 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.82749\n",
      "Epoch 181/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7353 - acc: 0.7597 - val_loss: 0.8520 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.82749\n",
      "Epoch 182/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7105 - acc: 0.7585 - val_loss: 0.8536 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.82749\n",
      "Epoch 183/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7280 - acc: 0.7677 - val_loss: 0.8556 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.82749\n",
      "Epoch 184/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7316 - acc: 0.7580 - val_loss: 0.8555 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.82749\n",
      "Epoch 185/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 1.7276 - acc: 0.7571 - val_loss: 0.8585 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.82749\n",
      "Epoch 186/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7150 - acc: 0.7674 - val_loss: 0.8575 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.82749\n",
      "Epoch 187/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7562 - val_loss: 0.8561 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.82749\n",
      "Epoch 188/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7238 - acc: 0.7609 - val_loss: 0.8564 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.82749\n",
      "Epoch 189/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7324 - acc: 0.7644 - val_loss: 0.8585 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.82749\n",
      "Epoch 190/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7179 - acc: 0.7591 - val_loss: 0.8589 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.82749\n",
      "Epoch 191/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7318 - acc: 0.7527 - val_loss: 0.8584 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.82749\n",
      "Epoch 192/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7013 - acc: 0.7727 - val_loss: 0.8565 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.82749\n",
      "Epoch 193/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7010 - acc: 0.7709 - val_loss: 0.8545 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.82749\n",
      "Epoch 194/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7335 - acc: 0.7594 - val_loss: 0.8552 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.82749\n",
      "Epoch 195/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7068 - acc: 0.7680 - val_loss: 0.8564 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.82749\n",
      "Epoch 196/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7070 - acc: 0.7662 - val_loss: 0.8585 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.82749\n",
      "Epoch 197/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7226 - acc: 0.7686 - val_loss: 0.8588 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.82749\n",
      "Epoch 198/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7187 - acc: 0.7698 - val_loss: 0.8570 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.82749\n",
      "Epoch 199/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7319 - acc: 0.7591 - val_loss: 0.8573 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.82749\n",
      "Epoch 200/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7109 - acc: 0.7677 - val_loss: 0.8592 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.82749\n",
      "Epoch 201/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7107 - acc: 0.7674 - val_loss: 0.8598 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.82749\n",
      "Epoch 202/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7140 - acc: 0.7665 - val_loss: 0.8602 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.82749\n",
      "Epoch 00202: early stopping\n",
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold8_mfcc7===\n",
      "using resnet model: 3\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 30, 130, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 30, 130, 64)  256         conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 30, 130, 64)  0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 15, 65, 64)   0           activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 15, 65, 64)   256         conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 15, 65, 64)   0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 15, 65, 64)   36928       activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_511 (BatchN (None, 15, 65, 64)   256         conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 15, 65, 64)   0           batch_normalization_511[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 15, 65, 256)  16640       activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 15, 65, 256)  0           conv2d_540[0][0]                 \n",
      "                                                                 conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_512 (BatchN (None, 15, 65, 256)  1024        add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 15, 65, 256)  0           batch_normalization_512[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 15, 65, 64)   16448       activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_513 (BatchN (None, 15, 65, 64)   256         conv2d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 15, 65, 64)   0           batch_normalization_513[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 15, 65, 64)   36928       activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_514 (BatchN (None, 15, 65, 64)   256         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 15, 65, 64)   0           batch_normalization_514[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 15, 65, 256)  16640       activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 15, 65, 256)  0           add_187[0][0]                    \n",
      "                                                                 conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_515 (BatchN (None, 15, 65, 256)  1024        add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 15, 65, 256)  0           batch_normalization_515[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 15, 65, 64)   16448       activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_516 (BatchN (None, 15, 65, 64)   256         conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 15, 65, 64)   0           batch_normalization_516[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 15, 65, 64)   36928       activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_517 (BatchN (None, 15, 65, 64)   256         conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 15, 65, 64)   0           batch_normalization_517[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 15, 65, 256)  16640       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 15, 65, 256)  0           add_188[0][0]                    \n",
      "                                                                 conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_518 (BatchN (None, 15, 65, 256)  1024        add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 15, 65, 256)  0           batch_normalization_518[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 8, 33, 128)   32896       activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_519 (BatchN (None, 8, 33, 128)   512         conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 8, 33, 128)   0           batch_normalization_519[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 8, 33, 128)   147584      activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_520 (BatchN (None, 8, 33, 128)   512         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 8, 33, 128)   0           batch_normalization_520[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 8, 33, 512)   131584      add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 8, 33, 512)   66048       activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 8, 33, 512)   0           conv2d_550[0][0]                 \n",
      "                                                                 conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_521 (BatchN (None, 8, 33, 512)   2048        add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 8, 33, 512)   0           batch_normalization_521[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_551 (Conv2D)             (None, 8, 33, 128)   65664       activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_522 (BatchN (None, 8, 33, 128)   512         conv2d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 8, 33, 128)   0           batch_normalization_522[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 8, 33, 128)   147584      activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_523 (BatchN (None, 8, 33, 128)   512         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 8, 33, 128)   0           batch_normalization_523[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 8, 33, 512)   66048       activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 8, 33, 512)   0           add_190[0][0]                    \n",
      "                                                                 conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_524 (BatchN (None, 8, 33, 512)   2048        add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 8, 33, 512)   0           batch_normalization_524[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_554 (Conv2D)             (None, 8, 33, 128)   65664       activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_525 (BatchN (None, 8, 33, 128)   512         conv2d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 8, 33, 128)   0           batch_normalization_525[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 8, 33, 128)   147584      activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_526 (BatchN (None, 8, 33, 128)   512         conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 8, 33, 128)   0           batch_normalization_526[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 8, 33, 512)   66048       activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 8, 33, 512)   0           add_191[0][0]                    \n",
      "                                                                 conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_527 (BatchN (None, 8, 33, 512)   2048        add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 8, 33, 512)   0           batch_normalization_527[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 8, 33, 128)   65664       activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 8, 33, 128)   512         conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 8, 33, 128)   0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_558 (Conv2D)             (None, 8, 33, 128)   147584      activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 8, 33, 128)   512         conv2d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 8, 33, 128)   0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 8, 33, 512)   66048       activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 8, 33, 512)   0           add_192[0][0]                    \n",
      "                                                                 conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 8, 33, 512)   2048        add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 8, 33, 512)   0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 4, 17, 256)   131328      activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 4, 17, 256)   1024        conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 4, 17, 256)   0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_561 (Conv2D)             (None, 4, 17, 256)   590080      activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 4, 17, 256)   1024        conv2d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 4, 17, 256)   0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 4, 17, 1024)  525312      add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 4, 17, 1024)  263168      activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 4, 17, 1024)  0           conv2d_563[0][0]                 \n",
      "                                                                 conv2d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 4, 17, 1024)  4096        add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_564 (Conv2D)             (None, 4, 17, 256)   262400      activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 4, 17, 256)   1024        conv2d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 4, 17, 256)   0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 4, 17, 256)   590080      activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 4, 17, 256)   1024        conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 4, 17, 256)   0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 4, 17, 1024)  263168      activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 4, 17, 1024)  0           add_194[0][0]                    \n",
      "                                                                 conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 4, 17, 1024)  4096        add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_567 (Conv2D)             (None, 4, 17, 256)   262400      activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 4, 17, 256)   1024        conv2d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 4, 17, 256)   0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 4, 17, 256)   590080      activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 4, 17, 256)   1024        conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 4, 17, 256)   0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 4, 17, 1024)  263168      activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 4, 17, 1024)  0           add_195[0][0]                    \n",
      "                                                                 conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 4, 17, 1024)  4096        add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 4, 17, 256)   262400      activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 4, 17, 256)   1024        conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 4, 17, 256)   0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 4, 17, 256)   590080      activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 4, 17, 256)   1024        conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 4, 17, 256)   0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 4, 17, 1024)  263168      activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 4, 17, 1024)  0           add_196[0][0]                    \n",
      "                                                                 conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 4, 17, 1024)  4096        add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 4, 17, 256)   262400      activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 4, 17, 256)   1024        conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 4, 17, 256)   0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 4, 17, 256)   590080      activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 4, 17, 256)   1024        conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 4, 17, 256)   0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 4, 17, 1024)  263168      activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 4, 17, 1024)  0           add_197[0][0]                    \n",
      "                                                                 conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 4, 17, 1024)  4096        add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 4, 17, 256)   262400      activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 4, 17, 256)   1024        conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 4, 17, 256)   0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 4, 17, 256)   590080      activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 4, 17, 256)   1024        conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 4, 17, 256)   0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 4, 17, 1024)  263168      activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 4, 17, 1024)  0           add_198[0][0]                    \n",
      "                                                                 conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 4, 17, 1024)  4096        add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 4, 17, 256)   262400      activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 4, 17, 256)   1024        conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 4, 17, 256)   0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 4, 17, 256)   590080      activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 4, 17, 256)   1024        conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 4, 17, 256)   0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 4, 17, 1024)  263168      activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 4, 17, 1024)  0           add_199[0][0]                    \n",
      "                                                                 conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 4, 17, 1024)  4096        add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 4, 17, 256)   262400      activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 4, 17, 256)   1024        conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 4, 17, 256)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 4, 17, 256)   590080      activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 4, 17, 256)   1024        conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 4, 17, 256)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 4, 17, 1024)  263168      activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 4, 17, 1024)  0           add_200[0][0]                    \n",
      "                                                                 conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 4, 17, 1024)  4096        add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 4, 17, 256)   262400      activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 4, 17, 256)   1024        conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 4, 17, 256)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 4, 17, 256)   590080      activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 4, 17, 256)   1024        conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 4, 17, 256)   0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 4, 17, 1024)  263168      activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 4, 17, 1024)  0           add_201[0][0]                    \n",
      "                                                                 conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 4, 17, 1024)  4096        add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 4, 17, 256)   262400      activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 4, 17, 256)   1024        conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 4, 17, 256)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 4, 17, 256)   590080      activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 4, 17, 256)   1024        conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 4, 17, 256)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 4, 17, 1024)  263168      activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 4, 17, 1024)  0           add_202[0][0]                    \n",
      "                                                                 conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 4, 17, 1024)  4096        add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 4, 17, 256)   262400      activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 4, 17, 256)   1024        conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 4, 17, 256)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 4, 17, 256)   590080      activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 4, 17, 256)   1024        conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 4, 17, 256)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 4, 17, 1024)  263168      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 4, 17, 1024)  0           add_203[0][0]                    \n",
      "                                                                 conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 4, 17, 1024)  4096        add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 4, 17, 256)   262400      activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 4, 17, 256)   1024        conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 4, 17, 256)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 4, 17, 256)   590080      activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 4, 17, 256)   1024        conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 4, 17, 256)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 4, 17, 1024)  263168      activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 4, 17, 1024)  0           add_204[0][0]                    \n",
      "                                                                 conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 4, 17, 1024)  4096        add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 4, 17, 256)   262400      activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 4, 17, 256)   1024        conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 4, 17, 256)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 4, 17, 256)   590080      activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 4, 17, 256)   1024        conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 4, 17, 256)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 4, 17, 1024)  263168      activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 4, 17, 1024)  0           add_205[0][0]                    \n",
      "                                                                 conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 4, 17, 1024)  4096        add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 4, 17, 256)   262400      activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 4, 17, 256)   1024        conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 4, 17, 256)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 4, 17, 256)   590080      activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 4, 17, 256)   1024        conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 4, 17, 256)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 4, 17, 1024)  263168      activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 4, 17, 1024)  0           add_206[0][0]                    \n",
      "                                                                 conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 4, 17, 1024)  4096        add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 4, 17, 256)   262400      activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 4, 17, 256)   1024        conv2d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 4, 17, 256)   0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 4, 17, 256)   590080      activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 4, 17, 256)   1024        conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 4, 17, 256)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 4, 17, 1024)  263168      activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_208 (Add)                   (None, 4, 17, 1024)  0           add_207[0][0]                    \n",
      "                                                                 conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 4, 17, 1024)  4096        add_208[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 4, 17, 256)   262400      activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 4, 17, 256)   1024        conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 4, 17, 256)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 4, 17, 256)   590080      activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 4, 17, 256)   1024        conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 4, 17, 256)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 4, 17, 1024)  263168      activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_209 (Add)                   (None, 4, 17, 1024)  0           add_208[0][0]                    \n",
      "                                                                 conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 4, 17, 1024)  4096        add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 4, 17, 256)   262400      activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 4, 17, 256)   1024        conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 4, 17, 256)   0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 4, 17, 256)   590080      activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 4, 17, 256)   1024        conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 4, 17, 256)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 4, 17, 1024)  263168      activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_210 (Add)                   (None, 4, 17, 1024)  0           add_209[0][0]                    \n",
      "                                                                 conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 4, 17, 1024)  4096        add_210[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 4, 17, 256)   262400      activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 4, 17, 256)   1024        conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 4, 17, 256)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 4, 17, 256)   590080      activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 4, 17, 256)   1024        conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 4, 17, 256)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 4, 17, 1024)  263168      activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_211 (Add)                   (None, 4, 17, 1024)  0           add_210[0][0]                    \n",
      "                                                                 conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 4, 17, 1024)  4096        add_211[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 4, 17, 256)   262400      activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 4, 17, 256)   1024        conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 4, 17, 256)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 4, 17, 256)   590080      activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 4, 17, 256)   1024        conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 4, 17, 256)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 4, 17, 1024)  263168      activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_212 (Add)                   (None, 4, 17, 1024)  0           add_211[0][0]                    \n",
      "                                                                 conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 4, 17, 1024)  4096        add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 4, 17, 256)   262400      activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 4, 17, 256)   1024        conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 4, 17, 256)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 4, 17, 256)   590080      activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 4, 17, 256)   1024        conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 4, 17, 256)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 4, 17, 1024)  263168      activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_213 (Add)                   (None, 4, 17, 1024)  0           add_212[0][0]                    \n",
      "                                                                 conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 4, 17, 1024)  4096        add_213[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 4, 17, 256)   262400      activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 4, 17, 256)   1024        conv2d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 4, 17, 256)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 4, 17, 256)   590080      activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 4, 17, 256)   1024        conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 4, 17, 256)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 4, 17, 1024)  263168      activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_214 (Add)                   (None, 4, 17, 1024)  0           add_213[0][0]                    \n",
      "                                                                 conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 4, 17, 1024)  4096        add_214[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 4, 17, 256)   262400      activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 4, 17, 256)   1024        conv2d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 4, 17, 256)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 4, 17, 256)   590080      activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 4, 17, 256)   1024        conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 4, 17, 256)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 4, 17, 1024)  263168      activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_215 (Add)                   (None, 4, 17, 1024)  0           add_214[0][0]                    \n",
      "                                                                 conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 4, 17, 1024)  4096        add_215[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 4, 17, 256)   262400      activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 4, 17, 256)   1024        conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 4, 17, 256)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 4, 17, 256)   590080      activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 4, 17, 256)   1024        conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 4, 17, 256)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 4, 17, 1024)  263168      activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_216 (Add)                   (None, 4, 17, 1024)  0           add_215[0][0]                    \n",
      "                                                                 conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 4, 17, 1024)  4096        add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 2, 9, 512)    524800      activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 2, 9, 512)    2048        conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 2, 9, 512)    0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 2, 9, 512)    2359808     activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 2, 9, 512)    2048        conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 2, 9, 512)    0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 2, 9, 2048)   2099200     add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_217 (Add)                   (None, 2, 9, 2048)   0           conv2d_633[0][0]                 \n",
      "                                                                 conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 2, 9, 2048)   8192        add_217[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 2, 9, 512)    1049088     activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 2, 9, 512)    2048        conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 2, 9, 512)    0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 2, 9, 512)    2359808     activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 2, 9, 512)    2048        conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 2, 9, 512)    0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_218 (Add)                   (None, 2, 9, 2048)   0           add_217[0][0]                    \n",
      "                                                                 conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 2, 9, 2048)   8192        add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 2, 9, 512)    1049088     activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 2, 9, 512)    2048        conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 2, 9, 512)    0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 2, 9, 512)    2359808     activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 2, 9, 512)    2048        conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 2, 9, 512)    0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_219 (Add)                   (None, 2, 9, 2048)   0           add_218[0][0]                    \n",
      "                                                                 conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 2, 9, 2048)   8192        add_219[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 2048)   0           activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 53)           108597      flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 53)           212         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 53)           0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           2214        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,747,567\n",
      "Trainable params: 42,649,797\n",
      "Non-trainable params: 97,770\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "104/104 [==============================] - 32s 309ms/step - loss: 13.3760 - acc: 0.0484 - val_loss: 11.4375 - val_acc: 0.0970\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.09704, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 2/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 9.9622 - acc: 0.0787 - val_loss: 9.1653 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.09704\n",
      "Epoch 3/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 7.6242 - acc: 0.0802 - val_loss: 6.6265 - val_acc: 0.1078\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.09704 to 0.10782, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 4/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 6.1890 - acc: 0.0913 - val_loss: 5.6130 - val_acc: 0.1186\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.10782 to 0.11860, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 5.3541 - acc: 0.1007 - val_loss: 4.9472 - val_acc: 0.1024\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.11860\n",
      "Epoch 6/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 4.7893 - acc: 0.1142 - val_loss: 5.5962 - val_acc: 0.0566\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.11860\n",
      "Epoch 7/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 4.4489 - acc: 0.1259 - val_loss: 5.8441 - val_acc: 0.0404\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.11860\n",
      "Epoch 8/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 4.1622 - acc: 0.1478 - val_loss: 3.8388 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.11860 to 0.13747, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 9/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.9620 - acc: 0.1596 - val_loss: 4.5607 - val_acc: 0.1024\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.13747\n",
      "Epoch 10/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.8204 - acc: 0.1605 - val_loss: 3.1612 - val_acc: 0.2318\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.13747 to 0.23181, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 11/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.7270 - acc: 0.1749 - val_loss: 4.6787 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.23181\n",
      "Epoch 12/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.5893 - acc: 0.2001 - val_loss: 3.4752 - val_acc: 0.1995\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.23181\n",
      "Epoch 13/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.4940 - acc: 0.2109 - val_loss: 3.2462 - val_acc: 0.1941\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.23181\n",
      "Epoch 14/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 3.4775 - acc: 0.2004 - val_loss: 3.3004 - val_acc: 0.1833\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.23181\n",
      "Epoch 15/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.3974 - acc: 0.2218 - val_loss: 3.1354 - val_acc: 0.2399\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.23181 to 0.23989, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 16/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.3348 - acc: 0.2401 - val_loss: 3.2077 - val_acc: 0.2183\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.23989\n",
      "Epoch 17/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2869 - acc: 0.2503 - val_loss: 4.7257 - val_acc: 0.1752\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.23989\n",
      "Epoch 18/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2732 - acc: 0.2500 - val_loss: 2.9575 - val_acc: 0.2049\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.23989\n",
      "Epoch 19/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2107 - acc: 0.2587 - val_loss: 2.9994 - val_acc: 0.2695\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.23989 to 0.26954, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 20/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2043 - acc: 0.2662 - val_loss: 3.2953 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.26954\n",
      "Epoch 21/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2032 - acc: 0.2641 - val_loss: 3.4943 - val_acc: 0.2075\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.26954\n",
      "Epoch 22/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1336 - acc: 0.2791 - val_loss: 2.9502 - val_acc: 0.2264\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.26954\n",
      "Epoch 23/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1512 - acc: 0.2674 - val_loss: 2.6885 - val_acc: 0.2803\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.26954 to 0.28032, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 24/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1189 - acc: 0.2855 - val_loss: 2.6818 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.28032\n",
      "Epoch 25/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1224 - acc: 0.2906 - val_loss: 2.9823 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.28032\n",
      "Epoch 26/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0853 - acc: 0.2927 - val_loss: 3.2746 - val_acc: 0.2156\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.28032\n",
      "Epoch 27/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0823 - acc: 0.2909 - val_loss: 2.6944 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.28032 to 0.33423, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 28/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0586 - acc: 0.3074 - val_loss: 2.7699 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.33423\n",
      "Epoch 29/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0548 - acc: 0.2900 - val_loss: 2.6674 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.33423\n",
      "Epoch 30/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0622 - acc: 0.2990 - val_loss: 2.8039 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.33423\n",
      "Epoch 31/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0584 - acc: 0.3026 - val_loss: 2.6839 - val_acc: 0.3073\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.33423\n",
      "Epoch 32/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0092 - acc: 0.3224 - val_loss: 2.3437 - val_acc: 0.3693\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.33423 to 0.36927, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 33/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9971 - acc: 0.3149 - val_loss: 2.3258 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.36927 to 0.42049, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 34/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9981 - acc: 0.3233 - val_loss: 2.2911 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.42049\n",
      "Epoch 35/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0481 - acc: 0.3059 - val_loss: 2.4513 - val_acc: 0.3585\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.42049\n",
      "Epoch 36/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9874 - acc: 0.3203 - val_loss: 2.4716 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.42049\n",
      "Epoch 37/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9505 - acc: 0.3347 - val_loss: 2.4600 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.42049\n",
      "Epoch 38/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9482 - acc: 0.3425 - val_loss: 2.5134 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.42049\n",
      "Epoch 39/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9163 - acc: 0.3456 - val_loss: 2.1414 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.42049 to 0.44744, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 40/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9393 - acc: 0.3383 - val_loss: 2.7708 - val_acc: 0.3046\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.44744\n",
      "Epoch 41/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9414 - acc: 0.3447 - val_loss: 2.2969 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.44744\n",
      "Epoch 42/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9453 - acc: 0.3501 - val_loss: 2.1681 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.44744 to 0.45283, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 43/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8943 - acc: 0.3570 - val_loss: 2.2468 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.45283\n",
      "Epoch 44/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8965 - acc: 0.3567 - val_loss: 2.3220 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.45283\n",
      "Epoch 45/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8890 - acc: 0.3525 - val_loss: 2.3919 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.45283\n",
      "Epoch 46/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8549 - acc: 0.3645 - val_loss: 2.0684 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.45283\n",
      "Epoch 47/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8364 - acc: 0.3744 - val_loss: 2.0704 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.45283 to 0.46900, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 48/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8389 - acc: 0.3849 - val_loss: 2.7072 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.46900\n",
      "Epoch 49/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8553 - acc: 0.3753 - val_loss: 2.2492 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.46900\n",
      "Epoch 50/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8488 - acc: 0.3789 - val_loss: 2.7633 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.46900\n",
      "Epoch 51/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.8387 - acc: 0.3774 - val_loss: 2.3037 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.46900\n",
      "Epoch 52/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8298 - acc: 0.3804 - val_loss: 2.1228 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.46900\n",
      "Epoch 53/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7910 - acc: 0.3939 - val_loss: 2.1783 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.46900\n",
      "Epoch 54/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8107 - acc: 0.3852 - val_loss: 2.5295 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.46900\n",
      "Epoch 55/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7885 - acc: 0.3900 - val_loss: 2.2072 - val_acc: 0.4178\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.46900\n",
      "Epoch 56/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7815 - acc: 0.3996 - val_loss: 2.0184 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.46900\n",
      "Epoch 57/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8000 - acc: 0.3900 - val_loss: 2.1208 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.46900 to 0.49865, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 58/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7826 - acc: 0.3978 - val_loss: 2.2737 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.49865\n",
      "Epoch 59/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7821 - acc: 0.4090 - val_loss: 1.9779 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.49865\n",
      "Epoch 60/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7570 - acc: 0.4099 - val_loss: 2.0789 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.49865 to 0.51752, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 61/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7887 - acc: 0.4123 - val_loss: 1.9556 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.51752 to 0.52022, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 62/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.7375 - acc: 0.4117 - val_loss: 2.1493 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.52022\n",
      "Epoch 63/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7536 - acc: 0.4114 - val_loss: 1.9850 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.52022\n",
      "Epoch 64/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7636 - acc: 0.4099 - val_loss: 2.2549 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.52022\n",
      "Epoch 65/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7801 - acc: 0.4072 - val_loss: 2.1358 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.52022\n",
      "Epoch 66/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7637 - acc: 0.4168 - val_loss: 2.3267 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.52022\n",
      "Epoch 67/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7005 - acc: 0.4312 - val_loss: 1.8387 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.52022 to 0.55256, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 68/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7113 - acc: 0.4207 - val_loss: 1.9080 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.55256\n",
      "Epoch 69/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7122 - acc: 0.4198 - val_loss: 2.0377 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.55256\n",
      "Epoch 70/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6661 - acc: 0.4495 - val_loss: 2.4363 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.55256\n",
      "Epoch 71/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6911 - acc: 0.4282 - val_loss: 1.9419 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.55256\n",
      "Epoch 72/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6831 - acc: 0.4372 - val_loss: 1.7359 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.55256\n",
      "Epoch 73/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6814 - acc: 0.4471 - val_loss: 2.1874 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.55256\n",
      "Epoch 74/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6645 - acc: 0.4432 - val_loss: 1.7770 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.55256\n",
      "Epoch 75/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6698 - acc: 0.4354 - val_loss: 1.7869 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.55256\n",
      "Epoch 76/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6537 - acc: 0.4573 - val_loss: 1.7357 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.55256\n",
      "Epoch 77/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6562 - acc: 0.4534 - val_loss: 2.1362 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.55256\n",
      "Epoch 78/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6300 - acc: 0.4486 - val_loss: 2.3665 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.55256\n",
      "Epoch 79/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6350 - acc: 0.4609 - val_loss: 2.2681 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.55256\n",
      "Epoch 80/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6109 - acc: 0.4573 - val_loss: 1.8385 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.55256\n",
      "Epoch 81/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6460 - acc: 0.4516 - val_loss: 2.2295 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.55256\n",
      "Epoch 82/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6526 - acc: 0.4462 - val_loss: 2.5134 - val_acc: 0.3774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_acc did not improve from 0.55256\n",
      "Epoch 83/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6303 - acc: 0.4697 - val_loss: 1.6977 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.55256 to 0.59838, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 84/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6500 - acc: 0.4549 - val_loss: 2.3124 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.59838\n",
      "Epoch 85/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5910 - acc: 0.4681 - val_loss: 1.8620 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.59838\n",
      "Epoch 86/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6366 - acc: 0.4561 - val_loss: 1.6842 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.59838\n",
      "Epoch 87/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6128 - acc: 0.4513 - val_loss: 2.2236 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.59838\n",
      "Epoch 88/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.5987 - acc: 0.4742 - val_loss: 1.7575 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.59838\n",
      "Epoch 89/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6002 - acc: 0.4784 - val_loss: 1.8336 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.59838\n",
      "Epoch 90/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6001 - acc: 0.4709 - val_loss: 1.7059 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.59838\n",
      "Epoch 91/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5804 - acc: 0.4811 - val_loss: 1.8844 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.59838\n",
      "Epoch 92/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5650 - acc: 0.4904 - val_loss: 1.8378 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.59838\n",
      "Epoch 93/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5761 - acc: 0.4730 - val_loss: 2.1639 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.59838\n",
      "Epoch 94/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5775 - acc: 0.4793 - val_loss: 1.7873 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.59838\n",
      "Epoch 95/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5830 - acc: 0.4910 - val_loss: 2.1218 - val_acc: 0.4744\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.59838\n",
      "Epoch 96/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5818 - acc: 0.4784 - val_loss: 1.9914 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.59838\n",
      "Epoch 97/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5866 - acc: 0.4684 - val_loss: 1.7562 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.59838\n",
      "Epoch 98/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5884 - acc: 0.4742 - val_loss: 2.2572 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.59838\n",
      "Epoch 99/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.5573 - acc: 0.4877 - val_loss: 1.6709 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.59838\n",
      "Epoch 100/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5607 - acc: 0.4754 - val_loss: 1.5671 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.59838\n",
      "Epoch 101/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5263 - acc: 0.5039 - val_loss: 1.8124 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.59838\n",
      "Epoch 102/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5728 - acc: 0.4787 - val_loss: 2.1612 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.59838\n",
      "Epoch 103/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5491 - acc: 0.4901 - val_loss: 1.6900 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.59838\n",
      "Epoch 104/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5442 - acc: 0.4901 - val_loss: 1.7118 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.59838\n",
      "Epoch 105/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5523 - acc: 0.4853 - val_loss: 1.9534 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.59838\n",
      "Epoch 106/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5138 - acc: 0.5027 - val_loss: 1.6562 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.59838\n",
      "Epoch 107/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5309 - acc: 0.4937 - val_loss: 1.8808 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.59838\n",
      "Epoch 108/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5045 - acc: 0.5051 - val_loss: 1.8519 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.59838\n",
      "Epoch 109/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5118 - acc: 0.5027 - val_loss: 1.7220 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.59838\n",
      "Epoch 110/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4993 - acc: 0.5177 - val_loss: 1.8604 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.59838\n",
      "Epoch 111/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4908 - acc: 0.5072 - val_loss: 1.9922 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.59838\n",
      "Epoch 112/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5110 - acc: 0.5021 - val_loss: 2.0670 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.59838\n",
      "Epoch 113/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5004 - acc: 0.5051 - val_loss: 2.2915 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.59838\n",
      "Epoch 114/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5066 - acc: 0.5000 - val_loss: 2.7619 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.59838\n",
      "Epoch 115/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4747 - acc: 0.5198 - val_loss: 1.6341 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00115: val_acc improved from 0.59838 to 0.61725, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 116/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5105 - acc: 0.5063 - val_loss: 1.7404 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.61725\n",
      "Epoch 117/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4737 - acc: 0.5225 - val_loss: 1.8199 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.61725\n",
      "Epoch 118/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4736 - acc: 0.5129 - val_loss: 1.6340 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.61725\n",
      "Epoch 119/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4767 - acc: 0.5054 - val_loss: 1.8085 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.61725\n",
      "Epoch 120/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4654 - acc: 0.5243 - val_loss: 1.7456 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.61725\n",
      "Epoch 121/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.4591 - acc: 0.5270 - val_loss: 1.6399 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.61725\n",
      "Epoch 122/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4434 - acc: 0.5237 - val_loss: 1.5050 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.61725\n",
      "Epoch 123/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4724 - acc: 0.5246 - val_loss: 1.7150 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.61725\n",
      "Epoch 124/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4809 - acc: 0.5219 - val_loss: 1.6722 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.61725\n",
      "Epoch 125/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4376 - acc: 0.5309 - val_loss: 1.4753 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00125: val_acc improved from 0.61725 to 0.64420, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 126/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4627 - acc: 0.5120 - val_loss: 1.6153 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.64420\n",
      "Epoch 127/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4655 - acc: 0.5264 - val_loss: 1.7719 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.64420\n",
      "Epoch 128/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4472 - acc: 0.5279 - val_loss: 1.4644 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.64420 to 0.64420, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 129/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4598 - acc: 0.5237 - val_loss: 1.7405 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.64420\n",
      "Epoch 130/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4627 - acc: 0.5246 - val_loss: 1.6704 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.64420\n",
      "Epoch 131/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4403 - acc: 0.5319 - val_loss: 1.7909 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.64420\n",
      "Epoch 132/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4121 - acc: 0.5270 - val_loss: 1.5668 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.64420\n",
      "Epoch 133/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4541 - acc: 0.5174 - val_loss: 1.9243 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.64420\n",
      "Epoch 134/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4262 - acc: 0.5487 - val_loss: 1.8307 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.64420\n",
      "Epoch 135/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4329 - acc: 0.5276 - val_loss: 1.6366 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.64420\n",
      "Epoch 136/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4073 - acc: 0.5412 - val_loss: 1.4841 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.64420\n",
      "Epoch 137/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4255 - acc: 0.5225 - val_loss: 1.5846 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.64420\n",
      "Epoch 138/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3654 - acc: 0.5538 - val_loss: 1.6237 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.64420\n",
      "Epoch 139/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4329 - acc: 0.5379 - val_loss: 2.2759 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.64420\n",
      "Epoch 140/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4158 - acc: 0.5397 - val_loss: 1.6788 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.64420\n",
      "Epoch 141/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4370 - acc: 0.5349 - val_loss: 1.6345 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.64420\n",
      "Epoch 142/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4036 - acc: 0.5442 - val_loss: 1.5217 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.64420\n",
      "Epoch 143/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3661 - acc: 0.5475 - val_loss: 2.0611 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.64420\n",
      "Epoch 144/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4197 - acc: 0.5403 - val_loss: 1.5374 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.64420\n",
      "Epoch 145/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3897 - acc: 0.5535 - val_loss: 1.5892 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.64420\n",
      "Epoch 146/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4008 - acc: 0.5433 - val_loss: 1.4412 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.64420\n",
      "Epoch 147/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3975 - acc: 0.5388 - val_loss: 1.6185 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.64420\n",
      "Epoch 148/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3979 - acc: 0.5403 - val_loss: 1.7861 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.64420\n",
      "Epoch 149/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3726 - acc: 0.5502 - val_loss: 1.7012 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.64420\n",
      "Epoch 150/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3855 - acc: 0.5556 - val_loss: 1.6587 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.64420\n",
      "Epoch 151/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3877 - acc: 0.5520 - val_loss: 1.3957 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.64420 to 0.67385, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 152/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4067 - acc: 0.5466 - val_loss: 1.4484 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.67385\n",
      "Epoch 153/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3883 - acc: 0.5469 - val_loss: 1.8142 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.67385\n",
      "Epoch 154/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3978 - acc: 0.5490 - val_loss: 1.8574 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.67385\n",
      "Epoch 155/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3722 - acc: 0.5562 - val_loss: 1.6603 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.67385\n",
      "Epoch 156/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3682 - acc: 0.5589 - val_loss: 1.5980 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.67385\n",
      "Epoch 157/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3456 - acc: 0.5691 - val_loss: 1.5606 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.67385\n",
      "Epoch 158/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3325 - acc: 0.5691 - val_loss: 1.6084 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.67385\n",
      "Epoch 159/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3598 - acc: 0.5610 - val_loss: 1.4255 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.67385\n",
      "Epoch 160/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3641 - acc: 0.5559 - val_loss: 1.7017 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.67385\n",
      "Epoch 161/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3785 - acc: 0.5532 - val_loss: 1.4723 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.67385\n",
      "Epoch 162/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.3546 - acc: 0.5616 - val_loss: 1.5701 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.67385\n",
      "Epoch 163/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3595 - acc: 0.5655 - val_loss: 1.4137 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.67385\n",
      "Epoch 164/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3395 - acc: 0.5649 - val_loss: 1.6430 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.67385\n",
      "Epoch 165/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3576 - acc: 0.5688 - val_loss: 1.5863 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.67385\n",
      "Epoch 166/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3761 - acc: 0.5562 - val_loss: 1.3600 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.67385\n",
      "Epoch 167/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3472 - acc: 0.5625 - val_loss: 2.0388 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.67385\n",
      "Epoch 168/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3539 - acc: 0.5565 - val_loss: 1.9881 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.67385\n",
      "Epoch 169/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3492 - acc: 0.5697 - val_loss: 1.7569 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.67385\n",
      "Epoch 170/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3426 - acc: 0.5685 - val_loss: 1.5321 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.67385\n",
      "Epoch 171/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3592 - acc: 0.5583 - val_loss: 1.5958 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.67385\n",
      "Epoch 172/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3408 - acc: 0.5793 - val_loss: 1.3412 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.67385\n",
      "Epoch 173/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3319 - acc: 0.5721 - val_loss: 1.7396 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.67385\n",
      "Epoch 174/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3504 - acc: 0.5706 - val_loss: 1.4273 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.67385\n",
      "Epoch 175/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3202 - acc: 0.5886 - val_loss: 1.9042 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.67385\n",
      "Epoch 176/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3492 - acc: 0.5775 - val_loss: 1.4455 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.67385\n",
      "Epoch 177/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2987 - acc: 0.5796 - val_loss: 1.9618 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.67385\n",
      "Epoch 178/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3157 - acc: 0.5739 - val_loss: 1.5500 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.67385\n",
      "Epoch 179/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3214 - acc: 0.5808 - val_loss: 1.7937 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.67385\n",
      "Epoch 180/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3287 - acc: 0.5730 - val_loss: 1.5885 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.67385\n",
      "Epoch 181/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3112 - acc: 0.5754 - val_loss: 1.4034 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.67385\n",
      "Epoch 182/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3229 - acc: 0.5763 - val_loss: 1.3567 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.67385\n",
      "Epoch 183/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2804 - acc: 0.5962 - val_loss: 1.4140 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.67385\n",
      "Epoch 184/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.3090 - acc: 0.5829 - val_loss: 1.3772 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.67385\n",
      "Epoch 185/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.2822 - acc: 0.5892 - val_loss: 1.5597 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.67385\n",
      "Epoch 186/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2900 - acc: 0.6001 - val_loss: 1.5213 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.67385\n",
      "Epoch 187/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2821 - acc: 0.6088 - val_loss: 1.4611 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.67385\n",
      "Epoch 188/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3372 - acc: 0.5865 - val_loss: 1.6932 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.67385\n",
      "Epoch 189/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2809 - acc: 0.6001 - val_loss: 1.7212 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.67385\n",
      "Epoch 190/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3027 - acc: 0.5953 - val_loss: 1.6482 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.67385\n",
      "Epoch 191/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3096 - acc: 0.5895 - val_loss: 1.5095 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.67385\n",
      "Epoch 192/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3133 - acc: 0.5802 - val_loss: 1.4957 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.67385\n",
      "Epoch 193/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3010 - acc: 0.5859 - val_loss: 1.7213 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.67385\n",
      "Epoch 194/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2960 - acc: 0.5820 - val_loss: 2.0937 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.67385\n",
      "Epoch 195/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2813 - acc: 0.6025 - val_loss: 1.5650 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.67385\n",
      "Epoch 196/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2848 - acc: 0.5871 - val_loss: 1.8307 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.67385\n",
      "Epoch 197/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2899 - acc: 0.5934 - val_loss: 1.7104 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.67385\n",
      "Epoch 198/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2804 - acc: 0.5962 - val_loss: 1.4433 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.67385\n",
      "Epoch 199/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2646 - acc: 0.6022 - val_loss: 1.4124 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.67385\n",
      "Epoch 200/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2787 - acc: 0.6055 - val_loss: 1.4939 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.67385\n",
      "Epoch 201/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2714 - acc: 0.5944 - val_loss: 1.5784 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.67385\n",
      "Epoch 202/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2622 - acc: 0.5953 - val_loss: 1.7189 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.67385\n",
      "Epoch 203/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2461 - acc: 0.6070 - val_loss: 1.4696 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.67385\n",
      "Epoch 204/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2530 - acc: 0.6073 - val_loss: 1.4335 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.67385\n",
      "Epoch 205/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2618 - acc: 0.6001 - val_loss: 1.3803 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00205: val_acc improved from 0.67385 to 0.68733, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 206/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2849 - acc: 0.5962 - val_loss: 1.4073 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.68733\n",
      "Epoch 207/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2560 - acc: 0.6127 - val_loss: 1.3448 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00207: val_acc improved from 0.68733 to 0.69003, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 208/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2747 - acc: 0.5841 - val_loss: 1.4986 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.69003\n",
      "Epoch 209/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2692 - acc: 0.5995 - val_loss: 1.3139 - val_acc: 0.6954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00209: val_acc improved from 0.69003 to 0.69542, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 210/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2764 - acc: 0.6100 - val_loss: 1.4736 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.69542\n",
      "Epoch 211/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2535 - acc: 0.6016 - val_loss: 2.3240 - val_acc: 0.4636\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.69542\n",
      "Epoch 212/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2556 - acc: 0.6064 - val_loss: 1.3478 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.69542\n",
      "Epoch 213/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2493 - acc: 0.6004 - val_loss: 1.6621 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.69542\n",
      "Epoch 214/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2545 - acc: 0.6007 - val_loss: 1.6184 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.69542\n",
      "Epoch 215/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2463 - acc: 0.6085 - val_loss: 1.2520 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00215: val_acc improved from 0.69542 to 0.72776, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 216/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2635 - acc: 0.5856 - val_loss: 1.4411 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.72776\n",
      "Epoch 217/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2363 - acc: 0.6109 - val_loss: 1.2647 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.72776\n",
      "Epoch 218/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2549 - acc: 0.6061 - val_loss: 1.3616 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.72776\n",
      "Epoch 219/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2285 - acc: 0.6073 - val_loss: 1.7096 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.72776\n",
      "Epoch 220/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2337 - acc: 0.6091 - val_loss: 1.4332 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.72776\n",
      "Epoch 221/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2712 - acc: 0.5947 - val_loss: 1.5187 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.72776\n",
      "Epoch 222/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2955 - acc: 0.5868 - val_loss: 1.3979 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.72776\n",
      "Epoch 223/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2586 - acc: 0.6058 - val_loss: 1.2872 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.72776\n",
      "Epoch 224/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2392 - acc: 0.6181 - val_loss: 1.7485 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.72776\n",
      "Epoch 225/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2473 - acc: 0.6172 - val_loss: 1.6020 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.72776\n",
      "Epoch 226/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2375 - acc: 0.6292 - val_loss: 1.5538 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.72776\n",
      "Epoch 227/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2173 - acc: 0.6181 - val_loss: 1.3546 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.72776\n",
      "Epoch 228/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2321 - acc: 0.6181 - val_loss: 1.5280 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.72776\n",
      "Epoch 229/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2259 - acc: 0.6181 - val_loss: 1.4718 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.72776\n",
      "Epoch 230/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1983 - acc: 0.6262 - val_loss: 1.3121 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.72776\n",
      "Epoch 231/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2258 - acc: 0.6259 - val_loss: 1.3045 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.72776\n",
      "Epoch 232/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2226 - acc: 0.6061 - val_loss: 1.6657 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.72776\n",
      "Epoch 233/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2081 - acc: 0.6358 - val_loss: 1.4914 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.72776\n",
      "Epoch 234/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2048 - acc: 0.6214 - val_loss: 2.0802 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.72776\n",
      "Epoch 235/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2359 - acc: 0.6274 - val_loss: 1.3152 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.72776\n",
      "Epoch 236/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1950 - acc: 0.6334 - val_loss: 1.5774 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.72776\n",
      "Epoch 237/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2234 - acc: 0.6133 - val_loss: 1.3851 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.72776\n",
      "Epoch 238/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2532 - acc: 0.6130 - val_loss: 1.3604 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.72776\n",
      "Epoch 239/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1984 - acc: 0.6292 - val_loss: 1.7289 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.72776\n",
      "Epoch 240/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2050 - acc: 0.6172 - val_loss: 1.3640 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.72776\n",
      "Epoch 241/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2111 - acc: 0.6223 - val_loss: 1.3221 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.72776\n",
      "Epoch 242/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2046 - acc: 0.6199 - val_loss: 1.5423 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.72776\n",
      "Epoch 243/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1834 - acc: 0.6349 - val_loss: 1.3481 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.72776\n",
      "Epoch 244/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2349 - acc: 0.6187 - val_loss: 1.3277 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.72776\n",
      "Epoch 245/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1966 - acc: 0.6289 - val_loss: 1.2797 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.72776\n",
      "Epoch 246/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2105 - acc: 0.6271 - val_loss: 1.3501 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.72776\n",
      "Epoch 247/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2060 - acc: 0.6358 - val_loss: 1.4767 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.72776\n",
      "Epoch 248/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2105 - acc: 0.6217 - val_loss: 1.4602 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.72776\n",
      "Epoch 249/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2025 - acc: 0.6367 - val_loss: 1.4184 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.72776\n",
      "Epoch 250/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1951 - acc: 0.6346 - val_loss: 1.5264 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.72776\n",
      "Epoch 251/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2079 - acc: 0.6277 - val_loss: 1.2014 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00251: val_acc improved from 0.72776 to 0.74663, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2047 - acc: 0.6244 - val_loss: 1.8090 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.74663\n",
      "Epoch 253/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1916 - acc: 0.6361 - val_loss: 1.4803 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.74663\n",
      "Epoch 254/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1622 - acc: 0.6535 - val_loss: 1.4311 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.74663\n",
      "Epoch 255/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2302 - acc: 0.6277 - val_loss: 1.7178 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.74663\n",
      "Epoch 256/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2045 - acc: 0.6325 - val_loss: 1.4554 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.74663\n",
      "Epoch 257/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1558 - acc: 0.6388 - val_loss: 1.2614 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.74663\n",
      "Epoch 258/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1897 - acc: 0.6406 - val_loss: 1.6408 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.74663\n",
      "Epoch 259/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1572 - acc: 0.6502 - val_loss: 1.1478 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00259: val_acc improved from 0.74663 to 0.76011, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 260/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1553 - acc: 0.6385 - val_loss: 1.3442 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.76011\n",
      "Epoch 261/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1863 - acc: 0.6307 - val_loss: 1.2407 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.76011\n",
      "Epoch 262/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1902 - acc: 0.6421 - val_loss: 1.4609 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.76011\n",
      "Epoch 263/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1632 - acc: 0.6451 - val_loss: 1.5094 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.76011\n",
      "Epoch 264/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1886 - acc: 0.6448 - val_loss: 1.4353 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.76011\n",
      "Epoch 265/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1637 - acc: 0.6340 - val_loss: 1.2087 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.76011\n",
      "Epoch 266/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1879 - acc: 0.6358 - val_loss: 1.5911 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.76011\n",
      "Epoch 267/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1717 - acc: 0.6424 - val_loss: 1.4522 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.76011\n",
      "Epoch 268/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1717 - acc: 0.6382 - val_loss: 1.3617 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.76011\n",
      "Epoch 269/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1804 - acc: 0.6346 - val_loss: 1.4805 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.76011\n",
      "Epoch 270/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1786 - acc: 0.6361 - val_loss: 1.4846 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.76011\n",
      "Epoch 271/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1829 - acc: 0.6385 - val_loss: 1.8227 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.76011\n",
      "Epoch 272/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.1884 - acc: 0.6268 - val_loss: 1.4238 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.76011\n",
      "Epoch 273/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1675 - acc: 0.6406 - val_loss: 1.2447 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.76011\n",
      "Epoch 274/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1726 - acc: 0.6352 - val_loss: 1.3208 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.76011\n",
      "Epoch 275/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1793 - acc: 0.6355 - val_loss: 1.3380 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.76011\n",
      "Epoch 276/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1629 - acc: 0.6484 - val_loss: 1.2901 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.76011\n",
      "Epoch 277/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1930 - acc: 0.6355 - val_loss: 1.3354 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.76011\n",
      "Epoch 278/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1479 - acc: 0.6472 - val_loss: 1.3855 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.76011\n",
      "Epoch 279/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1647 - acc: 0.6400 - val_loss: 1.2017 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.76011\n",
      "Epoch 280/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1647 - acc: 0.6436 - val_loss: 1.3905 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.76011\n",
      "Epoch 281/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1579 - acc: 0.6559 - val_loss: 1.3379 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.76011\n",
      "Epoch 282/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1451 - acc: 0.6572 - val_loss: 1.5121 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.76011\n",
      "Epoch 283/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1522 - acc: 0.6520 - val_loss: 1.4511 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.76011\n",
      "Epoch 284/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1637 - acc: 0.6505 - val_loss: 1.2920 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.76011\n",
      "Epoch 285/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1820 - acc: 0.6457 - val_loss: 1.4288 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.76011\n",
      "Epoch 286/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1396 - acc: 0.6547 - val_loss: 1.7723 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.76011\n",
      "Epoch 287/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1521 - acc: 0.6490 - val_loss: 1.3949 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.76011\n",
      "Epoch 288/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1362 - acc: 0.6617 - val_loss: 1.3355 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.76011\n",
      "Epoch 289/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1579 - acc: 0.6517 - val_loss: 1.5759 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.76011\n",
      "Epoch 290/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1646 - acc: 0.6484 - val_loss: 1.5075 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.76011\n",
      "Epoch 291/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1563 - acc: 0.6448 - val_loss: 1.7078 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.76011\n",
      "Epoch 292/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1610 - acc: 0.6553 - val_loss: 1.3442 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.76011\n",
      "Epoch 293/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1462 - acc: 0.6478 - val_loss: 1.5131 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.76011\n",
      "Epoch 294/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1533 - acc: 0.6511 - val_loss: 1.4763 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.76011\n",
      "Epoch 295/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1325 - acc: 0.6538 - val_loss: 1.3034 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.76011\n",
      "Epoch 296/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1514 - acc: 0.6517 - val_loss: 1.4901 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.76011\n",
      "Epoch 297/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1486 - acc: 0.6508 - val_loss: 1.2521 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.76011\n",
      "Epoch 298/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1092 - acc: 0.6737 - val_loss: 1.2174 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.76011\n",
      "Epoch 299/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1451 - acc: 0.6575 - val_loss: 1.3106 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.76011\n",
      "Epoch 300/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1198 - acc: 0.6526 - val_loss: 1.4113 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.76011\n",
      "Epoch 301/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1467 - acc: 0.6475 - val_loss: 1.3549 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.76011\n",
      "Epoch 302/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1746 - acc: 0.6505 - val_loss: 1.5140 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.76011\n",
      "Epoch 303/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1209 - acc: 0.6650 - val_loss: 1.3698 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.76011\n",
      "Epoch 304/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1423 - acc: 0.6599 - val_loss: 1.1795 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.76011\n",
      "Epoch 305/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1125 - acc: 0.6656 - val_loss: 1.4738 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.76011\n",
      "Epoch 306/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1364 - acc: 0.6575 - val_loss: 1.4117 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.76011\n",
      "Epoch 307/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1225 - acc: 0.6746 - val_loss: 1.4611 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.76011\n",
      "Epoch 308/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1443 - acc: 0.6569 - val_loss: 1.7039 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.76011\n",
      "Epoch 309/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1499 - acc: 0.6478 - val_loss: 1.7642 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.76011\n",
      "Epoch 310/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1310 - acc: 0.6505 - val_loss: 1.3140 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.76011\n",
      "Epoch 311/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1217 - acc: 0.6749 - val_loss: 1.3427 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.76011\n",
      "Epoch 312/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1061 - acc: 0.6692 - val_loss: 1.4923 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.76011\n",
      "Epoch 313/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1230 - acc: 0.6508 - val_loss: 1.1707 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.76011\n",
      "Epoch 314/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1107 - acc: 0.6556 - val_loss: 1.5693 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.76011\n",
      "Epoch 315/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1032 - acc: 0.6638 - val_loss: 1.5349 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.76011\n",
      "Epoch 316/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1220 - acc: 0.6701 - val_loss: 1.3265 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.76011\n",
      "Epoch 317/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1363 - acc: 0.6623 - val_loss: 1.6065 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.76011\n",
      "Epoch 318/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0939 - acc: 0.6707 - val_loss: 1.4451 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.76011\n",
      "Epoch 319/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1406 - acc: 0.6674 - val_loss: 1.1805 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.76011\n",
      "Epoch 320/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1047 - acc: 0.6788 - val_loss: 1.4164 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.76011\n",
      "Epoch 321/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1041 - acc: 0.6635 - val_loss: 1.3584 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.76011\n",
      "Epoch 322/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1070 - acc: 0.6662 - val_loss: 1.4774 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.76011\n",
      "Epoch 323/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1350 - acc: 0.6635 - val_loss: 1.1704 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.76011\n",
      "Epoch 324/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0794 - acc: 0.6881 - val_loss: 1.7919 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.76011\n",
      "Epoch 325/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1198 - acc: 0.6665 - val_loss: 1.3744 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.76011\n",
      "Epoch 326/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0942 - acc: 0.6764 - val_loss: 1.3311 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.76011\n",
      "Epoch 327/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1021 - acc: 0.6593 - val_loss: 1.3542 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.76011\n",
      "Epoch 328/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1053 - acc: 0.6668 - val_loss: 1.2933 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.76011\n",
      "Epoch 329/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0887 - acc: 0.6701 - val_loss: 1.4988 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.76011\n",
      "Epoch 330/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0779 - acc: 0.6875 - val_loss: 1.2454 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.76011\n",
      "Epoch 331/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1130 - acc: 0.6556 - val_loss: 1.3850 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.76011\n",
      "Epoch 332/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1172 - acc: 0.6569 - val_loss: 1.4160 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.76011\n",
      "Epoch 333/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1219 - acc: 0.6707 - val_loss: 1.3073 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.76011\n",
      "Epoch 334/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0760 - acc: 0.6731 - val_loss: 1.1290 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00334: val_acc improved from 0.76011 to 0.76550, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 335/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1130 - acc: 0.6680 - val_loss: 1.3220 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.76550\n",
      "Epoch 336/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0966 - acc: 0.6761 - val_loss: 1.3612 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.76550\n",
      "Epoch 337/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1091 - acc: 0.6797 - val_loss: 1.3348 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.76550\n",
      "Epoch 338/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1050 - acc: 0.6746 - val_loss: 1.3901 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.76550\n",
      "Epoch 339/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0847 - acc: 0.6677 - val_loss: 1.2867 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.76550\n",
      "Epoch 340/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0691 - acc: 0.6764 - val_loss: 1.3505 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.76550\n",
      "Epoch 341/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0988 - acc: 0.6746 - val_loss: 1.2817 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.76550\n",
      "Epoch 342/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0656 - acc: 0.6791 - val_loss: 1.4005 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.76550\n",
      "Epoch 343/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1029 - acc: 0.6755 - val_loss: 1.3200 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.76550\n",
      "Epoch 344/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0679 - acc: 0.6875 - val_loss: 1.3214 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.76550\n",
      "Epoch 345/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0998 - acc: 0.6602 - val_loss: 1.5151 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.76550\n",
      "Epoch 346/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0910 - acc: 0.6695 - val_loss: 1.1996 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.76550\n",
      "Epoch 347/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0914 - acc: 0.6677 - val_loss: 1.2809 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.76550\n",
      "Epoch 348/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0742 - acc: 0.6818 - val_loss: 1.5380 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.76550\n",
      "Epoch 349/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1074 - acc: 0.6818 - val_loss: 1.5222 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.76550\n",
      "Epoch 350/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0830 - acc: 0.6839 - val_loss: 1.5995 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.76550\n",
      "Epoch 351/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0749 - acc: 0.6815 - val_loss: 1.5206 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.76550\n",
      "Epoch 352/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0870 - acc: 0.6782 - val_loss: 1.2385 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.76550\n",
      "Epoch 353/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0790 - acc: 0.6770 - val_loss: 1.4381 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.76550\n",
      "Epoch 354/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0961 - acc: 0.6764 - val_loss: 1.8364 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.76550\n",
      "Epoch 355/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0828 - acc: 0.6797 - val_loss: 1.5120 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.76550\n",
      "Epoch 356/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1388 - acc: 0.6614 - val_loss: 1.2597 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.76550\n",
      "Epoch 357/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0384 - acc: 0.6944 - val_loss: 1.1992 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.76550\n",
      "Epoch 358/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0714 - acc: 0.6896 - val_loss: 1.3367 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.76550\n",
      "Epoch 359/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0661 - acc: 0.6920 - val_loss: 1.3931 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.76550\n",
      "Epoch 360/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0616 - acc: 0.6923 - val_loss: 1.2357 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.76550\n",
      "Epoch 361/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0713 - acc: 0.6842 - val_loss: 1.1424 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00361: val_acc improved from 0.76550 to 0.77358, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 362/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0629 - acc: 0.7013 - val_loss: 1.6032 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.77358\n",
      "Epoch 363/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0507 - acc: 0.6872 - val_loss: 1.1277 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.77358\n",
      "Epoch 364/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0387 - acc: 0.6974 - val_loss: 1.3545 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.77358\n",
      "Epoch 365/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0872 - acc: 0.6737 - val_loss: 1.6273 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.77358\n",
      "Epoch 366/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0666 - acc: 0.6773 - val_loss: 1.3695 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.77358\n",
      "Epoch 367/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0619 - acc: 0.6791 - val_loss: 1.5065 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.77358\n",
      "Epoch 368/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0455 - acc: 0.6965 - val_loss: 1.4619 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.77358\n",
      "Epoch 369/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0917 - acc: 0.6764 - val_loss: 1.7090 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.77358\n",
      "Epoch 370/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0755 - acc: 0.6857 - val_loss: 1.1353 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.77358\n",
      "Epoch 371/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0907 - acc: 0.6800 - val_loss: 1.6262 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.77358\n",
      "Epoch 372/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0421 - acc: 0.7010 - val_loss: 1.3477 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.77358\n",
      "Epoch 373/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0454 - acc: 0.6890 - val_loss: 1.3802 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.77358\n",
      "Epoch 374/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0816 - acc: 0.6716 - val_loss: 1.6592 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.77358\n",
      "Epoch 375/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0650 - acc: 0.6890 - val_loss: 1.6290 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.77358\n",
      "Epoch 376/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0481 - acc: 0.6893 - val_loss: 1.5552 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.77358\n",
      "Epoch 377/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0421 - acc: 0.6965 - val_loss: 1.3053 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.77358\n",
      "Epoch 378/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0449 - acc: 0.6905 - val_loss: 1.4365 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.77358\n",
      "Epoch 379/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0518 - acc: 0.6743 - val_loss: 1.2428 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.77358\n",
      "Epoch 380/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0445 - acc: 0.6848 - val_loss: 1.3195 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.77358\n",
      "Epoch 381/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0470 - acc: 0.6923 - val_loss: 1.4699 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.77358\n",
      "Epoch 382/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0881 - acc: 0.6788 - val_loss: 1.5779 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.77358\n",
      "Epoch 383/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0749 - acc: 0.6971 - val_loss: 1.4672 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.77358\n",
      "Epoch 384/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0600 - acc: 0.6953 - val_loss: 1.4049 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.77358\n",
      "Epoch 385/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0634 - acc: 0.6848 - val_loss: 1.3446 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.77358\n",
      "Epoch 386/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0518 - acc: 0.6923 - val_loss: 1.3297 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.77358\n",
      "Epoch 387/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0437 - acc: 0.6866 - val_loss: 1.4644 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.77358\n",
      "Epoch 388/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0570 - acc: 0.6890 - val_loss: 1.3059 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.77358\n",
      "Epoch 389/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0447 - acc: 0.6962 - val_loss: 1.4577 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.77358\n",
      "Epoch 390/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0629 - acc: 0.6857 - val_loss: 1.1393 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.77358\n",
      "Epoch 391/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0166 - acc: 0.7028 - val_loss: 1.2743 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.77358\n",
      "Epoch 392/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0635 - acc: 0.6887 - val_loss: 1.3475 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.77358\n",
      "Epoch 393/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0494 - acc: 0.6887 - val_loss: 1.1838 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00393: val_acc improved from 0.77358 to 0.77898, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 394/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0587 - acc: 0.6965 - val_loss: 1.4799 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.77898\n",
      "Epoch 395/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0482 - acc: 0.6932 - val_loss: 1.3315 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.77898\n",
      "Epoch 396/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0640 - acc: 0.6761 - val_loss: 1.6589 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.77898\n",
      "Epoch 397/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0909 - acc: 0.6767 - val_loss: 1.3222 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.77898\n",
      "Epoch 398/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0521 - acc: 0.6935 - val_loss: 1.4973 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.77898\n",
      "Epoch 399/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0587 - acc: 0.6908 - val_loss: 1.3065 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.77898\n",
      "Epoch 400/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0572 - acc: 0.6908 - val_loss: 1.2761 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.77898\n",
      "Epoch 401/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0123 - acc: 0.7094 - val_loss: 1.1416 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.77898\n",
      "Epoch 402/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0405 - acc: 0.6923 - val_loss: 1.2749 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.77898\n",
      "Epoch 403/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0246 - acc: 0.6986 - val_loss: 1.5348 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.77898\n",
      "Epoch 404/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0862 - acc: 0.6884 - val_loss: 1.7070 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.77898\n",
      "Epoch 405/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0469 - acc: 0.6995 - val_loss: 2.0891 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.77898\n",
      "Epoch 406/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0569 - acc: 0.6998 - val_loss: 1.3283 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.77898\n",
      "Epoch 407/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0110 - acc: 0.6950 - val_loss: 1.3723 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.77898\n",
      "Epoch 408/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0426 - acc: 0.6998 - val_loss: 1.3014 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.77898\n",
      "Epoch 409/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0501 - acc: 0.6899 - val_loss: 1.2723 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.77898\n",
      "Epoch 410/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0319 - acc: 0.7166 - val_loss: 1.2477 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.77898\n",
      "Epoch 411/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0427 - acc: 0.6929 - val_loss: 1.4089 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.77898\n",
      "Epoch 412/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0387 - acc: 0.7010 - val_loss: 1.3519 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.77898\n",
      "Epoch 413/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0507 - acc: 0.6962 - val_loss: 1.4193 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.77898\n",
      "Epoch 414/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0473 - acc: 0.7007 - val_loss: 1.4665 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.77898\n",
      "Epoch 415/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0517 - acc: 0.6926 - val_loss: 1.2426 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.77898\n",
      "Epoch 416/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0563 - acc: 0.7085 - val_loss: 1.2163 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.77898\n",
      "Epoch 417/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0433 - acc: 0.6977 - val_loss: 1.3318 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.77898\n",
      "Epoch 418/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.0097 - acc: 0.6983 - val_loss: 1.3255 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.77898\n",
      "Epoch 419/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0446 - acc: 0.6908 - val_loss: 1.2149 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.77898\n",
      "Epoch 420/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0133 - acc: 0.7016 - val_loss: 1.4827 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.77898\n",
      "Epoch 421/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0228 - acc: 0.6959 - val_loss: 1.4929 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.77898\n",
      "Epoch 422/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0348 - acc: 0.7070 - val_loss: 1.7366 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.77898\n",
      "Epoch 423/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0305 - acc: 0.6965 - val_loss: 1.7042 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.77898\n",
      "Epoch 424/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0198 - acc: 0.7058 - val_loss: 1.2009 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.77898\n",
      "Epoch 425/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0204 - acc: 0.7148 - val_loss: 1.6524 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.77898\n",
      "Epoch 426/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0210 - acc: 0.7052 - val_loss: 1.8983 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.77898\n",
      "Epoch 427/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0576 - acc: 0.6950 - val_loss: 2.1613 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.77898\n",
      "Epoch 428/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0394 - acc: 0.6863 - val_loss: 1.3578 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.77898\n",
      "Epoch 429/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0537 - acc: 0.6926 - val_loss: 1.4100 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.77898\n",
      "Epoch 430/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0426 - acc: 0.6983 - val_loss: 1.5267 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.77898\n",
      "Epoch 431/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0409 - acc: 0.7079 - val_loss: 1.4513 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.77898\n",
      "Epoch 432/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0584 - acc: 0.6896 - val_loss: 1.3273 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.77898\n",
      "Epoch 433/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0333 - acc: 0.6959 - val_loss: 1.4475 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.77898\n",
      "Epoch 434/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0245 - acc: 0.7019 - val_loss: 1.4843 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.77898\n",
      "Epoch 435/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0437 - acc: 0.7010 - val_loss: 1.2524 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.77898\n",
      "Epoch 436/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0045 - acc: 0.7184 - val_loss: 1.2109 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.77898\n",
      "Epoch 437/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9980 - acc: 0.7094 - val_loss: 1.2228 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.77898\n",
      "Epoch 438/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0366 - acc: 0.7079 - val_loss: 1.4893 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.77898\n",
      "Epoch 439/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0442 - acc: 0.6863 - val_loss: 1.4190 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.77898\n",
      "Epoch 440/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0350 - acc: 0.6989 - val_loss: 1.2536 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.77898\n",
      "Epoch 441/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9997 - acc: 0.7197 - val_loss: 1.3241 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.77898\n",
      "Epoch 442/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0143 - acc: 0.7016 - val_loss: 1.3494 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.77898\n",
      "Epoch 443/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0187 - acc: 0.7112 - val_loss: 1.2841 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.77898\n",
      "Epoch 444/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0186 - acc: 0.7061 - val_loss: 1.5130 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.77898\n",
      "Epoch 445/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0413 - acc: 0.7085 - val_loss: 1.2981 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.77898\n",
      "Epoch 446/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0204 - acc: 0.7079 - val_loss: 1.4171 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.77898\n",
      "Epoch 447/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9694 - acc: 0.7284 - val_loss: 1.5132 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.77898\n",
      "Epoch 448/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0207 - acc: 0.6914 - val_loss: 1.6377 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.77898\n",
      "Epoch 449/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0307 - acc: 0.7037 - val_loss: 1.5491 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.77898\n",
      "Epoch 450/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0213 - acc: 0.6983 - val_loss: 1.3336 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.77898\n",
      "Epoch 451/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0207 - acc: 0.7085 - val_loss: 1.3609 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.77898\n",
      "Epoch 452/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0134 - acc: 0.7184 - val_loss: 1.2364 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.77898\n",
      "Epoch 453/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7067 - val_loss: 1.4391 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.77898\n",
      "Epoch 454/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0135 - acc: 0.7118 - val_loss: 1.0787 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00454: val_acc improved from 0.77898 to 0.78706, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 455/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0329 - acc: 0.7043 - val_loss: 1.3795 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.78706\n",
      "Epoch 456/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0115 - acc: 0.7151 - val_loss: 1.6036 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.78706\n",
      "Epoch 457/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0225 - acc: 0.7061 - val_loss: 1.2865 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.78706\n",
      "Epoch 458/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0106 - acc: 0.7130 - val_loss: 1.1863 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.78706\n",
      "Epoch 459/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0308 - acc: 0.6956 - val_loss: 1.4159 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.78706\n",
      "Epoch 460/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0297 - acc: 0.7010 - val_loss: 1.2984 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.78706\n",
      "Epoch 461/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0239 - acc: 0.7049 - val_loss: 1.3225 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.78706\n",
      "Epoch 462/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0230 - acc: 0.7109 - val_loss: 1.3122 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.78706\n",
      "Epoch 463/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7163 - val_loss: 1.2647 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.78706\n",
      "Epoch 464/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0122 - acc: 0.7139 - val_loss: 1.4570 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.78706\n",
      "Epoch 465/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9956 - acc: 0.7239 - val_loss: 1.3036 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.78706\n",
      "Epoch 466/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9952 - acc: 0.7194 - val_loss: 1.1809 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.78706\n",
      "Epoch 467/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0068 - acc: 0.7154 - val_loss: 1.6629 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.78706\n",
      "Epoch 468/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9891 - acc: 0.7197 - val_loss: 1.3335 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.78706\n",
      "Epoch 469/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0107 - acc: 0.7160 - val_loss: 1.3074 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.78706\n",
      "Epoch 470/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0044 - acc: 0.7124 - val_loss: 1.4709 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.78706\n",
      "Epoch 471/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0102 - acc: 0.6983 - val_loss: 1.7916 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.78706\n",
      "Epoch 472/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0336 - acc: 0.7022 - val_loss: 1.2016 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.78706\n",
      "Epoch 473/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0152 - acc: 0.7046 - val_loss: 1.2247 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.78706\n",
      "Epoch 474/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9775 - acc: 0.7272 - val_loss: 1.1812 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.78706\n",
      "Epoch 475/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9749 - acc: 0.7269 - val_loss: 1.3025 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.78706\n",
      "Epoch 476/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 1.9965 - acc: 0.7148 - val_loss: 1.1594 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.78706\n",
      "Epoch 477/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0188 - acc: 0.7076 - val_loss: 1.2506 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.78706\n",
      "Epoch 478/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9955 - acc: 0.7308 - val_loss: 1.2627 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.78706\n",
      "Epoch 479/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0445 - acc: 0.7025 - val_loss: 1.5154 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.78706\n",
      "Epoch 480/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7275 - val_loss: 1.7855 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.78706\n",
      "Epoch 481/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9720 - acc: 0.7191 - val_loss: 1.4526 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.78706\n",
      "Epoch 482/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9906 - acc: 0.7070 - val_loss: 1.3813 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.78706\n",
      "Epoch 483/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0187 - acc: 0.7163 - val_loss: 1.2448 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.78706\n",
      "Epoch 484/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0065 - acc: 0.7233 - val_loss: 1.3578 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.78706\n",
      "Epoch 485/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9752 - acc: 0.7145 - val_loss: 1.2072 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.78706\n",
      "Epoch 486/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0134 - acc: 0.7142 - val_loss: 1.3697 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.78706\n",
      "Epoch 487/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9754 - acc: 0.7281 - val_loss: 1.4385 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.78706\n",
      "Epoch 488/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0085 - acc: 0.7013 - val_loss: 1.2965 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.78706\n",
      "Epoch 489/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0041 - acc: 0.7082 - val_loss: 1.3542 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.78706\n",
      "Epoch 490/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9956 - acc: 0.7154 - val_loss: 1.2308 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.78706\n",
      "Epoch 491/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9841 - acc: 0.7061 - val_loss: 1.2199 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.78706\n",
      "Epoch 492/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0010 - acc: 0.7136 - val_loss: 1.8585 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.78706\n",
      "Epoch 493/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9809 - acc: 0.7242 - val_loss: 1.4764 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.78706\n",
      "Epoch 494/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9691 - acc: 0.7365 - val_loss: 1.5158 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.78706\n",
      "Epoch 495/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9960 - acc: 0.7145 - val_loss: 1.3454 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.78706\n",
      "Epoch 496/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0033 - acc: 0.7197 - val_loss: 2.3917 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.78706\n",
      "Epoch 497/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9877 - acc: 0.7377 - val_loss: 1.5741 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.78706\n",
      "Epoch 498/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9932 - acc: 0.7191 - val_loss: 1.3902 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.78706\n",
      "Epoch 499/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9428 - acc: 0.7323 - val_loss: 1.3465 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.78706\n",
      "Epoch 500/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0062 - acc: 0.7082 - val_loss: 1.4115 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.78706\n",
      "Epoch 501/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9808 - acc: 0.7206 - val_loss: 1.4092 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00501: val_acc did not improve from 0.78706\n",
      "Epoch 502/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0219 - acc: 0.7028 - val_loss: 1.4202 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00502: val_acc did not improve from 0.78706\n",
      "Epoch 503/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0048 - acc: 0.7100 - val_loss: 1.2632 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00503: val_acc did not improve from 0.78706\n",
      "Epoch 504/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9778 - acc: 0.7163 - val_loss: 1.4013 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00504: val_acc did not improve from 0.78706\n",
      "Epoch 505/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9734 - acc: 0.7091 - val_loss: 1.2787 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00505: val_acc did not improve from 0.78706\n",
      "Epoch 506/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9922 - acc: 0.7178 - val_loss: 1.8232 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00506: val_acc did not improve from 0.78706\n",
      "Epoch 507/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9705 - acc: 0.7236 - val_loss: 1.3333 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00507: val_acc did not improve from 0.78706\n",
      "Epoch 508/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9896 - acc: 0.7191 - val_loss: 1.5818 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00508: val_acc did not improve from 0.78706\n",
      "Epoch 509/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9837 - acc: 0.7145 - val_loss: 2.5144 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00509: val_acc did not improve from 0.78706\n",
      "Epoch 510/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9553 - acc: 0.7371 - val_loss: 1.3212 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00510: val_acc did not improve from 0.78706\n",
      "Epoch 511/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 1.9787 - acc: 0.7178 - val_loss: 1.2502 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00511: val_acc did not improve from 0.78706\n",
      "Epoch 512/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9786 - acc: 0.7124 - val_loss: 1.3167 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00512: val_acc did not improve from 0.78706\n",
      "Epoch 513/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9549 - acc: 0.7239 - val_loss: 1.2101 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00513: val_acc did not improve from 0.78706\n",
      "Epoch 514/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 1.9964 - acc: 0.7145 - val_loss: 1.7197 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00514: val_acc did not improve from 0.78706\n",
      "Epoch 515/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9621 - acc: 0.7317 - val_loss: 1.5248 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00515: val_acc did not improve from 0.78706\n",
      "Epoch 516/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0188 - acc: 0.6920 - val_loss: 1.8315 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00516: val_acc did not improve from 0.78706\n",
      "Epoch 517/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9479 - acc: 0.7341 - val_loss: 1.4312 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00517: val_acc did not improve from 0.78706\n",
      "Epoch 518/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9754 - acc: 0.7293 - val_loss: 1.3772 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00518: val_acc did not improve from 0.78706\n",
      "Epoch 519/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9637 - acc: 0.7260 - val_loss: 1.3656 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00519: val_acc did not improve from 0.78706\n",
      "Epoch 520/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9762 - acc: 0.7200 - val_loss: 1.4643 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00520: val_acc did not improve from 0.78706\n",
      "Epoch 521/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9838 - acc: 0.7275 - val_loss: 1.3820 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00521: val_acc did not improve from 0.78706\n",
      "Epoch 522/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9416 - acc: 0.7338 - val_loss: 1.1738 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00522: val_acc did not improve from 0.78706\n",
      "Epoch 523/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9993 - acc: 0.7142 - val_loss: 1.4438 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00523: val_acc did not improve from 0.78706\n",
      "Epoch 524/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9852 - acc: 0.7248 - val_loss: 1.2463 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00524: val_acc did not improve from 0.78706\n",
      "Epoch 525/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9500 - acc: 0.7362 - val_loss: 1.4168 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00525: val_acc did not improve from 0.78706\n",
      "Epoch 526/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9738 - acc: 0.7233 - val_loss: 1.9376 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00526: val_acc did not improve from 0.78706\n",
      "Epoch 527/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9989 - acc: 0.7157 - val_loss: 1.1348 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00527: val_acc did not improve from 0.78706\n",
      "Epoch 528/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9792 - acc: 0.7154 - val_loss: 1.1843 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00528: val_acc did not improve from 0.78706\n",
      "Epoch 529/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9462 - acc: 0.7365 - val_loss: 1.1959 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00529: val_acc did not improve from 0.78706\n",
      "Epoch 530/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9426 - acc: 0.7275 - val_loss: 1.3018 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00530: val_acc did not improve from 0.78706\n",
      "Epoch 531/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9814 - acc: 0.7263 - val_loss: 1.5158 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00531: val_acc did not improve from 0.78706\n",
      "Epoch 532/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9631 - acc: 0.7266 - val_loss: 1.4143 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00532: val_acc did not improve from 0.78706\n",
      "Epoch 533/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9708 - acc: 0.7160 - val_loss: 1.4053 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00533: val_acc did not improve from 0.78706\n",
      "Epoch 534/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9862 - acc: 0.7320 - val_loss: 1.2874 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00534: val_acc did not improve from 0.78706\n",
      "Epoch 535/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9769 - acc: 0.7248 - val_loss: 1.3139 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00535: val_acc did not improve from 0.78706\n",
      "Epoch 536/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0051 - acc: 0.7269 - val_loss: 1.5126 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00536: val_acc did not improve from 0.78706\n",
      "Epoch 537/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0072 - acc: 0.7160 - val_loss: 1.9030 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00537: val_acc did not improve from 0.78706\n",
      "Epoch 538/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9623 - acc: 0.7221 - val_loss: 1.3258 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00538: val_acc did not improve from 0.78706\n",
      "Epoch 539/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9810 - acc: 0.7215 - val_loss: 1.2655 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00539: val_acc did not improve from 0.78706\n",
      "Epoch 540/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9267 - acc: 0.7464 - val_loss: 1.3738 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00540: val_acc did not improve from 0.78706\n",
      "Epoch 541/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9427 - acc: 0.7314 - val_loss: 1.2885 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00541: val_acc did not improve from 0.78706\n",
      "Epoch 542/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9740 - acc: 0.7287 - val_loss: 1.2382 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00542: val_acc did not improve from 0.78706\n",
      "Epoch 543/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9830 - acc: 0.7200 - val_loss: 1.2856 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00543: val_acc did not improve from 0.78706\n",
      "Epoch 544/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9574 - acc: 0.7317 - val_loss: 1.1107 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00544: val_acc did not improve from 0.78706\n",
      "Epoch 545/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9869 - acc: 0.7163 - val_loss: 1.2885 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00545: val_acc did not improve from 0.78706\n",
      "Epoch 546/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9841 - acc: 0.7109 - val_loss: 1.4684 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00546: val_acc did not improve from 0.78706\n",
      "Epoch 547/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9580 - acc: 0.7320 - val_loss: 1.3891 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00547: val_acc did not improve from 0.78706\n",
      "Epoch 548/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9821 - acc: 0.7272 - val_loss: 1.4118 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00548: val_acc did not improve from 0.78706\n",
      "Epoch 549/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9510 - acc: 0.7314 - val_loss: 1.3552 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00549: val_acc did not improve from 0.78706\n",
      "Epoch 550/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9446 - acc: 0.7365 - val_loss: 1.2315 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00550: val_acc did not improve from 0.78706\n",
      "Epoch 551/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9228 - acc: 0.7362 - val_loss: 1.2380 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00551: val_acc did not improve from 0.78706\n",
      "Epoch 552/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9625 - acc: 0.7329 - val_loss: 1.1993 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00552: val_acc did not improve from 0.78706\n",
      "Epoch 553/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9804 - acc: 0.7151 - val_loss: 1.7387 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00553: val_acc did not improve from 0.78706\n",
      "Epoch 554/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9737 - acc: 0.7290 - val_loss: 1.6991 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00554: val_acc did not improve from 0.78706\n",
      "Epoch 00554: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_8===\n",
      "semi loading: model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "106/106 [==============================] - 33s 314ms/step - loss: 1.6954 - acc: 0.7759 - val_loss: 1.0037 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.80593, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 6/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6621 - acc: 0.8028 - val_loss: 0.9805 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.80593 to 0.81402, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 7/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6562 - acc: 0.8045 - val_loss: 0.9747 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81402\n",
      "Epoch 8/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6381 - acc: 0.8090 - val_loss: 0.9812 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81402\n",
      "Epoch 9/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6305 - acc: 0.8037 - val_loss: 0.9925 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81402\n",
      "Epoch 10/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6125 - acc: 0.8063 - val_loss: 0.9783 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81402\n",
      "Epoch 11/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.5906 - acc: 0.8125 - val_loss: 0.9554 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81402 to 0.81941, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 12/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5885 - acc: 0.8143 - val_loss: 0.9430 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.81941 to 0.82480, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 13/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.5754 - acc: 0.8149 - val_loss: 0.9473 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82480\n",
      "Epoch 14/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6330 - acc: 0.8034 - val_loss: 0.9206 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.82480 to 0.83019, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 15/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5997 - acc: 0.8116 - val_loss: 0.9660 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83019\n",
      "Epoch 16/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5658 - acc: 0.8216 - val_loss: 0.9434 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83019\n",
      "Epoch 17/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5660 - acc: 0.8243 - val_loss: 0.9415 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83019\n",
      "Epoch 18/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5419 - acc: 0.8213 - val_loss: 0.9328 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83019\n",
      "Epoch 19/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5579 - acc: 0.8205 - val_loss: 0.9580 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83019\n",
      "Epoch 20/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5718 - acc: 0.8278 - val_loss: 0.9217 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83019\n",
      "Epoch 21/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5603 - acc: 0.8314 - val_loss: 0.9579 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83019\n",
      "Epoch 22/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5543 - acc: 0.8269 - val_loss: 0.9416 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83019\n",
      "Epoch 23/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5962 - acc: 0.8007 - val_loss: 0.9196 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83019\n",
      "Epoch 24/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5422 - acc: 0.8231 - val_loss: 0.9301 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83019\n",
      "Epoch 25/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5383 - acc: 0.8219 - val_loss: 0.9532 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83019\n",
      "Epoch 26/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5582 - acc: 0.8128 - val_loss: 0.9224 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83019\n",
      "Epoch 27/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5618 - acc: 0.8131 - val_loss: 0.9069 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83019\n",
      "Epoch 28/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5461 - acc: 0.8237 - val_loss: 0.9382 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83019\n",
      "Epoch 29/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5588 - acc: 0.8299 - val_loss: 0.9674 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83019\n",
      "Epoch 30/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5323 - acc: 0.8237 - val_loss: 0.9295 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83019\n",
      "Epoch 31/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5306 - acc: 0.8269 - val_loss: 0.9295 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83019\n",
      "Epoch 32/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5128 - acc: 0.8349 - val_loss: 0.9343 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83019\n",
      "Epoch 33/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5407 - acc: 0.8187 - val_loss: 0.8957 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83019\n",
      "Epoch 34/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5204 - acc: 0.8216 - val_loss: 0.9226 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83019\n",
      "Epoch 35/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5302 - acc: 0.8296 - val_loss: 0.9205 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83019\n",
      "Epoch 36/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5232 - acc: 0.8299 - val_loss: 0.9170 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83019\n",
      "Epoch 37/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5313 - acc: 0.8261 - val_loss: 0.9232 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83019\n",
      "Epoch 38/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5300 - acc: 0.8252 - val_loss: 0.9178 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83019\n",
      "Epoch 39/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5115 - acc: 0.8290 - val_loss: 0.9130 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83019\n",
      "Epoch 40/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4939 - acc: 0.8278 - val_loss: 0.9285 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83019\n",
      "Epoch 41/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5096 - acc: 0.8228 - val_loss: 0.9109 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83019\n",
      "Epoch 42/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5154 - acc: 0.8325 - val_loss: 0.8982 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83019\n",
      "Epoch 43/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5062 - acc: 0.8325 - val_loss: 0.9415 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83019\n",
      "Epoch 44/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5139 - acc: 0.8249 - val_loss: 0.8943 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83019\n",
      "Epoch 45/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5248 - acc: 0.8172 - val_loss: 0.8979 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83019\n",
      "Epoch 46/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5342 - acc: 0.8219 - val_loss: 0.9035 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83019\n",
      "Epoch 47/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5035 - acc: 0.8281 - val_loss: 0.9120 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83019\n",
      "Epoch 48/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5161 - acc: 0.8202 - val_loss: 0.9037 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83019\n",
      "Epoch 49/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5108 - acc: 0.8305 - val_loss: 0.8860 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83019\n",
      "Epoch 50/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4824 - acc: 0.8314 - val_loss: 0.9062 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83019\n",
      "Epoch 51/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5229 - acc: 0.8225 - val_loss: 0.9024 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83019\n",
      "Epoch 52/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5000 - acc: 0.8355 - val_loss: 0.8834 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83019\n",
      "Epoch 53/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4995 - acc: 0.8317 - val_loss: 0.8926 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.83019 to 0.83288, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 54/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5089 - acc: 0.8314 - val_loss: 0.9144 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83288\n",
      "Epoch 55/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5159 - acc: 0.8278 - val_loss: 0.9119 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83288\n",
      "Epoch 56/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5035 - acc: 0.8296 - val_loss: 0.9224 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83288\n",
      "Epoch 57/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5009 - acc: 0.8299 - val_loss: 0.8811 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83288\n",
      "Epoch 58/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5038 - acc: 0.8331 - val_loss: 0.8927 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83288\n",
      "Epoch 59/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5013 - acc: 0.8328 - val_loss: 0.9045 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.83288\n",
      "Epoch 60/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5137 - acc: 0.8331 - val_loss: 0.8804 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.83288\n",
      "Epoch 61/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5037 - acc: 0.8340 - val_loss: 0.8838 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.83288\n",
      "Epoch 62/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4840 - acc: 0.8302 - val_loss: 0.9034 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.83288\n",
      "Epoch 63/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5089 - acc: 0.8314 - val_loss: 0.8797 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.83288\n",
      "Epoch 64/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4913 - acc: 0.8290 - val_loss: 0.8885 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.83288\n",
      "Epoch 65/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4550 - acc: 0.8349 - val_loss: 0.8680 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.83288\n",
      "Epoch 66/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5043 - acc: 0.8302 - val_loss: 0.8957 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.83288\n",
      "Epoch 67/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4884 - acc: 0.8272 - val_loss: 0.8961 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.83288\n",
      "Epoch 68/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4814 - acc: 0.8429 - val_loss: 0.8907 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.83288\n",
      "Epoch 69/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4812 - acc: 0.8320 - val_loss: 0.8749 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.83288\n",
      "Epoch 70/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4853 - acc: 0.8255 - val_loss: 0.8906 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.83288\n",
      "Epoch 71/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4592 - acc: 0.8435 - val_loss: 0.8799 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.83288\n",
      "Epoch 72/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4905 - acc: 0.8349 - val_loss: 0.8750 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.83288\n",
      "Epoch 73/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4731 - acc: 0.8364 - val_loss: 0.8832 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.83288\n",
      "Epoch 74/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4574 - acc: 0.8494 - val_loss: 0.8950 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.83288\n",
      "Epoch 75/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4858 - acc: 0.8461 - val_loss: 0.8962 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.83288\n",
      "Epoch 76/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4648 - acc: 0.8346 - val_loss: 0.8880 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.83288\n",
      "Epoch 77/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.4868 - acc: 0.8432 - val_loss: 0.9007 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.83288\n",
      "Epoch 78/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4769 - acc: 0.8458 - val_loss: 0.8923 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.83288\n",
      "Epoch 79/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4782 - acc: 0.8408 - val_loss: 0.8858 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.83288\n",
      "Epoch 80/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4700 - acc: 0.8402 - val_loss: 0.8861 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.83288\n",
      "Epoch 81/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4886 - acc: 0.8390 - val_loss: 0.8876 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.83288\n",
      "Epoch 82/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4719 - acc: 0.8308 - val_loss: 0.8962 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.83288\n",
      "Epoch 83/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4760 - acc: 0.8387 - val_loss: 0.8872 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.83288\n",
      "Epoch 84/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4715 - acc: 0.8311 - val_loss: 0.8831 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.83288\n",
      "Epoch 85/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4518 - acc: 0.8426 - val_loss: 0.8830 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.83288\n",
      "Epoch 86/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4767 - acc: 0.8429 - val_loss: 0.8907 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.83288\n",
      "Epoch 87/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4687 - acc: 0.8346 - val_loss: 0.8949 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.83288\n",
      "Epoch 88/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4649 - acc: 0.8470 - val_loss: 0.8966 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.83288\n",
      "Epoch 89/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4777 - acc: 0.8373 - val_loss: 0.8941 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.83288\n",
      "Epoch 90/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4694 - acc: 0.8390 - val_loss: 0.8980 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.83288\n",
      "Epoch 91/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4517 - acc: 0.8429 - val_loss: 0.8956 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.83288\n",
      "Epoch 92/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4699 - acc: 0.8379 - val_loss: 0.8959 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.83288\n",
      "Epoch 93/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4606 - acc: 0.8411 - val_loss: 0.8820 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.83288\n",
      "Epoch 94/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4667 - acc: 0.8423 - val_loss: 0.8905 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.83288\n",
      "Epoch 95/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4583 - acc: 0.8390 - val_loss: 0.8932 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.83288\n",
      "Epoch 96/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4780 - acc: 0.8379 - val_loss: 0.8952 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.83288\n",
      "Epoch 97/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4739 - acc: 0.8449 - val_loss: 0.8903 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.83288\n",
      "Epoch 98/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4674 - acc: 0.8384 - val_loss: 0.8894 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.83288\n",
      "Epoch 99/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4645 - acc: 0.8499 - val_loss: 0.8869 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.83288\n",
      "Epoch 100/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4782 - acc: 0.8370 - val_loss: 0.8845 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.83288\n",
      "Epoch 101/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4646 - acc: 0.8390 - val_loss: 0.8837 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.83288\n",
      "Epoch 102/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4608 - acc: 0.8467 - val_loss: 0.8930 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.83288\n",
      "Epoch 103/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4734 - acc: 0.8558 - val_loss: 0.8862 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.83288\n",
      "Epoch 104/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4435 - acc: 0.8393 - val_loss: 0.8881 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.83288\n",
      "Epoch 105/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4858 - acc: 0.8337 - val_loss: 0.8867 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.83288\n",
      "Epoch 106/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4321 - acc: 0.8449 - val_loss: 0.8787 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.83288\n",
      "Epoch 107/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4426 - acc: 0.8464 - val_loss: 0.8856 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.83288\n",
      "Epoch 108/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4731 - acc: 0.8272 - val_loss: 0.8849 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.83288\n",
      "Epoch 109/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4917 - acc: 0.8261 - val_loss: 0.8827 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.83288\n",
      "Epoch 110/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4400 - acc: 0.8390 - val_loss: 0.8812 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.83288\n",
      "Epoch 111/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4480 - acc: 0.8387 - val_loss: 0.8815 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.83288\n",
      "Epoch 112/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4467 - acc: 0.8408 - val_loss: 0.8811 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.83288\n",
      "Epoch 113/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4384 - acc: 0.8323 - val_loss: 0.8845 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.83288\n",
      "Epoch 114/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4837 - acc: 0.8261 - val_loss: 0.8889 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.83288\n",
      "Epoch 115/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4577 - acc: 0.8411 - val_loss: 0.8844 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.83288\n",
      "Epoch 116/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4734 - acc: 0.8432 - val_loss: 0.8840 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.83288\n",
      "Epoch 117/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4709 - acc: 0.8494 - val_loss: 0.8845 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.83288\n",
      "Epoch 118/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4689 - acc: 0.8420 - val_loss: 0.8839 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.83288\n",
      "Epoch 119/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.4724 - acc: 0.8402 - val_loss: 0.8829 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.83288\n",
      "Epoch 120/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4698 - acc: 0.8361 - val_loss: 0.8815 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.83288\n",
      "Epoch 121/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4807 - acc: 0.8364 - val_loss: 0.8860 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.83288\n",
      "Epoch 122/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4547 - acc: 0.8343 - val_loss: 0.8850 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.83288\n",
      "Epoch 123/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4584 - acc: 0.8373 - val_loss: 0.8847 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.83288\n",
      "Epoch 124/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4708 - acc: 0.8361 - val_loss: 0.8804 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.83288\n",
      "Epoch 125/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4715 - acc: 0.8396 - val_loss: 0.8835 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.83288\n",
      "Epoch 126/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4746 - acc: 0.8417 - val_loss: 0.8847 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.83288\n",
      "Epoch 127/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4921 - acc: 0.8352 - val_loss: 0.8873 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.83288\n",
      "Epoch 128/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4391 - acc: 0.8317 - val_loss: 0.8842 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.83288\n",
      "Epoch 129/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4706 - acc: 0.8340 - val_loss: 0.8877 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.83288\n",
      "Epoch 130/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4698 - acc: 0.8299 - val_loss: 0.8852 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.83288\n",
      "Epoch 131/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4631 - acc: 0.8343 - val_loss: 0.8834 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.83288\n",
      "Epoch 132/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4779 - acc: 0.8381 - val_loss: 0.8881 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.83288\n",
      "Epoch 133/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4720 - acc: 0.8278 - val_loss: 0.8864 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83288\n",
      "Epoch 134/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4443 - acc: 0.8464 - val_loss: 0.8858 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83288\n",
      "Epoch 135/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4431 - acc: 0.8482 - val_loss: 0.8866 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.83288\n",
      "Epoch 136/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4572 - acc: 0.8390 - val_loss: 0.8847 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83288\n",
      "Epoch 137/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4775 - acc: 0.8320 - val_loss: 0.8833 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83288\n",
      "Epoch 138/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4581 - acc: 0.8373 - val_loss: 0.8823 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83288\n",
      "Epoch 139/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4619 - acc: 0.8414 - val_loss: 0.8830 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83288\n",
      "Epoch 140/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4470 - acc: 0.8467 - val_loss: 0.8840 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83288\n",
      "Epoch 141/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4725 - acc: 0.8361 - val_loss: 0.8888 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83288\n",
      "Epoch 142/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4460 - acc: 0.8443 - val_loss: 0.8847 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83288\n",
      "Epoch 143/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4553 - acc: 0.8393 - val_loss: 0.8826 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83288\n",
      "Epoch 144/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4603 - acc: 0.8452 - val_loss: 0.8809 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83288\n",
      "Epoch 145/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4508 - acc: 0.8414 - val_loss: 0.8864 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83288\n",
      "Epoch 146/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4637 - acc: 0.8384 - val_loss: 0.8801 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83288\n",
      "Epoch 147/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4542 - acc: 0.8435 - val_loss: 0.8808 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83288\n",
      "Epoch 148/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4501 - acc: 0.8429 - val_loss: 0.8829 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83288\n",
      "Epoch 149/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4516 - acc: 0.8452 - val_loss: 0.8802 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83288\n",
      "Epoch 150/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4630 - acc: 0.8396 - val_loss: 0.8808 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83288\n",
      "Epoch 151/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4438 - acc: 0.8399 - val_loss: 0.8851 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.83288\n",
      "Epoch 152/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4530 - acc: 0.8461 - val_loss: 0.8885 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83288\n",
      "Epoch 153/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4821 - acc: 0.8358 - val_loss: 0.8904 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83288\n",
      "Epoch 154/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4716 - acc: 0.8408 - val_loss: 0.8869 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83288\n",
      "Epoch 155/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4509 - acc: 0.8384 - val_loss: 0.8912 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83288\n",
      "Epoch 156/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4408 - acc: 0.8485 - val_loss: 0.8874 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.83288\n",
      "Epoch 157/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4516 - acc: 0.8393 - val_loss: 0.8886 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83288\n",
      "Epoch 158/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4604 - acc: 0.8367 - val_loss: 0.8872 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83288\n",
      "Epoch 159/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4616 - acc: 0.8432 - val_loss: 0.8828 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.83288\n",
      "Epoch 160/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4643 - acc: 0.8411 - val_loss: 0.8835 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83288\n",
      "Epoch 161/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4633 - acc: 0.8370 - val_loss: 0.8834 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83288\n",
      "Epoch 162/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4677 - acc: 0.8432 - val_loss: 0.8843 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83288\n",
      "Epoch 163/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4383 - acc: 0.8420 - val_loss: 0.8833 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83288\n",
      "Epoch 164/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4438 - acc: 0.8376 - val_loss: 0.8849 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83288\n",
      "Epoch 165/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4597 - acc: 0.8494 - val_loss: 0.8843 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83288\n",
      "Epoch 00165: early stopping\n",
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold9_mfcc7===\n",
      "using resnet model: 1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 30, 130, 64)  3200        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 30, 130, 64)  256         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 30, 130, 64)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 15, 65, 64)   0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 15, 65, 64)   36928       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 15, 65, 64)   256         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 15, 65, 64)   0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 15, 65, 64)   36928       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 15, 65, 64)   0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 15, 65, 64)   256         add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 15, 65, 64)   0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 15, 65, 64)   36928       activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 15, 65, 64)   256         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 15, 65, 64)   0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 15, 65, 64)   36928       activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 15, 65, 64)   0           add_228[0][0]                    \n",
      "                                                                 conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 15, 65, 64)   256         add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 15, 65, 64)   0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 15, 65, 64)   36928       activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 15, 65, 64)   256         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 15, 65, 64)   0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 15, 65, 64)   36928       activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 15, 65, 64)   0           add_229[0][0]                    \n",
      "                                                                 conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 15, 65, 64)   256         add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 15, 65, 64)   0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 8, 33, 128)   73856       activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 8, 33, 128)   512         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 8, 33, 128)   0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 8, 33, 128)   8320        add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 8, 33, 128)   147584      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 8, 33, 128)   0           conv2d_669[0][0]                 \n",
      "                                                                 conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 8, 33, 128)   512         add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 8, 33, 128)   0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 8, 33, 128)   147584      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 8, 33, 128)   512         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 8, 33, 128)   0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 8, 33, 128)   147584      activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 8, 33, 128)   0           add_231[0][0]                    \n",
      "                                                                 conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 8, 33, 128)   512         add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 8, 33, 128)   0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 8, 33, 128)   147584      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 8, 33, 128)   512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 8, 33, 128)   0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 8, 33, 128)   147584      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 8, 33, 128)   0           add_232[0][0]                    \n",
      "                                                                 conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 8, 33, 128)   512         add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 8, 33, 128)   0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 8, 33, 128)   147584      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 8, 33, 128)   512         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 8, 33, 128)   0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 8, 33, 128)   147584      activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 8, 33, 128)   0           add_233[0][0]                    \n",
      "                                                                 conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 8, 33, 128)   512         add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 8, 33, 128)   0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 4, 17, 256)   295168      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 4, 17, 256)   1024        conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 4, 17, 256)   0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 4, 17, 256)   33024       add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 4, 17, 256)   590080      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 4, 17, 256)   0           conv2d_678[0][0]                 \n",
      "                                                                 conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 4, 17, 256)   1024        add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 4, 17, 256)   0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 4, 17, 256)   590080      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 4, 17, 256)   1024        conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 4, 17, 256)   0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 4, 17, 256)   590080      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 4, 17, 256)   0           add_235[0][0]                    \n",
      "                                                                 conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 4, 17, 256)   1024        add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 4, 17, 256)   0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 4, 17, 256)   590080      activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 4, 17, 256)   1024        conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 4, 17, 256)   0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 4, 17, 256)   590080      activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 4, 17, 256)   0           add_236[0][0]                    \n",
      "                                                                 conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 4, 17, 256)   1024        add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 4, 17, 256)   0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 4, 17, 256)   590080      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 4, 17, 256)   1024        conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 4, 17, 256)   0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 4, 17, 256)   590080      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 4, 17, 256)   0           add_237[0][0]                    \n",
      "                                                                 conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 4, 17, 256)   1024        add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 4, 17, 256)   0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 4, 17, 256)   590080      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 4, 17, 256)   1024        conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 4, 17, 256)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 4, 17, 256)   590080      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 4, 17, 256)   0           add_238[0][0]                    \n",
      "                                                                 conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 4, 17, 256)   1024        add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 4, 17, 256)   0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 4, 17, 256)   590080      activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 4, 17, 256)   1024        conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 4, 17, 256)   0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 4, 17, 256)   590080      activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 4, 17, 256)   0           add_239[0][0]                    \n",
      "                                                                 conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 4, 17, 256)   1024        add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 4, 17, 256)   0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 2, 9, 512)    1180160     activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 2, 9, 512)    2048        conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 2, 9, 512)    0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 2, 9, 512)    131584      add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 2, 9, 512)    2359808     activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_241 (Add)                   (None, 2, 9, 512)    0           conv2d_691[0][0]                 \n",
      "                                                                 conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 2, 9, 512)    2048        add_241[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 2, 9, 512)    0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 2, 9, 512)    2359808     activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 2, 9, 512)    2048        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 2, 9, 512)    0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 2, 9, 512)    2359808     activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_242 (Add)                   (None, 2, 9, 512)    0           add_241[0][0]                    \n",
      "                                                                 conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 2, 9, 512)    2048        add_242[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 2, 9, 512)    0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 2, 9, 512)    2359808     activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 2, 9, 512)    2048        conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 2, 9, 512)    0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 2, 9, 512)    2359808     activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_243 (Add)                   (None, 2, 9, 512)    0           add_242[0][0]                    \n",
      "                                                                 conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 2, 9, 512)    2048        add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 2, 9, 512)    0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 1, 1, 512)    0           activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 512)          0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 43)           22059       flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 43)           172         dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 43)           0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 41)           1804        dropout_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,324,387\n",
      "Trainable params: 21,309,069\n",
      "Non-trainable params: 15,318\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 5.4971 - acc: 0.0469 - val_loss: 6.3937 - val_acc: 0.0620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.06199, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 2/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.9433 - acc: 0.0919 - val_loss: 6.4333 - val_acc: 0.0863\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.06199 to 0.08625, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 3/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 4.6451 - acc: 0.0877 - val_loss: 5.2812 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.08625\n",
      "Epoch 4/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.3810 - acc: 0.1052 - val_loss: 4.5357 - val_acc: 0.0512\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.08625\n",
      "Epoch 5/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.1587 - acc: 0.1262 - val_loss: 4.5848 - val_acc: 0.0836\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.08625\n",
      "Epoch 6/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.0086 - acc: 0.1337 - val_loss: 4.0574 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.08625 to 0.12938, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 7/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.8183 - acc: 0.1653 - val_loss: 3.8645 - val_acc: 0.1509\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.12938 to 0.15094, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 8/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.7033 - acc: 0.1845 - val_loss: 3.2781 - val_acc: 0.2156\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.15094 to 0.21563, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 9/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.5858 - acc: 0.2019 - val_loss: 4.2039 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.21563\n",
      "Epoch 10/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.4870 - acc: 0.2203 - val_loss: 3.2442 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.21563 to 0.22911, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 11/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.3652 - acc: 0.2545 - val_loss: 3.9869 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.22911\n",
      "Epoch 12/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 3.3236 - acc: 0.2689 - val_loss: 3.0216 - val_acc: 0.3019\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.22911 to 0.30189, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 13/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.2276 - acc: 0.2936 - val_loss: 3.4522 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.30189\n",
      "Epoch 14/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.1802 - acc: 0.3170 - val_loss: 2.6630 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.30189 to 0.32884, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 15/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0845 - acc: 0.3398 - val_loss: 2.4967 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.32884 to 0.39892, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 16/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0849 - acc: 0.3314 - val_loss: 2.3910 - val_acc: 0.3854\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.39892\n",
      "Epoch 17/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0304 - acc: 0.3516 - val_loss: 2.8012 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.39892\n",
      "Epoch 18/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.9977 - acc: 0.3738 - val_loss: 2.6922 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.39892\n",
      "Epoch 19/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.9675 - acc: 0.3660 - val_loss: 2.3541 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.39892 to 0.40162, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 20/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.9421 - acc: 0.3918 - val_loss: 2.0011 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.40162 to 0.53908, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 21/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8933 - acc: 0.4047 - val_loss: 2.2955 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.53908\n",
      "Epoch 22/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.8419 - acc: 0.4168 - val_loss: 2.5975 - val_acc: 0.3396\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.53908\n",
      "Epoch 23/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8152 - acc: 0.4387 - val_loss: 2.1626 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.53908\n",
      "Epoch 24/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8239 - acc: 0.4390 - val_loss: 1.8890 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.53908 to 0.57412, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 25/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8086 - acc: 0.4399 - val_loss: 2.3936 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.57412\n",
      "Epoch 26/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7532 - acc: 0.4528 - val_loss: 1.9163 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.57412\n",
      "Epoch 27/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7645 - acc: 0.4528 - val_loss: 2.3758 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.57412\n",
      "Epoch 28/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7280 - acc: 0.4691 - val_loss: 2.4155 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.57412\n",
      "Epoch 29/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7031 - acc: 0.4694 - val_loss: 2.2590 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.57412\n",
      "Epoch 30/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.6998 - acc: 0.4718 - val_loss: 2.7661 - val_acc: 0.2938\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.57412\n",
      "Epoch 31/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6789 - acc: 0.4847 - val_loss: 1.8095 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.57412\n",
      "Epoch 32/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.6710 - acc: 0.4808 - val_loss: 1.9752 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.57412\n",
      "Epoch 33/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6498 - acc: 0.4826 - val_loss: 1.9676 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.57412\n",
      "Epoch 34/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6550 - acc: 0.4829 - val_loss: 2.1273 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.57412\n",
      "Epoch 35/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5973 - acc: 0.5192 - val_loss: 1.8021 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.57412 to 0.58760, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 36/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5881 - acc: 0.5072 - val_loss: 2.1362 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.58760\n",
      "Epoch 37/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.5520 - acc: 0.5216 - val_loss: 2.9984 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.58760\n",
      "Epoch 38/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5679 - acc: 0.5174 - val_loss: 2.4282 - val_acc: 0.4097\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.58760\n",
      "Epoch 39/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5258 - acc: 0.5385 - val_loss: 1.9172 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.58760\n",
      "Epoch 40/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4965 - acc: 0.5349 - val_loss: 1.6676 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.58760 to 0.61995, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5282 - acc: 0.5252 - val_loss: 1.8646 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.61995\n",
      "Epoch 42/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4976 - acc: 0.5529 - val_loss: 1.8756 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.61995\n",
      "Epoch 43/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5427 - acc: 0.5276 - val_loss: 1.7960 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.61995\n",
      "Epoch 44/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4846 - acc: 0.5628 - val_loss: 1.6770 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.61995\n",
      "Epoch 45/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4563 - acc: 0.5628 - val_loss: 1.7897 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.61995\n",
      "Epoch 46/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4605 - acc: 0.5652 - val_loss: 2.0079 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.61995\n",
      "Epoch 47/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4159 - acc: 0.5811 - val_loss: 1.8004 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.61995\n",
      "Epoch 48/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4135 - acc: 0.5724 - val_loss: 1.5408 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.61995 to 0.65768, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 49/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4051 - acc: 0.5736 - val_loss: 1.7407 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.65768\n",
      "Epoch 50/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4082 - acc: 0.5754 - val_loss: 1.5870 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.65768\n",
      "Epoch 51/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4177 - acc: 0.5808 - val_loss: 1.7443 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.65768\n",
      "Epoch 52/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3952 - acc: 0.5862 - val_loss: 1.6028 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.65768\n",
      "Epoch 53/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3758 - acc: 0.5947 - val_loss: 1.7821 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.65768\n",
      "Epoch 54/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3624 - acc: 0.5953 - val_loss: 1.5437 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.65768 to 0.68733, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 55/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3263 - acc: 0.6073 - val_loss: 1.4203 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.68733 to 0.71159, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 56/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3303 - acc: 0.5983 - val_loss: 1.7761 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.71159\n",
      "Epoch 57/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3301 - acc: 0.6178 - val_loss: 1.4283 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.71159\n",
      "Epoch 58/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3544 - acc: 0.5880 - val_loss: 1.3528 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.71159 to 0.72507, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 59/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3426 - acc: 0.6016 - val_loss: 1.6911 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.72507\n",
      "Epoch 60/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3492 - acc: 0.6022 - val_loss: 1.3610 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.72507 to 0.72507, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 61/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3031 - acc: 0.6145 - val_loss: 1.3833 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.72507\n",
      "Epoch 62/3000\n",
      "52/52 [==============================] - 6s 118ms/step - loss: 2.3060 - acc: 0.6148 - val_loss: 1.5282 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.72507\n",
      "Epoch 63/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3221 - acc: 0.6145 - val_loss: 1.5392 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.72507\n",
      "Epoch 64/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2804 - acc: 0.6247 - val_loss: 1.6057 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.72507\n",
      "Epoch 65/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2476 - acc: 0.6388 - val_loss: 1.3865 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.72507\n",
      "Epoch 66/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2553 - acc: 0.6391 - val_loss: 1.4540 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.72507\n",
      "Epoch 67/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2541 - acc: 0.6226 - val_loss: 1.5567 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.72507\n",
      "Epoch 68/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2504 - acc: 0.6394 - val_loss: 1.3980 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.72507\n",
      "Epoch 69/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2472 - acc: 0.6337 - val_loss: 1.4236 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.72507\n",
      "Epoch 70/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.2393 - acc: 0.6406 - val_loss: 1.2602 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00070: val_acc improved from 0.72507 to 0.74394, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 71/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2134 - acc: 0.6406 - val_loss: 1.2905 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.74394\n",
      "Epoch 72/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1796 - acc: 0.6644 - val_loss: 1.5347 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.74394\n",
      "Epoch 73/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1944 - acc: 0.6638 - val_loss: 1.3348 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.74394 to 0.74663, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 74/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1501 - acc: 0.6743 - val_loss: 1.4145 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.74663\n",
      "Epoch 75/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1806 - acc: 0.6593 - val_loss: 1.2945 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.74663\n",
      "Epoch 76/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1759 - acc: 0.6665 - val_loss: 1.3535 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.74663\n",
      "Epoch 77/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1777 - acc: 0.6629 - val_loss: 1.2361 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.74663\n",
      "Epoch 78/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1745 - acc: 0.6677 - val_loss: 1.6148 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.74663\n",
      "Epoch 79/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1803 - acc: 0.6562 - val_loss: 1.4534 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.74663\n",
      "Epoch 80/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1542 - acc: 0.6740 - val_loss: 1.3563 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.74663\n",
      "Epoch 81/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1299 - acc: 0.6866 - val_loss: 1.5159 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.74663\n",
      "Epoch 82/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1747 - acc: 0.6599 - val_loss: 1.2298 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.74663 to 0.75741, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1455 - acc: 0.6809 - val_loss: 1.1509 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.75741 to 0.76819, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 84/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1309 - acc: 0.6779 - val_loss: 1.3462 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76819\n",
      "Epoch 85/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1150 - acc: 0.6794 - val_loss: 1.2776 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76819\n",
      "Epoch 86/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1030 - acc: 0.6878 - val_loss: 1.1395 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76819\n",
      "Epoch 87/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1633 - acc: 0.6653 - val_loss: 1.6134 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76819\n",
      "Epoch 88/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0858 - acc: 0.6947 - val_loss: 1.5085 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76819\n",
      "Epoch 89/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0880 - acc: 0.6863 - val_loss: 1.2815 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76819\n",
      "Epoch 90/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1034 - acc: 0.6812 - val_loss: 1.5284 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76819\n",
      "Epoch 91/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0823 - acc: 0.6944 - val_loss: 1.5216 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76819\n",
      "Epoch 92/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0439 - acc: 0.7055 - val_loss: 1.3324 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76819\n",
      "Epoch 93/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0783 - acc: 0.6944 - val_loss: 1.6448 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76819\n",
      "Epoch 94/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0706 - acc: 0.6983 - val_loss: 1.4254 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76819\n",
      "Epoch 95/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0583 - acc: 0.6947 - val_loss: 1.1851 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.76819 to 0.77628, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 96/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0380 - acc: 0.7049 - val_loss: 1.0123 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.77628 to 0.82749, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 97/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0595 - acc: 0.7031 - val_loss: 1.5489 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.82749\n",
      "Epoch 98/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0538 - acc: 0.6986 - val_loss: 1.1504 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.82749\n",
      "Epoch 99/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0526 - acc: 0.6992 - val_loss: 1.2546 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.82749\n",
      "Epoch 100/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0291 - acc: 0.7031 - val_loss: 1.2147 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.82749\n",
      "Epoch 101/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0251 - acc: 0.7010 - val_loss: 1.3633 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.82749\n",
      "Epoch 102/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0179 - acc: 0.7112 - val_loss: 1.4068 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.82749\n",
      "Epoch 103/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0151 - acc: 0.7166 - val_loss: 1.1251 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82749\n",
      "Epoch 104/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0144 - acc: 0.7254 - val_loss: 1.2227 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82749\n",
      "Epoch 105/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0109 - acc: 0.7239 - val_loss: 1.1006 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82749\n",
      "Epoch 106/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0052 - acc: 0.7178 - val_loss: 1.1956 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.82749\n",
      "Epoch 107/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9981 - acc: 0.7191 - val_loss: 1.2807 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.82749\n",
      "Epoch 108/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9916 - acc: 0.7308 - val_loss: 1.1782 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.82749\n",
      "Epoch 109/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9787 - acc: 0.7266 - val_loss: 1.3225 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.82749\n",
      "Epoch 110/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0063 - acc: 0.7215 - val_loss: 1.1580 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.82749\n",
      "Epoch 111/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9825 - acc: 0.7239 - val_loss: 1.1792 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.82749\n",
      "Epoch 112/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9496 - acc: 0.7434 - val_loss: 1.3325 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.82749\n",
      "Epoch 113/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9671 - acc: 0.7257 - val_loss: 1.2432 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.82749\n",
      "Epoch 114/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9487 - acc: 0.7242 - val_loss: 1.1865 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.82749\n",
      "Epoch 115/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9505 - acc: 0.7371 - val_loss: 1.2766 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.82749\n",
      "Epoch 116/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9659 - acc: 0.7269 - val_loss: 1.2464 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.82749\n",
      "Epoch 117/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9329 - acc: 0.7437 - val_loss: 1.2101 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.82749\n",
      "Epoch 118/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9470 - acc: 0.7329 - val_loss: 1.1324 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.82749\n",
      "Epoch 119/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9313 - acc: 0.7476 - val_loss: 1.1696 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.82749\n",
      "Epoch 120/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9505 - acc: 0.7347 - val_loss: 1.1710 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.82749\n",
      "Epoch 121/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9341 - acc: 0.7461 - val_loss: 1.2192 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.82749\n",
      "Epoch 122/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9325 - acc: 0.7320 - val_loss: 1.0979 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.82749\n",
      "Epoch 123/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9155 - acc: 0.7482 - val_loss: 1.1108 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.82749\n",
      "Epoch 124/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9223 - acc: 0.7422 - val_loss: 1.0891 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.82749\n",
      "Epoch 125/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9168 - acc: 0.7440 - val_loss: 1.1973 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.82749\n",
      "Epoch 126/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9340 - acc: 0.7434 - val_loss: 1.1436 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.82749\n",
      "Epoch 127/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9201 - acc: 0.7515 - val_loss: 1.1462 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.82749\n",
      "Epoch 128/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9171 - acc: 0.7437 - val_loss: 1.1516 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.82749\n",
      "Epoch 129/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9112 - acc: 0.7458 - val_loss: 1.1277 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.82749\n",
      "Epoch 130/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8748 - acc: 0.7563 - val_loss: 1.2279 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.82749\n",
      "Epoch 131/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8846 - acc: 0.7590 - val_loss: 1.0994 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.82749\n",
      "Epoch 132/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9077 - acc: 0.7500 - val_loss: 1.1533 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.82749\n",
      "Epoch 133/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9212 - acc: 0.7371 - val_loss: 1.1475 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.82749\n",
      "Epoch 134/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8819 - acc: 0.7638 - val_loss: 1.1533 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.82749\n",
      "Epoch 135/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8879 - acc: 0.7497 - val_loss: 1.1151 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.82749\n",
      "Epoch 136/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8673 - acc: 0.7650 - val_loss: 1.1858 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.82749\n",
      "Epoch 137/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8781 - acc: 0.7578 - val_loss: 1.1630 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.82749\n",
      "Epoch 138/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8798 - acc: 0.7596 - val_loss: 1.1112 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.82749\n",
      "Epoch 139/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8825 - acc: 0.7518 - val_loss: 1.1195 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.82749\n",
      "Epoch 140/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8635 - acc: 0.7572 - val_loss: 1.2057 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.82749\n",
      "Epoch 141/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8633 - acc: 0.7650 - val_loss: 1.2079 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.82749\n",
      "Epoch 142/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8578 - acc: 0.7629 - val_loss: 1.2061 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.82749\n",
      "Epoch 143/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8491 - acc: 0.7644 - val_loss: 1.1376 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.82749\n",
      "Epoch 144/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8464 - acc: 0.7701 - val_loss: 1.0858 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.82749\n",
      "Epoch 145/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8581 - acc: 0.7587 - val_loss: 1.2252 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.82749\n",
      "Epoch 146/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8584 - acc: 0.7590 - val_loss: 1.3471 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.82749\n",
      "Epoch 147/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8633 - acc: 0.7659 - val_loss: 1.1197 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.82749\n",
      "Epoch 148/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8244 - acc: 0.7770 - val_loss: 1.2773 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.82749\n",
      "Epoch 149/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8566 - acc: 0.7668 - val_loss: 1.0103 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.82749\n",
      "Epoch 150/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8436 - acc: 0.7671 - val_loss: 1.1007 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.82749\n",
      "Epoch 151/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8494 - acc: 0.7713 - val_loss: 1.0075 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.82749 to 0.83558, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 152/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8396 - acc: 0.7701 - val_loss: 1.1855 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83558\n",
      "Epoch 153/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8257 - acc: 0.7788 - val_loss: 1.0930 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83558\n",
      "Epoch 154/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8196 - acc: 0.7740 - val_loss: 1.2022 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83558\n",
      "Epoch 155/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7977 - acc: 0.7873 - val_loss: 1.0916 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83558\n",
      "Epoch 156/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8009 - acc: 0.7734 - val_loss: 1.1262 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.83558\n",
      "Epoch 157/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8105 - acc: 0.7794 - val_loss: 1.2564 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83558\n",
      "Epoch 158/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8313 - acc: 0.7740 - val_loss: 1.2038 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83558\n",
      "Epoch 159/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8178 - acc: 0.7767 - val_loss: 1.1717 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.83558\n",
      "Epoch 160/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8168 - acc: 0.7791 - val_loss: 1.0843 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83558\n",
      "Epoch 161/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8296 - acc: 0.7689 - val_loss: 1.3784 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83558\n",
      "Epoch 162/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8296 - acc: 0.7764 - val_loss: 1.1571 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83558\n",
      "Epoch 163/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7734 - acc: 0.7930 - val_loss: 1.0661 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83558\n",
      "Epoch 164/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8045 - acc: 0.7822 - val_loss: 1.0817 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83558\n",
      "Epoch 165/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7831 - acc: 0.7816 - val_loss: 1.1346 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83558\n",
      "Epoch 166/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7672 - acc: 0.8008 - val_loss: 1.1085 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.83558\n",
      "Epoch 167/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7889 - acc: 0.7773 - val_loss: 1.1265 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.83558\n",
      "Epoch 168/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8070 - acc: 0.7734 - val_loss: 1.0613 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.83558\n",
      "Epoch 169/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7776 - acc: 0.7885 - val_loss: 1.0171 - val_acc: 0.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00169: val_acc did not improve from 0.83558\n",
      "Epoch 170/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8221 - acc: 0.7683 - val_loss: 1.3640 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.83558\n",
      "Epoch 171/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7859 - acc: 0.7855 - val_loss: 1.1673 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.83558\n",
      "Epoch 172/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7814 - acc: 0.7861 - val_loss: 1.0640 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.83558\n",
      "Epoch 173/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7627 - acc: 0.7945 - val_loss: 1.0827 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.83558\n",
      "Epoch 174/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7777 - acc: 0.7894 - val_loss: 1.0214 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.83558\n",
      "Epoch 175/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7862 - acc: 0.7737 - val_loss: 1.0835 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.83558\n",
      "Epoch 176/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7821 - acc: 0.7957 - val_loss: 1.2108 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.83558\n",
      "Epoch 177/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7814 - acc: 0.7837 - val_loss: 1.0378 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.83558\n",
      "Epoch 178/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7463 - acc: 0.7933 - val_loss: 1.1780 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.83558\n",
      "Epoch 179/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7956 - acc: 0.7740 - val_loss: 1.3694 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.83558\n",
      "Epoch 180/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7832 - acc: 0.7855 - val_loss: 1.2417 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.83558\n",
      "Epoch 181/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7732 - acc: 0.7788 - val_loss: 1.1892 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.83558\n",
      "Epoch 182/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7645 - acc: 0.7966 - val_loss: 1.3980 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.83558\n",
      "Epoch 183/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7594 - acc: 0.7966 - val_loss: 1.0951 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.83558\n",
      "Epoch 184/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7358 - acc: 0.7993 - val_loss: 1.1611 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.83558\n",
      "Epoch 185/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7434 - acc: 0.7900 - val_loss: 1.0008 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.83558\n",
      "Epoch 186/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7280 - acc: 0.8023 - val_loss: 1.0947 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.83558\n",
      "Epoch 187/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7381 - acc: 0.7930 - val_loss: 1.0297 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.83558\n",
      "Epoch 188/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7640 - acc: 0.7858 - val_loss: 1.1115 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.83558\n",
      "Epoch 189/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7491 - acc: 0.7900 - val_loss: 1.0493 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.83558\n",
      "Epoch 190/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7657 - acc: 0.7840 - val_loss: 1.0597 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.83558\n",
      "Epoch 191/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7654 - acc: 0.7915 - val_loss: 1.1038 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.83558\n",
      "Epoch 192/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7447 - acc: 0.7867 - val_loss: 1.2374 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.83558\n",
      "Epoch 193/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7473 - acc: 0.7900 - val_loss: 1.1973 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.83558\n",
      "Epoch 194/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7437 - acc: 0.7924 - val_loss: 1.1804 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.83558\n",
      "Epoch 195/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7532 - acc: 0.7927 - val_loss: 1.3237 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.83558\n",
      "Epoch 196/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7342 - acc: 0.7960 - val_loss: 1.0750 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.83558\n",
      "Epoch 197/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7323 - acc: 0.8035 - val_loss: 1.1456 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.83558\n",
      "Epoch 198/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7435 - acc: 0.7960 - val_loss: 1.1011 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.83558\n",
      "Epoch 199/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7688 - acc: 0.7951 - val_loss: 1.0672 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.83558\n",
      "Epoch 200/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7558 - acc: 0.7957 - val_loss: 1.3339 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.83558\n",
      "Epoch 201/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7349 - acc: 0.7951 - val_loss: 1.0644 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.83558\n",
      "Epoch 202/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7144 - acc: 0.8092 - val_loss: 1.2054 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.83558\n",
      "Epoch 203/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7128 - acc: 0.8005 - val_loss: 1.0973 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.83558\n",
      "Epoch 204/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7130 - acc: 0.8026 - val_loss: 1.0603 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.83558\n",
      "Epoch 205/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7375 - acc: 0.8068 - val_loss: 1.1104 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.83558\n",
      "Epoch 206/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7256 - acc: 0.7930 - val_loss: 1.2445 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.83558\n",
      "Epoch 207/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7335 - acc: 0.7942 - val_loss: 1.2019 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.83558\n",
      "Epoch 208/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7223 - acc: 0.7948 - val_loss: 1.0854 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.83558\n",
      "Epoch 209/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7022 - acc: 0.8089 - val_loss: 1.1002 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.83558\n",
      "Epoch 210/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7138 - acc: 0.7984 - val_loss: 1.2387 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.83558\n",
      "Epoch 211/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7080 - acc: 0.7999 - val_loss: 1.1467 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.83558\n",
      "Epoch 212/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7346 - acc: 0.7951 - val_loss: 1.2185 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.83558\n",
      "Epoch 213/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7249 - acc: 0.8062 - val_loss: 1.0127 - val_acc: 0.8086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00213: val_acc did not improve from 0.83558\n",
      "Epoch 214/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7023 - acc: 0.8062 - val_loss: 2.4547 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.83558\n",
      "Epoch 215/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6786 - acc: 0.8140 - val_loss: 1.1134 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.83558\n",
      "Epoch 216/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6886 - acc: 0.8179 - val_loss: 1.2818 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.83558\n",
      "Epoch 217/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7292 - acc: 0.7939 - val_loss: 1.0521 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.83558\n",
      "Epoch 218/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7278 - acc: 0.8050 - val_loss: 1.1343 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.83558\n",
      "Epoch 219/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7200 - acc: 0.7996 - val_loss: 1.1406 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.83558\n",
      "Epoch 220/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6923 - acc: 0.7957 - val_loss: 1.0610 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.83558\n",
      "Epoch 221/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7057 - acc: 0.8080 - val_loss: 1.1600 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.83558\n",
      "Epoch 222/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7097 - acc: 0.7939 - val_loss: 1.2054 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.83558\n",
      "Epoch 223/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7013 - acc: 0.8110 - val_loss: 1.1078 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.83558\n",
      "Epoch 224/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6832 - acc: 0.8008 - val_loss: 1.1256 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.83558\n",
      "Epoch 225/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6650 - acc: 0.8116 - val_loss: 1.0189 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00225: val_acc improved from 0.83558 to 0.84097, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 226/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6796 - acc: 0.8062 - val_loss: 1.0791 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.84097\n",
      "Epoch 227/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7024 - acc: 0.8098 - val_loss: 1.2581 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.84097\n",
      "Epoch 228/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7309 - acc: 0.7852 - val_loss: 1.1584 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.84097\n",
      "Epoch 229/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6792 - acc: 0.8071 - val_loss: 1.0977 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.84097\n",
      "Epoch 230/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6811 - acc: 0.8050 - val_loss: 1.0784 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.84097\n",
      "Epoch 231/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6744 - acc: 0.8095 - val_loss: 1.4286 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.84097\n",
      "Epoch 232/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6905 - acc: 0.7975 - val_loss: 1.0885 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.84097\n",
      "Epoch 233/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6854 - acc: 0.8056 - val_loss: 1.2185 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.84097\n",
      "Epoch 234/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6951 - acc: 0.8059 - val_loss: 1.0752 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.84097\n",
      "Epoch 235/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6377 - acc: 0.8281 - val_loss: 1.0598 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.84097\n",
      "Epoch 236/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6853 - acc: 0.8044 - val_loss: 1.3036 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.84097\n",
      "Epoch 237/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6462 - acc: 0.8083 - val_loss: 1.0123 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.84097\n",
      "Epoch 238/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6678 - acc: 0.8227 - val_loss: 1.0957 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.84097\n",
      "Epoch 239/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6386 - acc: 0.8185 - val_loss: 0.9974 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.84097\n",
      "Epoch 240/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6557 - acc: 0.8212 - val_loss: 1.0299 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.84097\n",
      "Epoch 241/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6687 - acc: 0.8086 - val_loss: 1.0773 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.84097\n",
      "Epoch 242/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6780 - acc: 0.8053 - val_loss: 1.2106 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.84097\n",
      "Epoch 243/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6756 - acc: 0.8092 - val_loss: 0.9951 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.84097\n",
      "Epoch 244/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6771 - acc: 0.8071 - val_loss: 1.0531 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.84097\n",
      "Epoch 245/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6698 - acc: 0.8062 - val_loss: 1.2682 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.84097\n",
      "Epoch 246/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6594 - acc: 0.8197 - val_loss: 1.0899 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.84097\n",
      "Epoch 247/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6375 - acc: 0.8146 - val_loss: 1.0863 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.84097\n",
      "Epoch 248/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6561 - acc: 0.8191 - val_loss: 1.3226 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.84097\n",
      "Epoch 249/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6864 - acc: 0.8044 - val_loss: 1.1819 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.84097\n",
      "Epoch 250/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6713 - acc: 0.8098 - val_loss: 1.1937 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.84097\n",
      "Epoch 251/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6535 - acc: 0.8098 - val_loss: 1.1912 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.84097\n",
      "Epoch 252/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6489 - acc: 0.8125 - val_loss: 1.0692 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.84097\n",
      "Epoch 253/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6860 - acc: 0.8158 - val_loss: 1.1639 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.84097\n",
      "Epoch 254/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6789 - acc: 0.8134 - val_loss: 1.1040 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.84097\n",
      "Epoch 255/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6402 - acc: 0.8167 - val_loss: 1.1204 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.84097\n",
      "Epoch 256/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6585 - acc: 0.8137 - val_loss: 1.1037 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.84097\n",
      "Epoch 257/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6196 - acc: 0.8197 - val_loss: 1.3734 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.84097\n",
      "Epoch 258/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6451 - acc: 0.8104 - val_loss: 1.1974 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.84097\n",
      "Epoch 259/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6477 - acc: 0.8239 - val_loss: 1.1408 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.84097\n",
      "Epoch 260/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6743 - acc: 0.8026 - val_loss: 1.0782 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.84097\n",
      "Epoch 261/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6434 - acc: 0.8176 - val_loss: 1.0185 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.84097\n",
      "Epoch 262/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6577 - acc: 0.8194 - val_loss: 1.0547 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.84097\n",
      "Epoch 263/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6427 - acc: 0.8101 - val_loss: 1.2481 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.84097\n",
      "Epoch 264/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6584 - acc: 0.8074 - val_loss: 1.0562 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.84097\n",
      "Epoch 265/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6498 - acc: 0.8134 - val_loss: 0.9890 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.84097\n",
      "Epoch 266/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6427 - acc: 0.8215 - val_loss: 1.2137 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.84097\n",
      "Epoch 267/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6447 - acc: 0.8203 - val_loss: 1.2881 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.84097\n",
      "Epoch 268/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6466 - acc: 0.8251 - val_loss: 1.1002 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.84097\n",
      "Epoch 269/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6626 - acc: 0.8101 - val_loss: 1.1099 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.84097\n",
      "Epoch 270/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6434 - acc: 0.8170 - val_loss: 1.4225 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.84097\n",
      "Epoch 271/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6273 - acc: 0.8197 - val_loss: 1.2017 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.84097\n",
      "Epoch 272/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6685 - acc: 0.8041 - val_loss: 1.1503 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.84097\n",
      "Epoch 273/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6151 - acc: 0.8257 - val_loss: 1.1298 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.84097\n",
      "Epoch 274/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6455 - acc: 0.8191 - val_loss: 1.1493 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.84097\n",
      "Epoch 275/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6268 - acc: 0.8134 - val_loss: 1.1245 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.84097\n",
      "Epoch 276/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6590 - acc: 0.8065 - val_loss: 1.1935 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.84097\n",
      "Epoch 277/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6164 - acc: 0.8254 - val_loss: 0.9927 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.84097\n",
      "Epoch 278/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6277 - acc: 0.8212 - val_loss: 1.0826 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.84097\n",
      "Epoch 279/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6367 - acc: 0.8101 - val_loss: 1.2276 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.84097\n",
      "Epoch 280/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6363 - acc: 0.8134 - val_loss: 1.2375 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.84097\n",
      "Epoch 281/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6329 - acc: 0.8176 - val_loss: 1.0839 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.84097\n",
      "Epoch 282/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6369 - acc: 0.8188 - val_loss: 1.0497 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.84097\n",
      "Epoch 283/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6217 - acc: 0.8176 - val_loss: 1.1372 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.84097\n",
      "Epoch 284/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6449 - acc: 0.8140 - val_loss: 1.1776 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.84097\n",
      "Epoch 285/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6572 - acc: 0.8092 - val_loss: 1.3931 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.84097\n",
      "Epoch 286/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6228 - acc: 0.8281 - val_loss: 1.1438 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.84097\n",
      "Epoch 287/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6476 - acc: 0.8080 - val_loss: 1.0357 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.84097\n",
      "Epoch 288/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6394 - acc: 0.8065 - val_loss: 1.0924 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.84097\n",
      "Epoch 289/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6639 - acc: 0.8011 - val_loss: 1.0355 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.84097\n",
      "Epoch 290/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6198 - acc: 0.8146 - val_loss: 1.1990 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.84097\n",
      "Epoch 291/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6199 - acc: 0.8176 - val_loss: 1.1462 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.84097\n",
      "Epoch 292/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6285 - acc: 0.8068 - val_loss: 1.1114 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.84097\n",
      "Epoch 293/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6202 - acc: 0.8143 - val_loss: 1.2725 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.84097\n",
      "Epoch 294/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6371 - acc: 0.8167 - val_loss: 1.1634 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.84097\n",
      "Epoch 295/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6212 - acc: 0.8251 - val_loss: 1.1453 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.84097\n",
      "Epoch 296/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6243 - acc: 0.8260 - val_loss: 1.1441 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.84097\n",
      "Epoch 297/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6253 - acc: 0.8212 - val_loss: 1.1353 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.84097\n",
      "Epoch 298/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6333 - acc: 0.8125 - val_loss: 1.1323 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.84097\n",
      "Epoch 299/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6193 - acc: 0.8236 - val_loss: 1.1447 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.84097\n",
      "Epoch 300/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6047 - acc: 0.8257 - val_loss: 1.1909 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.84097\n",
      "Epoch 301/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6258 - acc: 0.8137 - val_loss: 1.1918 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.84097\n",
      "Epoch 302/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6409 - acc: 0.8122 - val_loss: 1.0719 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.84097\n",
      "Epoch 303/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6165 - acc: 0.8251 - val_loss: 1.1831 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.84097\n",
      "Epoch 304/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6114 - acc: 0.8182 - val_loss: 1.0449 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.84097\n",
      "Epoch 305/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6182 - acc: 0.8149 - val_loss: 1.0721 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.84097\n",
      "Epoch 306/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6077 - acc: 0.8233 - val_loss: 1.2765 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.84097\n",
      "Epoch 307/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6108 - acc: 0.8239 - val_loss: 1.0095 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.84097\n",
      "Epoch 308/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5935 - acc: 0.8314 - val_loss: 0.9898 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.84097\n",
      "Epoch 309/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6083 - acc: 0.8212 - val_loss: 0.9899 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.84097\n",
      "Epoch 310/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6201 - acc: 0.8266 - val_loss: 1.0725 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.84097\n",
      "Epoch 311/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5915 - acc: 0.8311 - val_loss: 1.0464 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.84097\n",
      "Epoch 312/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5968 - acc: 0.8272 - val_loss: 1.0435 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.84097\n",
      "Epoch 313/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6256 - acc: 0.8116 - val_loss: 1.0590 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.84097\n",
      "Epoch 314/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6066 - acc: 0.8212 - val_loss: 1.2239 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.84097\n",
      "Epoch 315/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6205 - acc: 0.8170 - val_loss: 1.1546 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.84097\n",
      "Epoch 316/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6220 - acc: 0.8137 - val_loss: 1.1748 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.84097\n",
      "Epoch 317/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6208 - acc: 0.8152 - val_loss: 1.0870 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.84097\n",
      "Epoch 318/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5932 - acc: 0.8140 - val_loss: 1.0122 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00318: val_acc improved from 0.84097 to 0.85445, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 319/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6149 - acc: 0.8167 - val_loss: 1.0794 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.85445\n",
      "Epoch 320/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5963 - acc: 0.8257 - val_loss: 1.0935 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.85445\n",
      "Epoch 321/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5929 - acc: 0.8191 - val_loss: 1.1638 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.85445\n",
      "Epoch 322/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5796 - acc: 0.8269 - val_loss: 1.2065 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.85445\n",
      "Epoch 323/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5854 - acc: 0.8230 - val_loss: 1.1454 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.85445\n",
      "Epoch 324/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6142 - acc: 0.7990 - val_loss: 1.1810 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.85445\n",
      "Epoch 325/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6114 - acc: 0.8161 - val_loss: 1.0999 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.85445\n",
      "Epoch 326/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6032 - acc: 0.8182 - val_loss: 1.0357 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.85445\n",
      "Epoch 327/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5903 - acc: 0.8179 - val_loss: 1.1549 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.85445\n",
      "Epoch 328/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6087 - acc: 0.8149 - val_loss: 0.9813 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.85445\n",
      "Epoch 329/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6033 - acc: 0.8224 - val_loss: 1.1901 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.85445\n",
      "Epoch 330/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6108 - acc: 0.8197 - val_loss: 1.2585 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.85445\n",
      "Epoch 331/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5912 - acc: 0.8206 - val_loss: 1.0759 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.85445\n",
      "Epoch 332/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5897 - acc: 0.8293 - val_loss: 1.0809 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.85445\n",
      "Epoch 333/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5936 - acc: 0.8173 - val_loss: 1.1534 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.85445\n",
      "Epoch 334/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6057 - acc: 0.8140 - val_loss: 1.0606 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.85445\n",
      "Epoch 335/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5846 - acc: 0.8317 - val_loss: 1.0405 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.85445\n",
      "Epoch 336/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5895 - acc: 0.8254 - val_loss: 1.1066 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.85445\n",
      "Epoch 337/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5961 - acc: 0.8257 - val_loss: 0.9994 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.85445\n",
      "Epoch 338/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5858 - acc: 0.8191 - val_loss: 1.1438 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.85445\n",
      "Epoch 339/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6059 - acc: 0.8212 - val_loss: 1.1891 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.85445\n",
      "Epoch 340/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5975 - acc: 0.8224 - val_loss: 1.0360 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.85445\n",
      "Epoch 341/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5947 - acc: 0.8107 - val_loss: 1.1687 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.85445\n",
      "Epoch 342/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6003 - acc: 0.8263 - val_loss: 1.0326 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.85445\n",
      "Epoch 343/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6054 - acc: 0.8131 - val_loss: 1.1275 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.85445\n",
      "Epoch 344/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5950 - acc: 0.8290 - val_loss: 0.9868 - val_acc: 0.8383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00344: val_acc did not improve from 0.85445\n",
      "Epoch 345/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5943 - acc: 0.8356 - val_loss: 1.0432 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.85445\n",
      "Epoch 346/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5767 - acc: 0.8374 - val_loss: 1.0208 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.85445\n",
      "Epoch 347/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.5753 - acc: 0.8248 - val_loss: 1.0877 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.85445\n",
      "Epoch 348/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5920 - acc: 0.8269 - val_loss: 1.1040 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.85445\n",
      "Epoch 349/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5800 - acc: 0.8290 - val_loss: 1.1097 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.85445\n",
      "Epoch 350/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5728 - acc: 0.8227 - val_loss: 1.0315 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.85445\n",
      "Epoch 351/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6001 - acc: 0.8062 - val_loss: 1.0286 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.85445\n",
      "Epoch 352/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5881 - acc: 0.8278 - val_loss: 1.0846 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.85445\n",
      "Epoch 353/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5846 - acc: 0.8302 - val_loss: 1.3110 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.85445\n",
      "Epoch 354/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5864 - acc: 0.8137 - val_loss: 1.0697 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.85445\n",
      "Epoch 355/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5861 - acc: 0.8218 - val_loss: 1.0774 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.85445\n",
      "Epoch 356/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5821 - acc: 0.8269 - val_loss: 1.1799 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.85445\n",
      "Epoch 357/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5837 - acc: 0.8170 - val_loss: 1.1011 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.85445\n",
      "Epoch 358/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5775 - acc: 0.8263 - val_loss: 1.0122 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.85445\n",
      "Epoch 359/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5929 - acc: 0.8356 - val_loss: 1.2284 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.85445\n",
      "Epoch 360/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6016 - acc: 0.8143 - val_loss: 1.0544 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.85445\n",
      "Epoch 361/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5780 - acc: 0.8275 - val_loss: 1.2198 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.85445\n",
      "Epoch 362/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5814 - acc: 0.8242 - val_loss: 1.0846 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.85445\n",
      "Epoch 363/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5911 - acc: 0.8236 - val_loss: 1.0406 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.85445\n",
      "Epoch 364/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5604 - acc: 0.8266 - val_loss: 1.2105 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.85445\n",
      "Epoch 365/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5938 - acc: 0.8173 - val_loss: 1.0394 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.85445\n",
      "Epoch 366/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5546 - acc: 0.8302 - val_loss: 1.2183 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.85445\n",
      "Epoch 367/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5703 - acc: 0.8329 - val_loss: 1.0978 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.85445\n",
      "Epoch 368/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5700 - acc: 0.8305 - val_loss: 1.1278 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.85445\n",
      "Epoch 369/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5737 - acc: 0.8278 - val_loss: 0.9967 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.85445\n",
      "Epoch 370/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5634 - acc: 0.8224 - val_loss: 0.9791 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.85445\n",
      "Epoch 371/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5729 - acc: 0.8245 - val_loss: 1.0451 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.85445\n",
      "Epoch 372/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5683 - acc: 0.8203 - val_loss: 1.1760 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.85445\n",
      "Epoch 373/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5925 - acc: 0.8272 - val_loss: 1.2138 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.85445\n",
      "Epoch 374/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5722 - acc: 0.8194 - val_loss: 1.0253 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.85445\n",
      "Epoch 375/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5758 - acc: 0.8179 - val_loss: 1.1015 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.85445\n",
      "Epoch 376/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5956 - acc: 0.8104 - val_loss: 1.1225 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.85445\n",
      "Epoch 377/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5730 - acc: 0.8308 - val_loss: 1.1148 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.85445\n",
      "Epoch 378/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5505 - acc: 0.8278 - val_loss: 1.0749 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.85445\n",
      "Epoch 379/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5862 - acc: 0.8182 - val_loss: 1.0942 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.85445\n",
      "Epoch 380/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5567 - acc: 0.8299 - val_loss: 1.1208 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.85445\n",
      "Epoch 381/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5741 - acc: 0.8143 - val_loss: 1.1334 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.85445\n",
      "Epoch 382/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5892 - acc: 0.8236 - val_loss: 0.9891 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.85445\n",
      "Epoch 383/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5574 - acc: 0.8287 - val_loss: 1.0910 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.85445\n",
      "Epoch 384/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5513 - acc: 0.8320 - val_loss: 1.0386 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.85445\n",
      "Epoch 385/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5659 - acc: 0.8146 - val_loss: 1.0985 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.85445\n",
      "Epoch 386/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5692 - acc: 0.8269 - val_loss: 0.9869 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.85445\n",
      "Epoch 387/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5585 - acc: 0.8275 - val_loss: 1.1452 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.85445\n",
      "Epoch 388/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5663 - acc: 0.8266 - val_loss: 1.1303 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00388: val_acc did not improve from 0.85445\n",
      "Epoch 389/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.5774 - acc: 0.8272 - val_loss: 1.2563 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.85445\n",
      "Epoch 390/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5746 - acc: 0.8263 - val_loss: 1.1668 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.85445\n",
      "Epoch 391/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5764 - acc: 0.8275 - val_loss: 1.0111 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.85445\n",
      "Epoch 392/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5751 - acc: 0.8155 - val_loss: 1.0451 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.85445\n",
      "Epoch 393/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5366 - acc: 0.8404 - val_loss: 1.1378 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.85445\n",
      "Epoch 394/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5729 - acc: 0.8260 - val_loss: 1.0081 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.85445\n",
      "Epoch 395/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5697 - acc: 0.8314 - val_loss: 1.0236 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.85445\n",
      "Epoch 396/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5555 - acc: 0.8278 - val_loss: 1.1440 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.85445\n",
      "Epoch 397/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5616 - acc: 0.8272 - val_loss: 1.0165 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.85445\n",
      "Epoch 398/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5563 - acc: 0.8245 - val_loss: 1.1619 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.85445\n",
      "Epoch 399/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5450 - acc: 0.8326 - val_loss: 1.1788 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.85445\n",
      "Epoch 400/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5648 - acc: 0.8257 - val_loss: 1.1381 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.85445\n",
      "Epoch 401/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5797 - acc: 0.8218 - val_loss: 1.1222 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.85445\n",
      "Epoch 402/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5583 - acc: 0.8257 - val_loss: 1.0346 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.85445\n",
      "Epoch 403/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5572 - acc: 0.8233 - val_loss: 1.1383 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.85445\n",
      "Epoch 404/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5767 - acc: 0.8269 - val_loss: 0.9624 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.85445\n",
      "Epoch 405/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5457 - acc: 0.8188 - val_loss: 1.0143 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.85445\n",
      "Epoch 406/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5586 - acc: 0.8287 - val_loss: 1.0701 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.85445\n",
      "Epoch 407/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5747 - acc: 0.8164 - val_loss: 1.1632 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.85445\n",
      "Epoch 408/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5418 - acc: 0.8425 - val_loss: 1.0405 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.85445\n",
      "Epoch 409/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5492 - acc: 0.8299 - val_loss: 1.1745 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.85445\n",
      "Epoch 410/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5326 - acc: 0.8383 - val_loss: 1.2188 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.85445\n",
      "Epoch 411/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5388 - acc: 0.8377 - val_loss: 1.1287 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.85445\n",
      "Epoch 412/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5549 - acc: 0.8125 - val_loss: 1.1369 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.85445\n",
      "Epoch 413/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5315 - acc: 0.8338 - val_loss: 0.9860 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.85445\n",
      "Epoch 414/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5465 - acc: 0.8260 - val_loss: 1.3130 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.85445\n",
      "Epoch 415/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5458 - acc: 0.8230 - val_loss: 1.1512 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.85445\n",
      "Epoch 416/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5483 - acc: 0.8269 - val_loss: 1.0411 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.85445\n",
      "Epoch 417/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5504 - acc: 0.8146 - val_loss: 1.0739 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.85445\n",
      "Epoch 418/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5500 - acc: 0.8299 - val_loss: 0.9059 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00418: val_acc improved from 0.85445 to 0.85445, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 419/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5692 - acc: 0.8272 - val_loss: 1.0512 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.85445\n",
      "Epoch 420/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5362 - acc: 0.8266 - val_loss: 1.0645 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.85445\n",
      "Epoch 421/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5662 - acc: 0.8191 - val_loss: 1.1039 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.85445\n",
      "Epoch 422/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5371 - acc: 0.8347 - val_loss: 1.0976 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.85445\n",
      "Epoch 423/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5519 - acc: 0.8260 - val_loss: 1.0226 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.85445\n",
      "Epoch 424/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5855 - acc: 0.8176 - val_loss: 1.1140 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.85445\n",
      "Epoch 425/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5520 - acc: 0.8314 - val_loss: 1.1679 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.85445\n",
      "Epoch 426/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5699 - acc: 0.8191 - val_loss: 1.0438 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.85445\n",
      "Epoch 427/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5623 - acc: 0.8296 - val_loss: 1.1030 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.85445\n",
      "Epoch 428/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5528 - acc: 0.8233 - val_loss: 1.0949 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.85445\n",
      "Epoch 429/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5627 - acc: 0.8230 - val_loss: 1.0016 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.85445\n",
      "Epoch 430/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5502 - acc: 0.8266 - val_loss: 1.0966 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.85445\n",
      "Epoch 431/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5632 - acc: 0.8203 - val_loss: 1.0776 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.85445\n",
      "Epoch 432/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5312 - acc: 0.8341 - val_loss: 0.9853 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.85445\n",
      "Epoch 433/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5402 - acc: 0.8335 - val_loss: 0.9896 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.85445\n",
      "Epoch 434/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5688 - acc: 0.8158 - val_loss: 1.6527 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.85445\n",
      "Epoch 435/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5384 - acc: 0.8332 - val_loss: 1.0907 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.85445\n",
      "Epoch 436/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5216 - acc: 0.8383 - val_loss: 1.1250 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.85445\n",
      "Epoch 437/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5451 - acc: 0.8200 - val_loss: 1.4021 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.85445\n",
      "Epoch 438/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5416 - acc: 0.8377 - val_loss: 1.0262 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.85445\n",
      "Epoch 439/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5574 - acc: 0.8257 - val_loss: 1.0288 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.85445\n",
      "Epoch 440/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5584 - acc: 0.8245 - val_loss: 1.0606 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.85445\n",
      "Epoch 441/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5292 - acc: 0.8350 - val_loss: 1.0502 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.85445\n",
      "Epoch 442/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5295 - acc: 0.8344 - val_loss: 1.0278 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.85445\n",
      "Epoch 443/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5469 - acc: 0.8221 - val_loss: 1.0527 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.85445\n",
      "Epoch 444/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5479 - acc: 0.8257 - val_loss: 1.2318 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.85445\n",
      "Epoch 445/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5432 - acc: 0.8296 - val_loss: 1.1268 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.85445\n",
      "Epoch 446/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5217 - acc: 0.8453 - val_loss: 1.0784 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.85445\n",
      "Epoch 447/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5383 - acc: 0.8338 - val_loss: 1.1214 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.85445\n",
      "Epoch 448/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5465 - acc: 0.8356 - val_loss: 1.0711 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.85445\n",
      "Epoch 449/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5337 - acc: 0.8281 - val_loss: 0.9248 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00449: val_acc improved from 0.85445 to 0.87332, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 450/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5155 - acc: 0.8353 - val_loss: 1.0458 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.87332\n",
      "Epoch 451/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5489 - acc: 0.8248 - val_loss: 1.0746 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.87332\n",
      "Epoch 452/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5299 - acc: 0.8341 - val_loss: 0.9743 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.87332\n",
      "Epoch 453/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5074 - acc: 0.8335 - val_loss: 0.9802 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.87332\n",
      "Epoch 454/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5265 - acc: 0.8275 - val_loss: 1.1013 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.87332\n",
      "Epoch 455/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5322 - acc: 0.8377 - val_loss: 1.1619 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.87332\n",
      "Epoch 456/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5337 - acc: 0.8290 - val_loss: 1.1128 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.87332\n",
      "Epoch 457/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5394 - acc: 0.8245 - val_loss: 1.2607 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.87332\n",
      "Epoch 458/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5257 - acc: 0.8242 - val_loss: 1.1747 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.87332\n",
      "Epoch 459/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5162 - acc: 0.8347 - val_loss: 1.0695 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.87332\n",
      "Epoch 460/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5183 - acc: 0.8344 - val_loss: 1.0165 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.87332\n",
      "Epoch 461/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5308 - acc: 0.8293 - val_loss: 1.0346 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.87332\n",
      "Epoch 462/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5283 - acc: 0.8332 - val_loss: 1.0712 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.87332\n",
      "Epoch 463/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5384 - acc: 0.8242 - val_loss: 1.0540 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.87332\n",
      "Epoch 464/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5390 - acc: 0.8176 - val_loss: 1.1303 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.87332\n",
      "Epoch 465/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5340 - acc: 0.8293 - val_loss: 1.0787 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.87332\n",
      "Epoch 466/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5405 - acc: 0.8260 - val_loss: 0.9657 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.87332\n",
      "Epoch 467/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5016 - acc: 0.8404 - val_loss: 1.0563 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.87332\n",
      "Epoch 468/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5272 - acc: 0.8257 - val_loss: 0.9864 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.87332\n",
      "Epoch 469/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5312 - acc: 0.8389 - val_loss: 1.0462 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.87332\n",
      "Epoch 470/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5224 - acc: 0.8296 - val_loss: 1.0203 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.87332\n",
      "Epoch 471/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5285 - acc: 0.8314 - val_loss: 0.9847 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.87332\n",
      "Epoch 472/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5237 - acc: 0.8344 - val_loss: 0.9760 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.87332\n",
      "Epoch 473/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5288 - acc: 0.8269 - val_loss: 1.0796 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.87332\n",
      "Epoch 474/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4938 - acc: 0.8428 - val_loss: 1.0251 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.87332\n",
      "Epoch 475/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5083 - acc: 0.8305 - val_loss: 1.0514 - val_acc: 0.8437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00475: val_acc did not improve from 0.87332\n",
      "Epoch 476/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5243 - acc: 0.8371 - val_loss: 1.0135 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.87332\n",
      "Epoch 477/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5278 - acc: 0.8353 - val_loss: 1.1479 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.87332\n",
      "Epoch 478/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5116 - acc: 0.8438 - val_loss: 1.2744 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.87332\n",
      "Epoch 479/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5108 - acc: 0.8368 - val_loss: 1.0513 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.87332\n",
      "Epoch 480/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5124 - acc: 0.8374 - val_loss: 0.9774 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.87332\n",
      "Epoch 481/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5174 - acc: 0.8438 - val_loss: 1.0502 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.87332\n",
      "Epoch 482/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5247 - acc: 0.8335 - val_loss: 1.0889 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.87332\n",
      "Epoch 483/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5175 - acc: 0.8317 - val_loss: 1.2558 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.87332\n",
      "Epoch 484/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5251 - acc: 0.8389 - val_loss: 1.1325 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.87332\n",
      "Epoch 485/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8329 - val_loss: 1.0691 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.87332\n",
      "Epoch 486/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5210 - acc: 0.8395 - val_loss: 0.9541 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.87332\n",
      "Epoch 487/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5189 - acc: 0.8275 - val_loss: 1.0261 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.87332\n",
      "Epoch 488/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5242 - acc: 0.8380 - val_loss: 1.1889 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.87332\n",
      "Epoch 489/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5112 - acc: 0.8302 - val_loss: 0.9886 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.87332\n",
      "Epoch 490/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5363 - acc: 0.8092 - val_loss: 1.1119 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.87332\n",
      "Epoch 491/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5182 - acc: 0.8257 - val_loss: 1.1021 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.87332\n",
      "Epoch 492/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5375 - acc: 0.8281 - val_loss: 1.0447 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.87332\n",
      "Epoch 493/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5385 - acc: 0.8194 - val_loss: 1.1846 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.87332\n",
      "Epoch 494/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5136 - acc: 0.8422 - val_loss: 1.0755 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.87332\n",
      "Epoch 495/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5110 - acc: 0.8338 - val_loss: 1.0331 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.87332\n",
      "Epoch 496/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8311 - val_loss: 1.0633 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.87332\n",
      "Epoch 497/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5369 - acc: 0.8347 - val_loss: 0.9867 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.87332\n",
      "Epoch 498/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4969 - acc: 0.8392 - val_loss: 0.9712 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.87332\n",
      "Epoch 499/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4917 - acc: 0.8356 - val_loss: 0.9790 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.87332\n",
      "Epoch 500/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5142 - acc: 0.8260 - val_loss: 1.0288 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.87332\n",
      "Epoch 501/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5283 - acc: 0.8320 - val_loss: 1.0820 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00501: val_acc did not improve from 0.87332\n",
      "Epoch 502/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5406 - acc: 0.8299 - val_loss: 1.0942 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00502: val_acc did not improve from 0.87332\n",
      "Epoch 503/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8368 - val_loss: 1.1077 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00503: val_acc did not improve from 0.87332\n",
      "Epoch 504/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5194 - acc: 0.8374 - val_loss: 1.0882 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00504: val_acc did not improve from 0.87332\n",
      "Epoch 505/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5259 - acc: 0.8224 - val_loss: 0.9653 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00505: val_acc did not improve from 0.87332\n",
      "Epoch 506/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5135 - acc: 0.8371 - val_loss: 1.1401 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00506: val_acc did not improve from 0.87332\n",
      "Epoch 507/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5151 - acc: 0.8308 - val_loss: 1.0324 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00507: val_acc did not improve from 0.87332\n",
      "Epoch 508/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.4993 - acc: 0.8329 - val_loss: 1.1463 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00508: val_acc did not improve from 0.87332\n",
      "Epoch 509/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5040 - acc: 0.8401 - val_loss: 1.0510 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00509: val_acc did not improve from 0.87332\n",
      "Epoch 510/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4966 - acc: 0.8404 - val_loss: 1.0626 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00510: val_acc did not improve from 0.87332\n",
      "Epoch 511/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4792 - acc: 0.8278 - val_loss: 0.9711 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00511: val_acc did not improve from 0.87332\n",
      "Epoch 512/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5229 - acc: 0.8281 - val_loss: 1.1331 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00512: val_acc did not improve from 0.87332\n",
      "Epoch 513/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5314 - acc: 0.8398 - val_loss: 1.0362 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00513: val_acc did not improve from 0.87332\n",
      "Epoch 514/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5035 - acc: 0.8389 - val_loss: 0.9269 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00514: val_acc did not improve from 0.87332\n",
      "Epoch 515/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5077 - acc: 0.8359 - val_loss: 1.0504 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00515: val_acc did not improve from 0.87332\n",
      "Epoch 516/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5046 - acc: 0.8341 - val_loss: 0.9737 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00516: val_acc did not improve from 0.87332\n",
      "Epoch 517/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5091 - acc: 0.8299 - val_loss: 1.0048 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00517: val_acc did not improve from 0.87332\n",
      "Epoch 518/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.4953 - acc: 0.8410 - val_loss: 1.0361 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00518: val_acc did not improve from 0.87332\n",
      "Epoch 00518: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_9===\n",
      "semi loading: model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/3000\n",
      "53/53 [==============================] - 12s 229ms/step - loss: 2.1275 - acc: 0.6176 - val_loss: 0.8698 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.88410, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 6/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.9640 - acc: 0.6775 - val_loss: 0.8594 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88410\n",
      "Epoch 7/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.8558 - acc: 0.7117 - val_loss: 0.8715 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88410\n",
      "Epoch 8/3000\n",
      "53/53 [==============================] - 6s 116ms/step - loss: 1.8232 - acc: 0.7158 - val_loss: 0.8623 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88410\n",
      "Epoch 9/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.7704 - acc: 0.7344 - val_loss: 0.8595 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88410\n",
      "Epoch 10/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.7144 - acc: 0.7497 - val_loss: 0.8554 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88410\n",
      "Epoch 11/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6810 - acc: 0.7668 - val_loss: 0.8691 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88410\n",
      "Epoch 12/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6902 - acc: 0.7574 - val_loss: 0.8457 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88410\n",
      "Epoch 13/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6438 - acc: 0.7792 - val_loss: 0.8265 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88410\n",
      "Epoch 14/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6545 - acc: 0.7730 - val_loss: 0.8475 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88410\n",
      "Epoch 15/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.6478 - acc: 0.7807 - val_loss: 0.8513 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88410\n",
      "Epoch 16/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6244 - acc: 0.7795 - val_loss: 0.8369 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88410\n",
      "Epoch 17/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5760 - acc: 0.7998 - val_loss: 0.8336 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88410\n",
      "Epoch 18/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6021 - acc: 0.7857 - val_loss: 0.8435 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88410\n",
      "Epoch 19/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5826 - acc: 0.7922 - val_loss: 0.8236 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88410\n",
      "Epoch 20/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5762 - acc: 0.8090 - val_loss: 0.8159 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88410\n",
      "Epoch 21/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5575 - acc: 0.8028 - val_loss: 0.8023 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88410\n",
      "Epoch 22/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5580 - acc: 0.8084 - val_loss: 0.8128 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88410\n",
      "Epoch 23/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5555 - acc: 0.8075 - val_loss: 0.8175 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88410\n",
      "Epoch 24/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5170 - acc: 0.8060 - val_loss: 0.8271 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88410\n",
      "Epoch 25/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4982 - acc: 0.8296 - val_loss: 0.8081 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88410\n",
      "Epoch 26/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5266 - acc: 0.8134 - val_loss: 0.7970 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88410\n",
      "Epoch 27/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5173 - acc: 0.8078 - val_loss: 0.7929 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88410\n",
      "Epoch 28/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5162 - acc: 0.8116 - val_loss: 0.7962 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88410\n",
      "Epoch 29/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5289 - acc: 0.8075 - val_loss: 0.7933 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88410\n",
      "Epoch 30/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5115 - acc: 0.8261 - val_loss: 0.7977 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88410\n",
      "Epoch 31/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5124 - acc: 0.8172 - val_loss: 0.7834 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88410\n",
      "Epoch 32/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4979 - acc: 0.8166 - val_loss: 0.8151 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88410\n",
      "Epoch 33/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4818 - acc: 0.8157 - val_loss: 0.8076 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88410\n",
      "Epoch 34/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4813 - acc: 0.8202 - val_loss: 0.8019 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88410\n",
      "Epoch 35/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4594 - acc: 0.8337 - val_loss: 0.7971 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88410\n",
      "Epoch 36/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4663 - acc: 0.8275 - val_loss: 0.8042 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88410\n",
      "Epoch 37/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4862 - acc: 0.8334 - val_loss: 0.8274 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88410\n",
      "Epoch 38/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4662 - acc: 0.8370 - val_loss: 0.8192 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88410\n",
      "Epoch 39/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4768 - acc: 0.8216 - val_loss: 0.8054 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88410\n",
      "Epoch 40/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4679 - acc: 0.8293 - val_loss: 0.7918 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88410\n",
      "Epoch 41/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4944 - acc: 0.8272 - val_loss: 0.7974 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88410\n",
      "Epoch 42/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4569 - acc: 0.8290 - val_loss: 0.8028 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88410\n",
      "Epoch 43/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4474 - acc: 0.8296 - val_loss: 0.8237 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88410\n",
      "Epoch 44/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4271 - acc: 0.8370 - val_loss: 0.7880 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88410\n",
      "Epoch 45/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4513 - acc: 0.8379 - val_loss: 0.8002 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88410\n",
      "Epoch 46/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4158 - acc: 0.8373 - val_loss: 0.8003 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88410\n",
      "Epoch 47/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4540 - acc: 0.8293 - val_loss: 0.7930 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88410\n",
      "Epoch 48/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4229 - acc: 0.8429 - val_loss: 0.8211 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88410\n",
      "Epoch 49/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4259 - acc: 0.8435 - val_loss: 0.8058 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88410\n",
      "Epoch 50/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4271 - acc: 0.8358 - val_loss: 0.7939 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88410\n",
      "Epoch 51/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4341 - acc: 0.8231 - val_loss: 0.7974 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88410\n",
      "Epoch 52/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4396 - acc: 0.8302 - val_loss: 0.7965 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88410\n",
      "Epoch 53/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4057 - acc: 0.8455 - val_loss: 0.8038 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88410\n",
      "Epoch 54/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4310 - acc: 0.8358 - val_loss: 0.7976 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88410\n",
      "Epoch 55/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4235 - acc: 0.8355 - val_loss: 0.7952 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88410\n",
      "Epoch 56/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3852 - acc: 0.8538 - val_loss: 0.8042 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88410\n",
      "Epoch 57/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3949 - acc: 0.8346 - val_loss: 0.7949 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88410\n",
      "Epoch 58/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4120 - acc: 0.8414 - val_loss: 0.7872 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88410\n",
      "Epoch 59/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3858 - acc: 0.8499 - val_loss: 0.7961 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88410\n",
      "Epoch 60/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3755 - acc: 0.8508 - val_loss: 0.7672 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.88410 to 0.88949, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 61/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4319 - acc: 0.8278 - val_loss: 0.8104 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88949\n",
      "Epoch 62/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4173 - acc: 0.8514 - val_loss: 0.7709 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88949\n",
      "Epoch 63/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3943 - acc: 0.8496 - val_loss: 0.7859 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88949\n",
      "Epoch 64/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3813 - acc: 0.8470 - val_loss: 0.7789 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88949\n",
      "Epoch 65/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3919 - acc: 0.8573 - val_loss: 0.7583 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88949\n",
      "Epoch 66/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3846 - acc: 0.8517 - val_loss: 0.7651 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88949\n",
      "Epoch 67/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3987 - acc: 0.8411 - val_loss: 0.7502 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88949\n",
      "Epoch 68/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3707 - acc: 0.8523 - val_loss: 0.7627 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88949\n",
      "Epoch 69/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3911 - acc: 0.8426 - val_loss: 0.7665 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88949\n",
      "Epoch 70/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3935 - acc: 0.8352 - val_loss: 0.7745 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88949\n",
      "Epoch 71/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3876 - acc: 0.8502 - val_loss: 0.7791 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88949\n",
      "Epoch 72/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3947 - acc: 0.8458 - val_loss: 0.7885 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88949\n",
      "Epoch 73/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3671 - acc: 0.8576 - val_loss: 0.7813 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88949\n",
      "Epoch 74/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3656 - acc: 0.8555 - val_loss: 0.7834 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88949\n",
      "Epoch 75/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3780 - acc: 0.8461 - val_loss: 0.7898 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88949\n",
      "Epoch 76/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3557 - acc: 0.8541 - val_loss: 0.7958 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88949\n",
      "Epoch 77/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3453 - acc: 0.8600 - val_loss: 0.7556 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88949\n",
      "Epoch 78/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3822 - acc: 0.8488 - val_loss: 0.7700 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88949\n",
      "Epoch 79/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3810 - acc: 0.8502 - val_loss: 0.7567 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88949\n",
      "Epoch 80/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3246 - acc: 0.8576 - val_loss: 0.7528 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.88949 to 0.89218, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 81/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3488 - acc: 0.8532 - val_loss: 0.7599 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89218\n",
      "Epoch 82/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3668 - acc: 0.8470 - val_loss: 0.7552 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89218\n",
      "Epoch 83/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3795 - acc: 0.8449 - val_loss: 0.7567 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89218\n",
      "Epoch 84/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3650 - acc: 0.8435 - val_loss: 0.7477 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89218\n",
      "Epoch 85/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3606 - acc: 0.8573 - val_loss: 0.7644 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89218\n",
      "Epoch 86/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3453 - acc: 0.8617 - val_loss: 0.7666 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89218\n",
      "Epoch 87/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3426 - acc: 0.8547 - val_loss: 0.7629 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89218\n",
      "Epoch 88/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3875 - acc: 0.8600 - val_loss: 0.7703 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89218\n",
      "Epoch 89/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3353 - acc: 0.8723 - val_loss: 0.7334 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.89218 to 0.89488, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 90/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3581 - acc: 0.8629 - val_loss: 0.7364 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.89488 to 0.89757, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 91/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3473 - acc: 0.8532 - val_loss: 0.7410 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89757\n",
      "Epoch 92/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3442 - acc: 0.8555 - val_loss: 0.7491 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89757\n",
      "Epoch 93/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3381 - acc: 0.8552 - val_loss: 0.7533 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89757\n",
      "Epoch 94/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3271 - acc: 0.8667 - val_loss: 0.7395 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89757\n",
      "Epoch 95/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3423 - acc: 0.8688 - val_loss: 0.7435 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89757\n",
      "Epoch 96/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3362 - acc: 0.8694 - val_loss: 0.7391 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89757\n",
      "Epoch 97/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3395 - acc: 0.8603 - val_loss: 0.7373 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89757\n",
      "Epoch 98/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3570 - acc: 0.8523 - val_loss: 0.7341 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89757\n",
      "Epoch 99/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3319 - acc: 0.8608 - val_loss: 0.7351 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89757\n",
      "Epoch 100/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3320 - acc: 0.8638 - val_loss: 0.7455 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89757\n",
      "Epoch 101/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3077 - acc: 0.8688 - val_loss: 0.7452 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89757\n",
      "Epoch 102/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3172 - acc: 0.8608 - val_loss: 0.7450 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89757\n",
      "Epoch 103/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3439 - acc: 0.8508 - val_loss: 0.7416 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89757\n",
      "Epoch 104/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3134 - acc: 0.8673 - val_loss: 0.7423 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89757\n",
      "Epoch 105/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3211 - acc: 0.8588 - val_loss: 0.7348 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89757\n",
      "Epoch 106/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3246 - acc: 0.8603 - val_loss: 0.7327 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89757\n",
      "Epoch 107/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3166 - acc: 0.8644 - val_loss: 0.7363 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89757\n",
      "Epoch 108/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3210 - acc: 0.8523 - val_loss: 0.7444 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89757\n",
      "Epoch 109/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3236 - acc: 0.8544 - val_loss: 0.7466 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89757\n",
      "Epoch 110/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3095 - acc: 0.8729 - val_loss: 0.7397 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89757\n",
      "Epoch 111/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3110 - acc: 0.8647 - val_loss: 0.7403 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89757\n",
      "Epoch 112/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3290 - acc: 0.8526 - val_loss: 0.7382 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89757\n",
      "Epoch 113/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3091 - acc: 0.8667 - val_loss: 0.7363 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00113: val_acc improved from 0.89757 to 0.90027, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 114/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3017 - acc: 0.8588 - val_loss: 0.7340 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90027\n",
      "Epoch 115/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3249 - acc: 0.8579 - val_loss: 0.7342 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90027\n",
      "Epoch 116/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3140 - acc: 0.8606 - val_loss: 0.7355 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.90027 to 0.90296, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 117/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3086 - acc: 0.8629 - val_loss: 0.7363 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90296\n",
      "Epoch 118/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3069 - acc: 0.8679 - val_loss: 0.7367 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90296\n",
      "Epoch 119/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3166 - acc: 0.8700 - val_loss: 0.7396 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90296\n",
      "Epoch 120/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2784 - acc: 0.8756 - val_loss: 0.7369 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90296\n",
      "Epoch 121/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3146 - acc: 0.8694 - val_loss: 0.7353 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90296\n",
      "Epoch 122/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3203 - acc: 0.8591 - val_loss: 0.7356 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90296\n",
      "Epoch 123/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3106 - acc: 0.8538 - val_loss: 0.7365 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90296\n",
      "Epoch 124/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3268 - acc: 0.8576 - val_loss: 0.7364 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90296\n",
      "Epoch 125/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3063 - acc: 0.8721 - val_loss: 0.7383 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90296\n",
      "Epoch 126/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8662 - val_loss: 0.7414 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90296\n",
      "Epoch 127/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2993 - acc: 0.8726 - val_loss: 0.7416 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90296\n",
      "Epoch 128/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8685 - val_loss: 0.7381 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.90296 to 0.90566, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 129/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3287 - acc: 0.8614 - val_loss: 0.7382 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90566\n",
      "Epoch 130/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8576 - val_loss: 0.7386 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90566\n",
      "Epoch 131/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2911 - acc: 0.8623 - val_loss: 0.7391 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90566\n",
      "Epoch 132/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3036 - acc: 0.8576 - val_loss: 0.7383 - val_acc: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90566\n",
      "Epoch 133/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2915 - acc: 0.8617 - val_loss: 0.7379 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90566\n",
      "Epoch 134/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2984 - acc: 0.8735 - val_loss: 0.7374 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90566\n",
      "Epoch 135/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3162 - acc: 0.8647 - val_loss: 0.7385 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90566\n",
      "Epoch 136/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3096 - acc: 0.8597 - val_loss: 0.7406 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90566\n",
      "Epoch 137/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3134 - acc: 0.8603 - val_loss: 0.7394 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90566\n",
      "Epoch 138/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3081 - acc: 0.8691 - val_loss: 0.7406 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90566\n",
      "Epoch 139/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3024 - acc: 0.8682 - val_loss: 0.7391 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90566\n",
      "Epoch 140/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3272 - acc: 0.8617 - val_loss: 0.7392 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90566\n",
      "Epoch 141/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8632 - val_loss: 0.7387 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90566\n",
      "Epoch 142/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8685 - val_loss: 0.7363 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90566\n",
      "Epoch 143/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3148 - acc: 0.8608 - val_loss: 0.7360 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90566\n",
      "Epoch 144/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3297 - acc: 0.8608 - val_loss: 0.7370 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90566\n",
      "Epoch 145/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3079 - acc: 0.8659 - val_loss: 0.7373 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90566\n",
      "Epoch 146/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3241 - acc: 0.8647 - val_loss: 0.7364 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90566\n",
      "Epoch 147/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3092 - acc: 0.8606 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90566\n",
      "Epoch 148/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3136 - acc: 0.8502 - val_loss: 0.7358 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90566\n",
      "Epoch 149/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3223 - acc: 0.8623 - val_loss: 0.7353 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90566\n",
      "Epoch 150/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3175 - acc: 0.8641 - val_loss: 0.7359 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90566\n",
      "Epoch 151/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2974 - acc: 0.8606 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.90566\n",
      "Epoch 152/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3132 - acc: 0.8579 - val_loss: 0.7349 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.90566\n",
      "Epoch 153/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3007 - acc: 0.8768 - val_loss: 0.7347 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.90566\n",
      "Epoch 154/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3159 - acc: 0.8582 - val_loss: 0.7354 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.90566\n",
      "Epoch 155/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3173 - acc: 0.8626 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.90566\n",
      "Epoch 156/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2986 - acc: 0.8700 - val_loss: 0.7346 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.90566\n",
      "Epoch 157/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3102 - acc: 0.8567 - val_loss: 0.7378 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.90566\n",
      "Epoch 158/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2788 - acc: 0.8747 - val_loss: 0.7364 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.90566\n",
      "Epoch 159/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3069 - acc: 0.8697 - val_loss: 0.7359 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.90566\n",
      "Epoch 160/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2902 - acc: 0.8726 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.90566\n",
      "Epoch 161/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2976 - acc: 0.8685 - val_loss: 0.7365 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.90566\n",
      "Epoch 162/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2991 - acc: 0.8818 - val_loss: 0.7368 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.90566\n",
      "Epoch 163/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2871 - acc: 0.8741 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.90566\n",
      "Epoch 164/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2999 - acc: 0.8670 - val_loss: 0.7366 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.90566\n",
      "Epoch 165/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3148 - acc: 0.8659 - val_loss: 0.7375 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.90566\n",
      "Epoch 166/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3191 - acc: 0.8594 - val_loss: 0.7367 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.90566\n",
      "Epoch 167/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3145 - acc: 0.8670 - val_loss: 0.7379 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.90566\n",
      "Epoch 168/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2969 - acc: 0.8641 - val_loss: 0.7360 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.90566\n",
      "Epoch 169/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3032 - acc: 0.8620 - val_loss: 0.7375 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.90566\n",
      "Epoch 170/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3062 - acc: 0.8650 - val_loss: 0.7381 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.90566\n",
      "Epoch 171/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3077 - acc: 0.8582 - val_loss: 0.7373 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.90566\n",
      "Epoch 172/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2950 - acc: 0.8579 - val_loss: 0.7359 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.90566\n",
      "Epoch 173/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3065 - acc: 0.8679 - val_loss: 0.7368 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.90566\n",
      "Epoch 174/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3013 - acc: 0.8667 - val_loss: 0.7358 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.90566\n",
      "Epoch 175/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3034 - acc: 0.8715 - val_loss: 0.7342 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.90566\n",
      "Epoch 176/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3109 - acc: 0.8629 - val_loss: 0.7346 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.90566\n",
      "Epoch 177/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2943 - acc: 0.8588 - val_loss: 0.7350 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.90566\n",
      "Epoch 178/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3049 - acc: 0.8712 - val_loss: 0.7357 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.90566\n",
      "Epoch 179/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3020 - acc: 0.8591 - val_loss: 0.7363 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.90566\n",
      "Epoch 180/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3139 - acc: 0.8632 - val_loss: 0.7364 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.90566\n",
      "Epoch 181/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3079 - acc: 0.8682 - val_loss: 0.7350 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.90566\n",
      "Epoch 182/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3128 - acc: 0.8623 - val_loss: 0.7322 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.90566\n",
      "Epoch 183/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3006 - acc: 0.8670 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.90566\n",
      "Epoch 184/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3019 - acc: 0.8762 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.90566\n",
      "Epoch 185/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2916 - acc: 0.8608 - val_loss: 0.7342 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.90566\n",
      "Epoch 186/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2769 - acc: 0.8759 - val_loss: 0.7340 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.90566\n",
      "Epoch 187/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3184 - acc: 0.8670 - val_loss: 0.7342 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.90566\n",
      "Epoch 188/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8665 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.90566\n",
      "Epoch 189/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3059 - acc: 0.8635 - val_loss: 0.7327 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.90566\n",
      "Epoch 190/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3189 - acc: 0.8594 - val_loss: 0.7321 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.90566\n",
      "Epoch 191/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3125 - acc: 0.8712 - val_loss: 0.7319 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.90566\n",
      "Epoch 192/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2935 - acc: 0.8726 - val_loss: 0.7315 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.90566\n",
      "Epoch 193/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3041 - acc: 0.8744 - val_loss: 0.7317 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.90566\n",
      "Epoch 194/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3045 - acc: 0.8591 - val_loss: 0.7322 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.90566\n",
      "Epoch 195/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2920 - acc: 0.8688 - val_loss: 0.7328 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.90566\n",
      "Epoch 196/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3036 - acc: 0.8762 - val_loss: 0.7345 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.90566\n",
      "Epoch 197/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2991 - acc: 0.8726 - val_loss: 0.7338 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.90566\n",
      "Epoch 198/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3062 - acc: 0.8641 - val_loss: 0.7339 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.90566\n",
      "Epoch 199/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3002 - acc: 0.8620 - val_loss: 0.7343 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.90566\n",
      "Epoch 200/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3033 - acc: 0.8623 - val_loss: 0.7356 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.90566\n",
      "Epoch 201/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3129 - acc: 0.8700 - val_loss: 0.7353 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.90566\n",
      "Epoch 202/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2815 - acc: 0.8729 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.90566\n",
      "Epoch 203/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3102 - acc: 0.8632 - val_loss: 0.7320 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.90566\n",
      "Epoch 204/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2948 - acc: 0.8726 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.90566\n",
      "Epoch 205/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3185 - acc: 0.8653 - val_loss: 0.7319 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.90566\n",
      "Epoch 206/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3194 - acc: 0.8620 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.90566\n",
      "Epoch 207/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3065 - acc: 0.8653 - val_loss: 0.7326 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.90566\n",
      "Epoch 208/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3177 - acc: 0.8653 - val_loss: 0.7341 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.90566\n",
      "Epoch 209/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2984 - acc: 0.8762 - val_loss: 0.7343 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.90566\n",
      "Epoch 210/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3082 - acc: 0.8561 - val_loss: 0.7330 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.90566\n",
      "Epoch 211/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3003 - acc: 0.8665 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.90566\n",
      "Epoch 212/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2895 - acc: 0.8614 - val_loss: 0.7343 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.90566\n",
      "Epoch 213/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3004 - acc: 0.8782 - val_loss: 0.7336 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.90566\n",
      "Epoch 214/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2756 - acc: 0.8732 - val_loss: 0.7329 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.90566\n",
      "Epoch 215/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3099 - acc: 0.8650 - val_loss: 0.7324 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.90566\n",
      "Epoch 216/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8632 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.90566\n",
      "Epoch 217/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8685 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.90566\n",
      "Epoch 218/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2864 - acc: 0.8670 - val_loss: 0.7334 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.90566\n",
      "Epoch 219/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3110 - acc: 0.8732 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.90566\n",
      "Epoch 220/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2873 - acc: 0.8697 - val_loss: 0.7340 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.90566\n",
      "Epoch 221/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3053 - acc: 0.8653 - val_loss: 0.7355 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.90566\n",
      "Epoch 222/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8570 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.90566\n",
      "Epoch 223/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3140 - acc: 0.8641 - val_loss: 0.7356 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.90566\n",
      "Epoch 224/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3066 - acc: 0.8659 - val_loss: 0.7358 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.90566\n",
      "Epoch 225/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2930 - acc: 0.8726 - val_loss: 0.7358 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.90566\n",
      "Epoch 226/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2889 - acc: 0.8738 - val_loss: 0.7373 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.90566\n",
      "Epoch 227/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3028 - acc: 0.8600 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.90566\n",
      "Epoch 228/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3174 - acc: 0.8679 - val_loss: 0.7323 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.90566\n",
      "Epoch 229/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2974 - acc: 0.8635 - val_loss: 0.7330 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.90566\n",
      "Epoch 230/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2896 - acc: 0.8688 - val_loss: 0.7335 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.90566\n",
      "Epoch 231/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2980 - acc: 0.8591 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.90566\n",
      "Epoch 232/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3015 - acc: 0.8715 - val_loss: 0.7335 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.90566\n",
      "Epoch 233/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2969 - acc: 0.8676 - val_loss: 0.7351 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.90566\n",
      "Epoch 234/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2853 - acc: 0.8673 - val_loss: 0.7349 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.90566\n",
      "Epoch 235/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3108 - acc: 0.8726 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.90566\n",
      "Epoch 236/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3028 - acc: 0.8659 - val_loss: 0.7341 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.90566\n",
      "Epoch 237/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2951 - acc: 0.8694 - val_loss: 0.7340 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.90566\n",
      "Epoch 238/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8715 - val_loss: 0.7361 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.90566\n",
      "Epoch 239/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3281 - acc: 0.8597 - val_loss: 0.7353 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.90566\n",
      "Epoch 240/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3053 - acc: 0.8626 - val_loss: 0.7358 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.90566\n",
      "Epoch 241/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3004 - acc: 0.8659 - val_loss: 0.7341 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.90566\n",
      "Epoch 242/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2770 - acc: 0.8744 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.90566\n",
      "Epoch 243/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2881 - acc: 0.8688 - val_loss: 0.7367 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.90566\n",
      "Epoch 244/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2998 - acc: 0.8709 - val_loss: 0.7366 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.90566\n",
      "Epoch 245/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2769 - acc: 0.8676 - val_loss: 0.7371 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.90566\n",
      "Epoch 246/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2911 - acc: 0.8741 - val_loss: 0.7374 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.90566\n",
      "Epoch 247/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3090 - acc: 0.8718 - val_loss: 0.7366 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.90566\n",
      "Epoch 248/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8694 - val_loss: 0.7365 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.90566\n",
      "Epoch 249/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3045 - acc: 0.8756 - val_loss: 0.7354 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.90566\n",
      "Epoch 250/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3044 - acc: 0.8679 - val_loss: 0.7338 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.90566\n",
      "Epoch 251/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2888 - acc: 0.8679 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.90566\n",
      "Epoch 252/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2927 - acc: 0.8632 - val_loss: 0.7344 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.90566\n",
      "Epoch 253/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3071 - acc: 0.8647 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.90566\n",
      "Epoch 254/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2958 - acc: 0.8608 - val_loss: 0.7334 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.90566\n",
      "Epoch 255/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3184 - acc: 0.8620 - val_loss: 0.7344 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.90566\n",
      "Epoch 256/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2906 - acc: 0.8741 - val_loss: 0.7343 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.90566\n",
      "Epoch 257/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3106 - acc: 0.8641 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.90566\n",
      "Epoch 258/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3257 - acc: 0.8697 - val_loss: 0.7338 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.90566\n",
      "Epoch 259/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3000 - acc: 0.8638 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.90566\n",
      "Epoch 260/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2966 - acc: 0.8591 - val_loss: 0.7286 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.90566\n",
      "Epoch 261/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2896 - acc: 0.8667 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.90566\n",
      "Epoch 262/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3042 - acc: 0.8603 - val_loss: 0.7337 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.90566\n",
      "Epoch 263/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2920 - acc: 0.8659 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.90566\n",
      "Epoch 264/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2962 - acc: 0.8765 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.90566\n",
      "Epoch 265/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3079 - acc: 0.8662 - val_loss: 0.7305 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.90566\n",
      "Epoch 266/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2895 - acc: 0.8732 - val_loss: 0.7311 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.90566\n",
      "Epoch 267/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2971 - acc: 0.8673 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.90566\n",
      "Epoch 268/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3011 - acc: 0.8788 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.90566\n",
      "Epoch 269/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2999 - acc: 0.8726 - val_loss: 0.7321 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.90566\n",
      "Epoch 270/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3090 - acc: 0.8623 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.90566\n",
      "Epoch 271/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3135 - acc: 0.8538 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.90566\n",
      "Epoch 272/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3008 - acc: 0.8753 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.90566\n",
      "Epoch 273/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8721 - val_loss: 0.7323 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.90566\n",
      "Epoch 274/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2997 - acc: 0.8620 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.90566\n",
      "Epoch 275/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2888 - acc: 0.8715 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.90566\n",
      "Epoch 276/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3104 - acc: 0.8608 - val_loss: 0.7331 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.90566\n",
      "Epoch 277/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2952 - acc: 0.8659 - val_loss: 0.7315 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.90566\n",
      "Epoch 278/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2968 - acc: 0.8732 - val_loss: 0.7291 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.90566\n",
      "Epoch 279/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3063 - acc: 0.8679 - val_loss: 0.7297 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.90566\n",
      "Epoch 280/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2870 - acc: 0.8732 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.90566\n",
      "Epoch 281/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2830 - acc: 0.8679 - val_loss: 0.7331 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.90566\n",
      "Epoch 282/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3134 - acc: 0.8623 - val_loss: 0.7325 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.90566\n",
      "Epoch 283/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8638 - val_loss: 0.7335 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.90566\n",
      "Epoch 284/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2995 - acc: 0.8653 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.90566\n",
      "Epoch 285/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2881 - acc: 0.8682 - val_loss: 0.7349 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.90566\n",
      "Epoch 286/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2892 - acc: 0.8653 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.90566\n",
      "Epoch 287/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3173 - acc: 0.8614 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.90566\n",
      "Epoch 288/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2861 - acc: 0.8762 - val_loss: 0.7317 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.90566\n",
      "Epoch 289/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2836 - acc: 0.8741 - val_loss: 0.7308 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.90566\n",
      "Epoch 290/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2981 - acc: 0.8738 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.90566\n",
      "Epoch 291/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2873 - acc: 0.8644 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.90566\n",
      "Epoch 292/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3049 - acc: 0.8697 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.90566\n",
      "Epoch 293/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3075 - acc: 0.8726 - val_loss: 0.7312 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.90566\n",
      "Epoch 294/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2938 - acc: 0.8726 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.90566\n",
      "Epoch 295/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3004 - acc: 0.8647 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.90566\n",
      "Epoch 296/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3044 - acc: 0.8576 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.90566\n",
      "Epoch 297/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2738 - acc: 0.8735 - val_loss: 0.7326 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.90566\n",
      "Epoch 298/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3127 - acc: 0.8638 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.90566\n",
      "Epoch 299/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3196 - acc: 0.8535 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.90566\n",
      "Epoch 300/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2933 - acc: 0.8806 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.90566\n",
      "Epoch 301/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2782 - acc: 0.8729 - val_loss: 0.7336 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.90566\n",
      "Epoch 302/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2762 - acc: 0.8715 - val_loss: 0.7332 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.90566\n",
      "Epoch 303/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2831 - acc: 0.8735 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.90566\n",
      "Epoch 304/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2892 - acc: 0.8597 - val_loss: 0.7321 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.90566\n",
      "Epoch 305/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8614 - val_loss: 0.7322 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.90566\n",
      "Epoch 306/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3003 - acc: 0.8709 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.90566\n",
      "Epoch 307/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3016 - acc: 0.8712 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.90566\n",
      "Epoch 308/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8650 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.90566\n",
      "Epoch 309/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8806 - val_loss: 0.7327 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.90566\n",
      "Epoch 310/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2897 - acc: 0.8659 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.90566\n",
      "Epoch 311/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3001 - acc: 0.8656 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.90566\n",
      "Epoch 312/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3035 - acc: 0.8623 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.90566\n",
      "Epoch 313/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3029 - acc: 0.8603 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.90566\n",
      "Epoch 314/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2825 - acc: 0.8676 - val_loss: 0.7351 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.90566\n",
      "Epoch 315/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2862 - acc: 0.8641 - val_loss: 0.7334 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.90566\n",
      "Epoch 316/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2810 - acc: 0.8688 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.90566\n",
      "Epoch 317/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2765 - acc: 0.8818 - val_loss: 0.7307 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.90566\n",
      "Epoch 318/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2974 - acc: 0.8650 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.90566\n",
      "Epoch 319/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3060 - acc: 0.8670 - val_loss: 0.7286 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.90566\n",
      "Epoch 320/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2887 - acc: 0.8694 - val_loss: 0.7300 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.90566\n",
      "Epoch 321/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3102 - acc: 0.8626 - val_loss: 0.7296 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.90566\n",
      "Epoch 322/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2848 - acc: 0.8723 - val_loss: 0.7296 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.90566\n",
      "Epoch 323/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2803 - acc: 0.8877 - val_loss: 0.7308 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.90566\n",
      "Epoch 324/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8611 - val_loss: 0.7310 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.90566\n",
      "Epoch 325/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2724 - acc: 0.8827 - val_loss: 0.7339 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.90566\n",
      "Epoch 326/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2894 - acc: 0.8614 - val_loss: 0.7320 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.90566\n",
      "Epoch 327/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2901 - acc: 0.8665 - val_loss: 0.7315 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.90566\n",
      "Epoch 328/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2705 - acc: 0.8685 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.90566\n",
      "Epoch 329/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3101 - acc: 0.8650 - val_loss: 0.7329 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.90566\n",
      "Epoch 330/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2944 - acc: 0.8656 - val_loss: 0.7311 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.90566\n",
      "Epoch 331/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3066 - acc: 0.8632 - val_loss: 0.7331 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.90566\n",
      "Epoch 332/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3074 - acc: 0.8679 - val_loss: 0.7307 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.90566\n",
      "Epoch 333/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2807 - acc: 0.8691 - val_loss: 0.7305 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.90566\n",
      "Epoch 334/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2905 - acc: 0.8670 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.90566\n",
      "Epoch 335/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2959 - acc: 0.8611 - val_loss: 0.7306 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.90566\n",
      "Epoch 336/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3113 - acc: 0.8703 - val_loss: 0.7310 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.90566\n",
      "Epoch 337/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3007 - acc: 0.8650 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.90566\n",
      "Epoch 338/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3096 - acc: 0.8676 - val_loss: 0.7323 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.90566\n",
      "Epoch 339/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3011 - acc: 0.8659 - val_loss: 0.7330 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.90566\n",
      "Epoch 340/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2683 - acc: 0.8768 - val_loss: 0.7321 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.90566\n",
      "Epoch 341/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3017 - acc: 0.8644 - val_loss: 0.7329 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.90566\n",
      "Epoch 342/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3037 - acc: 0.8700 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.90566\n",
      "Epoch 343/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2960 - acc: 0.8718 - val_loss: 0.7327 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.90566\n",
      "Epoch 344/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3031 - acc: 0.8697 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.90566\n",
      "Epoch 345/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2936 - acc: 0.8676 - val_loss: 0.7333 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.90566\n",
      "Epoch 346/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3184 - acc: 0.8718 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.90566\n",
      "Epoch 347/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2978 - acc: 0.8673 - val_loss: 0.7328 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.90566\n",
      "Epoch 348/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2815 - acc: 0.8659 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.90566\n",
      "Epoch 349/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2838 - acc: 0.8709 - val_loss: 0.7341 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.90566\n",
      "Epoch 350/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2789 - acc: 0.8709 - val_loss: 0.7323 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.90566\n",
      "Epoch 351/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2943 - acc: 0.8579 - val_loss: 0.7307 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.90566\n",
      "Epoch 352/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2935 - acc: 0.8735 - val_loss: 0.7304 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.90566\n",
      "Epoch 353/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2987 - acc: 0.8691 - val_loss: 0.7304 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.90566\n",
      "Epoch 354/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2936 - acc: 0.8617 - val_loss: 0.7296 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.90566\n",
      "Epoch 355/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2881 - acc: 0.8620 - val_loss: 0.7292 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.90566\n",
      "Epoch 356/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2885 - acc: 0.8729 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.90566\n",
      "Epoch 357/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3047 - acc: 0.8667 - val_loss: 0.7281 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.90566\n",
      "Epoch 358/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8718 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.90566\n",
      "Epoch 359/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2858 - acc: 0.8694 - val_loss: 0.7287 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.90566\n",
      "Epoch 360/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2780 - acc: 0.8809 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.90566\n",
      "Epoch 361/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2927 - acc: 0.8726 - val_loss: 0.7292 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.90566\n",
      "Epoch 362/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2903 - acc: 0.8550 - val_loss: 0.7285 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.90566\n",
      "Epoch 363/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2950 - acc: 0.8694 - val_loss: 0.7272 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.90566\n",
      "Epoch 364/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3112 - acc: 0.8632 - val_loss: 0.7288 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.90566\n",
      "Epoch 365/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2823 - acc: 0.8667 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.90566\n",
      "Epoch 366/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2889 - acc: 0.8800 - val_loss: 0.7287 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.90566\n",
      "Epoch 367/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2800 - acc: 0.8718 - val_loss: 0.7302 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.90566\n",
      "Epoch 368/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2811 - acc: 0.8697 - val_loss: 0.7299 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.90566\n",
      "Epoch 369/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2848 - acc: 0.8715 - val_loss: 0.7285 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.90566\n",
      "Epoch 370/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3105 - acc: 0.8694 - val_loss: 0.7277 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.90566\n",
      "Epoch 371/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2787 - acc: 0.8706 - val_loss: 0.7284 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.90566\n",
      "Epoch 372/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2796 - acc: 0.8611 - val_loss: 0.7282 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.90566\n",
      "Epoch 373/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2981 - acc: 0.8768 - val_loss: 0.7290 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.90566\n",
      "Epoch 374/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2831 - acc: 0.8662 - val_loss: 0.7298 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.90566\n",
      "Epoch 375/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3051 - acc: 0.8667 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.90566\n",
      "Epoch 376/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3078 - acc: 0.8626 - val_loss: 0.7267 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.90566\n",
      "Epoch 377/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3095 - acc: 0.8709 - val_loss: 0.7264 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.90566\n",
      "Epoch 378/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2841 - acc: 0.8620 - val_loss: 0.7275 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.90566\n",
      "Epoch 379/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2876 - acc: 0.8718 - val_loss: 0.7268 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.90566\n",
      "Epoch 380/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2879 - acc: 0.8706 - val_loss: 0.7268 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.90566\n",
      "Epoch 381/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2789 - acc: 0.8576 - val_loss: 0.7270 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.90566\n",
      "Epoch 382/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3002 - acc: 0.8591 - val_loss: 0.7262 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.90566\n",
      "Epoch 383/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8641 - val_loss: 0.7250 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.90566\n",
      "Epoch 384/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2964 - acc: 0.8670 - val_loss: 0.7244 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.90566\n",
      "Epoch 385/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2965 - acc: 0.8697 - val_loss: 0.7259 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.90566\n",
      "Epoch 386/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3037 - acc: 0.8614 - val_loss: 0.7281 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.90566\n",
      "Epoch 387/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2822 - acc: 0.8715 - val_loss: 0.7285 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.90566\n",
      "Epoch 388/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2908 - acc: 0.8653 - val_loss: 0.7284 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.90566\n",
      "Epoch 389/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8635 - val_loss: 0.7286 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.90566\n",
      "Epoch 390/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2907 - acc: 0.8656 - val_loss: 0.7291 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.90566\n",
      "Epoch 391/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2798 - acc: 0.8759 - val_loss: 0.7293 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.90566\n",
      "Epoch 392/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2641 - acc: 0.8632 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.90566\n",
      "Epoch 393/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3075 - acc: 0.8632 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.90566\n",
      "Epoch 394/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2863 - acc: 0.8650 - val_loss: 0.7299 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.90566\n",
      "Epoch 395/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8656 - val_loss: 0.7312 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.90566\n",
      "Epoch 396/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2876 - acc: 0.8665 - val_loss: 0.7308 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.90566\n",
      "Epoch 397/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2779 - acc: 0.8700 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.90566\n",
      "Epoch 398/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2639 - acc: 0.8747 - val_loss: 0.7298 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.90566\n",
      "Epoch 399/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2938 - acc: 0.8756 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.90566\n",
      "Epoch 400/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2798 - acc: 0.8721 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.90566\n",
      "Epoch 401/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2719 - acc: 0.8729 - val_loss: 0.7313 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.90566\n",
      "Epoch 402/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2951 - acc: 0.8608 - val_loss: 0.7306 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.90566\n",
      "Epoch 403/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3031 - acc: 0.8653 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.90566\n",
      "Epoch 404/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2746 - acc: 0.8620 - val_loss: 0.7314 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.90566\n",
      "Epoch 405/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2877 - acc: 0.8629 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.90566\n",
      "Epoch 406/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3069 - acc: 0.8620 - val_loss: 0.7293 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.90566\n",
      "Epoch 407/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3104 - acc: 0.8626 - val_loss: 0.7301 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.90566\n",
      "Epoch 408/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3225 - acc: 0.8611 - val_loss: 0.7294 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.90566\n",
      "Epoch 409/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2780 - acc: 0.8679 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.90566\n",
      "Epoch 410/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2934 - acc: 0.8803 - val_loss: 0.7319 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.90566\n",
      "Epoch 411/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2905 - acc: 0.8712 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.90566\n",
      "Epoch 412/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2886 - acc: 0.8676 - val_loss: 0.7301 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.90566\n",
      "Epoch 413/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2868 - acc: 0.8676 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.90566\n",
      "Epoch 414/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3083 - acc: 0.8665 - val_loss: 0.7299 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.90566\n",
      "Epoch 415/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8641 - val_loss: 0.7323 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.90566\n",
      "Epoch 416/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2853 - acc: 0.8765 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.90566\n",
      "Epoch 417/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2591 - acc: 0.8844 - val_loss: 0.7306 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.90566\n",
      "Epoch 418/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2802 - acc: 0.8641 - val_loss: 0.7304 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.90566\n",
      "Epoch 419/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3014 - acc: 0.8667 - val_loss: 0.7308 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.90566\n",
      "Epoch 420/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3177 - acc: 0.8697 - val_loss: 0.7309 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.90566\n",
      "Epoch 421/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2792 - acc: 0.8809 - val_loss: 0.7323 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.90566\n",
      "Epoch 422/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2999 - acc: 0.8670 - val_loss: 0.7349 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.90566\n",
      "Epoch 423/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2990 - acc: 0.8659 - val_loss: 0.7351 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.90566\n",
      "Epoch 424/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2685 - acc: 0.8679 - val_loss: 0.7337 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.90566\n",
      "Epoch 425/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2906 - acc: 0.8638 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.90566\n",
      "Epoch 426/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2942 - acc: 0.8673 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.90566\n",
      "Epoch 427/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2679 - acc: 0.8756 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.90566\n",
      "Epoch 428/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3160 - acc: 0.8611 - val_loss: 0.7334 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.90566\n",
      "Epoch 429/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2784 - acc: 0.8676 - val_loss: 0.7335 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.90566\n",
      "Epoch 430/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2663 - acc: 0.8703 - val_loss: 0.7307 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.90566\n",
      "Epoch 431/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2778 - acc: 0.8712 - val_loss: 0.7317 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.90566\n",
      "Epoch 432/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2731 - acc: 0.8738 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.90566\n",
      "Epoch 433/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2710 - acc: 0.8800 - val_loss: 0.7349 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.90566\n",
      "Epoch 434/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2743 - acc: 0.8738 - val_loss: 0.7328 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.90566\n",
      "Epoch 435/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2856 - acc: 0.8697 - val_loss: 0.7310 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.90566\n",
      "Epoch 436/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2784 - acc: 0.8750 - val_loss: 0.7298 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.90566\n",
      "Epoch 437/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2678 - acc: 0.8762 - val_loss: 0.7284 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.90566\n",
      "Epoch 438/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2908 - acc: 0.8665 - val_loss: 0.7297 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.90566\n",
      "Epoch 439/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2822 - acc: 0.8694 - val_loss: 0.7305 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.90566\n",
      "Epoch 440/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2708 - acc: 0.8768 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.90566\n",
      "Epoch 441/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2832 - acc: 0.8726 - val_loss: 0.7316 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.90566\n",
      "Epoch 442/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2966 - acc: 0.8715 - val_loss: 0.7305 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.90566\n",
      "Epoch 443/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2741 - acc: 0.8726 - val_loss: 0.7311 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.90566\n",
      "Epoch 444/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2872 - acc: 0.8785 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.90566\n",
      "Epoch 445/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2809 - acc: 0.8750 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.90566\n",
      "Epoch 446/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2773 - acc: 0.8735 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.90566\n",
      "Epoch 447/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8718 - val_loss: 0.7311 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.90566\n",
      "Epoch 448/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2710 - acc: 0.8785 - val_loss: 0.7301 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.90566\n",
      "Epoch 449/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2754 - acc: 0.8771 - val_loss: 0.7302 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.90566\n",
      "Epoch 450/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2802 - acc: 0.8774 - val_loss: 0.7306 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.90566\n",
      "Epoch 451/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2724 - acc: 0.8673 - val_loss: 0.7311 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.90566\n",
      "Epoch 452/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2867 - acc: 0.8691 - val_loss: 0.7309 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.90566\n",
      "Epoch 453/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2950 - acc: 0.8673 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.90566\n",
      "Epoch 454/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2832 - acc: 0.8688 - val_loss: 0.7293 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.90566\n",
      "Epoch 455/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2666 - acc: 0.8777 - val_loss: 0.7294 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.90566\n",
      "Epoch 456/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3088 - acc: 0.8706 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.90566\n",
      "Epoch 457/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2860 - acc: 0.8620 - val_loss: 0.7288 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.90566\n",
      "Epoch 458/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2736 - acc: 0.8729 - val_loss: 0.7307 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.90566\n",
      "Epoch 459/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3017 - acc: 0.8644 - val_loss: 0.7309 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.90566\n",
      "Epoch 460/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8591 - val_loss: 0.7319 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.90566\n",
      "Epoch 461/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2948 - acc: 0.8629 - val_loss: 0.7311 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.90566\n",
      "Epoch 462/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2774 - acc: 0.8744 - val_loss: 0.7312 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.90566\n",
      "Epoch 463/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2805 - acc: 0.8718 - val_loss: 0.7325 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.90566\n",
      "Epoch 464/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2912 - acc: 0.8632 - val_loss: 0.7333 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.90566\n",
      "Epoch 465/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2777 - acc: 0.8700 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.90566\n",
      "Epoch 466/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2838 - acc: 0.8723 - val_loss: 0.7297 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.90566\n",
      "Epoch 467/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2924 - acc: 0.8659 - val_loss: 0.7292 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.90566\n",
      "Epoch 468/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2965 - acc: 0.8667 - val_loss: 0.7312 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.90566\n",
      "Epoch 469/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2736 - acc: 0.8753 - val_loss: 0.7317 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.90566\n",
      "Epoch 470/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3012 - acc: 0.8659 - val_loss: 0.7307 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.90566\n",
      "Epoch 471/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2668 - acc: 0.8735 - val_loss: 0.7293 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.90566\n",
      "Epoch 472/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2830 - acc: 0.8694 - val_loss: 0.7290 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.90566\n",
      "Epoch 473/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3091 - acc: 0.8567 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.90566\n",
      "Epoch 474/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2877 - acc: 0.8700 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.90566\n",
      "Epoch 475/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2764 - acc: 0.8785 - val_loss: 0.7327 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.90566\n",
      "Epoch 476/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2799 - acc: 0.8768 - val_loss: 0.7326 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.90566\n",
      "Epoch 477/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2657 - acc: 0.8753 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.90566\n",
      "Epoch 478/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2677 - acc: 0.8841 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.90566\n",
      "Epoch 479/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2864 - acc: 0.8697 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.90566\n",
      "Epoch 480/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8662 - val_loss: 0.7292 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.90566\n",
      "Epoch 481/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2865 - acc: 0.8718 - val_loss: 0.7294 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.90566\n",
      "Epoch 482/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2758 - acc: 0.8656 - val_loss: 0.7300 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.90566\n",
      "Epoch 483/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2763 - acc: 0.8665 - val_loss: 0.7298 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.90566\n",
      "Epoch 484/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2837 - acc: 0.8638 - val_loss: 0.7295 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.90566\n",
      "Epoch 00484: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fold=0\n",
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "    print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "    model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(model,X_semi,Y_semi,fold,model_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC7\n",
    "#0=>0.84367(resnet3_semi)/0.83288(not semi)\n",
    "#1=>0.84367(resnet3_semi)/0.81132(not semi)\n",
    "#2=>0.84097(resnet2_semi)/0.81671(not semi)\n",
    "#3=>0.88410(resnet1_semi)/0.84367\n",
    "#4=>0.82210(resnet4_semi)/0.74663(not semi)\n",
    "#5 => 0.82749(resnet4_semi)/0.77628\n",
    "#6 => 0.85445(resnet2_semi)/0.78437\n",
    "#7 => 0.82749(resnet3_semi)/0.76280\n",
    "#8 => 0.83288(resnet3_semi)/0.78706\n",
    "# 9=> 0.90566(resnet1_semi)/0.87332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC6\n",
    "# 0=> 0.81941 (not semi)\n",
    "# 1=>0.83019 (semi)\n",
    "# 2=>0.81941 (semi)\n",
    "# 3=>0.85984 (resnet1_not semi)、0.78437\n",
    "# 4=>0.84367 (renet1_not semi)、0.81132\n",
    "# 5=> 0.85175(resnet3_semi)、0.82749(not semi)\n",
    "#6 => 0.85904(resnet4_semi)、0.79784(not semi)\n",
    "# 7=>0.88949(resnet2_semi)、0.83558(not semi)\n",
    "# 8=>0.83558(resnet1 not semi)\n",
    "# 9=>0.86792(resnet2_semi)、0.8112(not semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00044347.wav', '002d256b.wav', '003b91e8.wav', ...,\n",
       "       'fff37590.wav', 'fff44ac6.wav', 'fff6a13d.wav'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.load('feature/mfcc6/fname_unverified.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "# Co-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def normalize(X_train, X_val):\n",
    "    X_train = (X_train - mean)/(std)\n",
    "#     X_train = (X_train - min_)/range_\n",
    "    X_val = (X_val - mean)/(std)\n",
    "#     X_val = (X_val - min_)/range_\n",
    "\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet.ResnetBuilder.build_resnet_50((1, 64, 431), 41)\n",
    "# model.summary()\n",
    "def train_unverified(X_semi,Y_semi,fold):\n",
    "    name = glob.glob('model/'+feature_type+'/'+'**_fold'+str(fold)+'_**')[0]\n",
    "#     if fold in [0,1,2,3,4,5]: # MFCC6\n",
    "#         name = glob.glob('model/'+feature_type+'/'+'**_fold'+str(fold)+'_co_**')[0]\n",
    "    print('semi loading: '+ name)\n",
    "    model = load_model(name)\n",
    "    model.summary()\n",
    "    if 'resnet4'in name:\n",
    "        batchSize=[32]\n",
    "    elif 'resnet2' in name or 'resnet3' in name:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif 'resnet1' in name:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64]\n",
    "#     batchSize=[32,64,128,256] ##ERR?\n",
    "    batchSize = random.choice(batchSize)\n",
    "    print('BS:',str(batchSize))\n",
    "    patien=30\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    opt = Adam(lr=0.0001,decay=1e-6)#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "#         featurewise_std_normalization=True,\n",
    "        width_shift_range=0.05+0.22*random.random(),\n",
    "        height_shift_range=0.05+0.22*random.random(),\n",
    "        shear_range=0.084375+0.187625*random.random(),\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(X_semi), v_h=np.max(X_semi)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_semi)\n",
    "#     test_datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True)\n",
    "    generator = MixupGenerator(X_semi, Y_semi, alpha=0.4+0.6*random.random(), \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "    \n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    history = History()\n",
    "    callback=[\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/4),min_lr=4e-6,\n",
    "                          mode='min', cooldown=1,verbose=1 ), #0.2,/25 #0.5/5/3e-6 #0.3/10/1e-6\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_self_resnet'+'.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,\n",
    "                        mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_self_resnet'),\n",
    "        history\n",
    "    ]\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=2*X_semi.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch,\n",
    "#                         initial_epoch = int(patien/20)\n",
    "                       )\n",
    "#     model.fit(X_semi,Y_semi,\n",
    "#               shuffle=True,\n",
    "#               callbacks=callback, \n",
    "#               class_weight='auto',\n",
    "#               validation_data=(X_valid,Y_valid),\n",
    "#               batch_size=batchSize,\n",
    "#               epochs=epoch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semi_data(X_train,Y_train):\n",
    "    X = np.load('feature/'+feature_type+'/semi/'+who+'/X_co.npy')\n",
    "#     X_test_ver = np.load('feature/'+feature_type+'/semi/'+who+'/X_test_ver.npy')\n",
    "#     X = np.concatenate((X_un_ver,X_test_ver))\n",
    "    Y = np.load('feature/'+feature_type+'/semi/'+who+'/Y_co.npy')\n",
    "#     Y_test_ver = np.load('feature/'+feature_type+'/semi/'+who+'/Y_test_ver.npy')\n",
    "#     Y = np.concatenate((Y_un_ver,Y_test_ver))\n",
    "    Y = to_categorical(Y,num_classes=41)\n",
    "    X_semi = np.concatenate((X_train,X))\n",
    "    Y_semi = np.concatenate((Y_train,Y))\n",
    "    X_semi , Y_semi = _shuffle(X_semi,Y_semi)\n",
    "    print(X_semi.shape , Y_semi.shape)\n",
    "    return X_semi , Y_semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'mfcc7'\n",
    "who = 'Y_selftrain_ens_verified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'feature/'+feature_type+'/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/'+feature_type+'/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10\n",
    "\n",
    "val_set_num = [4,8,9,5]#str(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "(8162, 60, 259, 1) (8162, 41)\n",
      "===train semi_4===\n",
      "semi loading: model/mfcc7/LGD_fold4_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 30, 130, 64)  3200        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 30, 130, 64)  256         conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 30, 130, 64)  0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 15, 65, 64)   0           activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 15, 65, 64)   256         conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 15, 65, 64)   0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 15, 65, 64)   36928       activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 15, 65, 64)   256         conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 15, 65, 64)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 15, 65, 256)  16640       activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 15, 65, 256)  0           conv2d_586[0][0]                 \n",
      "                                                                 conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 15, 65, 256)  1024        add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 15, 65, 256)  0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 15, 65, 64)   16448       activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 15, 65, 64)   256         conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 15, 65, 64)   0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 15, 65, 64)   36928       activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 15, 65, 64)   256         conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 15, 65, 64)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 15, 65, 256)  16640       activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 15, 65, 256)  0           add_197[0][0]                    \n",
      "                                                                 conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 15, 65, 256)  1024        add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 15, 65, 256)  0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 15, 65, 64)   16448       activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 15, 65, 64)   256         conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 15, 65, 64)   0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 15, 65, 64)   36928       activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 15, 65, 64)   256         conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 15, 65, 64)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 15, 65, 256)  16640       activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 15, 65, 256)  0           add_198[0][0]                    \n",
      "                                                                 conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 15, 65, 256)  1024        add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 15, 65, 256)  0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 8, 33, 128)   32896       activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 8, 33, 128)   512         conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 8, 33, 128)   0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 8, 33, 128)   147584      activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 8, 33, 128)   512         conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 8, 33, 128)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 8, 33, 512)   131584      add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 8, 33, 512)   66048       activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 8, 33, 512)   0           conv2d_596[0][0]                 \n",
      "                                                                 conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 8, 33, 512)   2048        add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 8, 33, 512)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 8, 33, 128)   65664       activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 8, 33, 128)   512         conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 8, 33, 128)   0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 8, 33, 128)   147584      activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 8, 33, 128)   512         conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 8, 33, 128)   0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 8, 33, 512)   66048       activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 8, 33, 512)   0           add_200[0][0]                    \n",
      "                                                                 conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 8, 33, 512)   2048        add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 8, 33, 512)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 8, 33, 128)   65664       activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 8, 33, 128)   512         conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 8, 33, 128)   0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 8, 33, 128)   147584      activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 8, 33, 128)   512         conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 8, 33, 128)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 8, 33, 512)   66048       activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 8, 33, 512)   0           add_201[0][0]                    \n",
      "                                                                 conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 8, 33, 512)   2048        add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 8, 33, 512)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 8, 33, 128)   65664       activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 8, 33, 128)   512         conv2d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 8, 33, 128)   0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 8, 33, 128)   147584      activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 8, 33, 128)   512         conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 8, 33, 128)   0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 8, 33, 512)   66048       activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 8, 33, 512)   0           add_202[0][0]                    \n",
      "                                                                 conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 8, 33, 512)   2048        add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 8, 33, 512)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 8, 33, 128)   65664       activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 8, 33, 128)   512         conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 8, 33, 128)   0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 8, 33, 128)   147584      activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 8, 33, 128)   512         conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 8, 33, 128)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 8, 33, 512)   66048       activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 8, 33, 512)   0           add_203[0][0]                    \n",
      "                                                                 conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 8, 33, 512)   2048        add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 8, 33, 512)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 8, 33, 128)   65664       activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 8, 33, 128)   512         conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 8, 33, 128)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 8, 33, 128)   147584      activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 8, 33, 128)   512         conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 8, 33, 128)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 8, 33, 512)   66048       activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 8, 33, 512)   0           add_204[0][0]                    \n",
      "                                                                 conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 8, 33, 512)   2048        add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 8, 33, 512)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 8, 33, 128)   65664       activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 8, 33, 128)   512         conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 8, 33, 128)   0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 8, 33, 128)   147584      activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 8, 33, 128)   512         conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 8, 33, 128)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 8, 33, 512)   66048       activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 8, 33, 512)   0           add_205[0][0]                    \n",
      "                                                                 conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 8, 33, 512)   2048        add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 8, 33, 512)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 8, 33, 128)   65664       activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 8, 33, 128)   512         conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 8, 33, 128)   0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 8, 33, 128)   147584      activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 8, 33, 128)   512         conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 8, 33, 128)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 8, 33, 512)   66048       activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 8, 33, 512)   0           add_206[0][0]                    \n",
      "                                                                 conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 8, 33, 512)   2048        add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 8, 33, 512)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 4, 17, 256)   131328      activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 4, 17, 256)   1024        conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 4, 17, 256)   0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 4, 17, 256)   590080      activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 4, 17, 256)   1024        conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 4, 17, 256)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 4, 17, 1024)  525312      add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 4, 17, 1024)  263168      activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_208 (Add)                   (None, 4, 17, 1024)  0           conv2d_621[0][0]                 \n",
      "                                                                 conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 4, 17, 1024)  4096        add_208[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 4, 17, 256)   262400      activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 4, 17, 256)   1024        conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 4, 17, 256)   0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 4, 17, 256)   590080      activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 4, 17, 256)   1024        conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 4, 17, 256)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 4, 17, 1024)  263168      activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_209 (Add)                   (None, 4, 17, 1024)  0           add_208[0][0]                    \n",
      "                                                                 conv2d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 4, 17, 1024)  4096        add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 4, 17, 256)   262400      activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 4, 17, 256)   1024        conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 4, 17, 256)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 4, 17, 256)   590080      activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 4, 17, 256)   1024        conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 4, 17, 256)   0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 4, 17, 1024)  263168      activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_210 (Add)                   (None, 4, 17, 1024)  0           add_209[0][0]                    \n",
      "                                                                 conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 4, 17, 1024)  4096        add_210[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 4, 17, 256)   262400      activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 4, 17, 256)   1024        conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 4, 17, 256)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 4, 17, 256)   590080      activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 4, 17, 256)   1024        conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 4, 17, 256)   0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 4, 17, 1024)  263168      activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_211 (Add)                   (None, 4, 17, 1024)  0           add_210[0][0]                    \n",
      "                                                                 conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 4, 17, 1024)  4096        add_211[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 4, 17, 256)   262400      activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 4, 17, 256)   1024        conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 4, 17, 256)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 4, 17, 256)   590080      activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 4, 17, 256)   1024        conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 4, 17, 256)   0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 4, 17, 1024)  263168      activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_212 (Add)                   (None, 4, 17, 1024)  0           add_211[0][0]                    \n",
      "                                                                 conv2d_633[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 4, 17, 1024)  4096        add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 4, 17, 256)   262400      activation_598[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 4, 17, 256)   1024        conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 4, 17, 256)   0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 4, 17, 256)   590080      activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 4, 17, 256)   1024        conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 4, 17, 256)   0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 4, 17, 1024)  263168      activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_213 (Add)                   (None, 4, 17, 1024)  0           add_212[0][0]                    \n",
      "                                                                 conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 4, 17, 1024)  4096        add_213[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 4, 17, 256)   262400      activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 4, 17, 256)   1024        conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 4, 17, 256)   0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 4, 17, 256)   590080      activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 4, 17, 256)   1024        conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 4, 17, 256)   0           batch_normalization_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 4, 17, 1024)  263168      activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_214 (Add)                   (None, 4, 17, 1024)  0           add_213[0][0]                    \n",
      "                                                                 conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 4, 17, 1024)  4096        add_214[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_640 (Conv2D)             (None, 4, 17, 256)   262400      activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 4, 17, 256)   1024        conv2d_640[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 4, 17, 256)   0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_641 (Conv2D)             (None, 4, 17, 256)   590080      activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 4, 17, 256)   1024        conv2d_641[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 4, 17, 256)   0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_642 (Conv2D)             (None, 4, 17, 1024)  263168      activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_215 (Add)                   (None, 4, 17, 1024)  0           add_214[0][0]                    \n",
      "                                                                 conv2d_642[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 4, 17, 1024)  4096        add_215[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_643 (Conv2D)             (None, 4, 17, 256)   262400      activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 4, 17, 256)   1024        conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 4, 17, 256)   0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_644 (Conv2D)             (None, 4, 17, 256)   590080      activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 4, 17, 256)   1024        conv2d_644[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 4, 17, 256)   0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_645 (Conv2D)             (None, 4, 17, 1024)  263168      activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_216 (Add)                   (None, 4, 17, 1024)  0           add_215[0][0]                    \n",
      "                                                                 conv2d_645[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 4, 17, 1024)  4096        add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_646 (Conv2D)             (None, 4, 17, 256)   262400      activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 4, 17, 256)   1024        conv2d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 4, 17, 256)   0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_647 (Conv2D)             (None, 4, 17, 256)   590080      activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 4, 17, 256)   1024        conv2d_647[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 4, 17, 256)   0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_648 (Conv2D)             (None, 4, 17, 1024)  263168      activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_217 (Add)                   (None, 4, 17, 1024)  0           add_216[0][0]                    \n",
      "                                                                 conv2d_648[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 4, 17, 1024)  4096        add_217[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_649 (Conv2D)             (None, 4, 17, 256)   262400      activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 4, 17, 256)   1024        conv2d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 4, 17, 256)   0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_650 (Conv2D)             (None, 4, 17, 256)   590080      activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 4, 17, 256)   1024        conv2d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 4, 17, 256)   0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_651 (Conv2D)             (None, 4, 17, 1024)  263168      activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_218 (Add)                   (None, 4, 17, 1024)  0           add_217[0][0]                    \n",
      "                                                                 conv2d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 4, 17, 1024)  4096        add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_652 (Conv2D)             (None, 4, 17, 256)   262400      activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 4, 17, 256)   1024        conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 4, 17, 256)   0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_653 (Conv2D)             (None, 4, 17, 256)   590080      activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 4, 17, 256)   1024        conv2d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 4, 17, 256)   0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_654 (Conv2D)             (None, 4, 17, 1024)  263168      activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_219 (Add)                   (None, 4, 17, 1024)  0           add_218[0][0]                    \n",
      "                                                                 conv2d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 4, 17, 1024)  4096        add_219[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_655 (Conv2D)             (None, 4, 17, 256)   262400      activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 4, 17, 256)   1024        conv2d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 4, 17, 256)   0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_656 (Conv2D)             (None, 4, 17, 256)   590080      activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 4, 17, 256)   1024        conv2d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 4, 17, 256)   0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_657 (Conv2D)             (None, 4, 17, 1024)  263168      activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_220 (Add)                   (None, 4, 17, 1024)  0           add_219[0][0]                    \n",
      "                                                                 conv2d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 4, 17, 1024)  4096        add_220[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 4, 17, 256)   262400      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 4, 17, 256)   1024        conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 4, 17, 256)   0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 4, 17, 256)   590080      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 4, 17, 256)   1024        conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 4, 17, 256)   0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 4, 17, 1024)  263168      activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_221 (Add)                   (None, 4, 17, 1024)  0           add_220[0][0]                    \n",
      "                                                                 conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 4, 17, 1024)  4096        add_221[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 4, 17, 256)   262400      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 4, 17, 256)   1024        conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 4, 17, 256)   0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 4, 17, 256)   590080      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 4, 17, 256)   1024        conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 4, 17, 256)   0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 4, 17, 1024)  263168      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_222 (Add)                   (None, 4, 17, 1024)  0           add_221[0][0]                    \n",
      "                                                                 conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 4, 17, 1024)  4096        add_222[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 4, 17, 256)   262400      activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 4, 17, 256)   1024        conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 4, 17, 256)   0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 4, 17, 256)   590080      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 4, 17, 256)   1024        conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 4, 17, 256)   0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 4, 17, 1024)  263168      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_223 (Add)                   (None, 4, 17, 1024)  0           add_222[0][0]                    \n",
      "                                                                 conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 4, 17, 1024)  4096        add_223[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 4, 17, 256)   262400      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 4, 17, 256)   1024        conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 4, 17, 256)   0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 4, 17, 256)   590080      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 4, 17, 256)   1024        conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 4, 17, 256)   0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 4, 17, 1024)  263168      activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_224 (Add)                   (None, 4, 17, 1024)  0           add_223[0][0]                    \n",
      "                                                                 conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 4, 17, 1024)  4096        add_224[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 4, 17, 256)   262400      activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 4, 17, 256)   1024        conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 4, 17, 256)   0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 4, 17, 256)   590080      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 4, 17, 256)   1024        conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 4, 17, 256)   0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 4, 17, 1024)  263168      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_225 (Add)                   (None, 4, 17, 1024)  0           add_224[0][0]                    \n",
      "                                                                 conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 4, 17, 1024)  4096        add_225[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 4, 17, 256)   262400      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 4, 17, 256)   1024        conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 4, 17, 256)   0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 4, 17, 256)   590080      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 4, 17, 256)   1024        conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 4, 17, 256)   0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 4, 17, 1024)  263168      activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_226 (Add)                   (None, 4, 17, 1024)  0           add_225[0][0]                    \n",
      "                                                                 conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 4, 17, 1024)  4096        add_226[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 4, 17, 256)   262400      activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 4, 17, 256)   1024        conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 4, 17, 256)   0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 4, 17, 256)   590080      activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 4, 17, 256)   1024        conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 4, 17, 256)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 4, 17, 1024)  263168      activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_227 (Add)                   (None, 4, 17, 1024)  0           add_226[0][0]                    \n",
      "                                                                 conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 4, 17, 1024)  4096        add_227[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 4, 17, 256)   262400      activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 4, 17, 256)   1024        conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 4, 17, 256)   0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 4, 17, 256)   590080      activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 4, 17, 256)   1024        conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 4, 17, 256)   0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 4, 17, 1024)  263168      activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 4, 17, 1024)  0           add_227[0][0]                    \n",
      "                                                                 conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 4, 17, 1024)  4096        add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 4, 17, 256)   262400      activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 4, 17, 256)   1024        conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 4, 17, 256)   0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 4, 17, 256)   590080      activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 4, 17, 256)   1024        conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, 4, 17, 256)   0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 4, 17, 1024)  263168      activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 4, 17, 1024)  0           add_228[0][0]                    \n",
      "                                                                 conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 4, 17, 1024)  4096        add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 4, 17, 256)   262400      activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 4, 17, 256)   1024        conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, 4, 17, 256)   0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 4, 17, 256)   590080      activation_650[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 4, 17, 256)   1024        conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 4, 17, 256)   0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 4, 17, 1024)  263168      activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 4, 17, 1024)  0           add_229[0][0]                    \n",
      "                                                                 conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 4, 17, 1024)  4096        add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 4, 17, 256)   262400      activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 4, 17, 256)   1024        conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 4, 17, 256)   0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 4, 17, 256)   590080      activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 4, 17, 256)   1024        conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, 4, 17, 256)   0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 4, 17, 1024)  263168      activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 4, 17, 1024)  0           add_230[0][0]                    \n",
      "                                                                 conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 4, 17, 1024)  4096        add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 4, 17, 256)   262400      activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 4, 17, 256)   1024        conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, 4, 17, 256)   0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 4, 17, 256)   590080      activation_656[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 4, 17, 256)   1024        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, 4, 17, 256)   0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 4, 17, 1024)  263168      activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 4, 17, 1024)  0           add_231[0][0]                    \n",
      "                                                                 conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 4, 17, 1024)  4096        add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 4, 17, 256)   262400      activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 4, 17, 256)   1024        conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_659 (Activation)     (None, 4, 17, 256)   0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 4, 17, 256)   590080      activation_659[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 4, 17, 256)   1024        conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_660 (Activation)     (None, 4, 17, 256)   0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 4, 17, 1024)  263168      activation_660[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 4, 17, 1024)  0           add_232[0][0]                    \n",
      "                                                                 conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 4, 17, 1024)  4096        add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_661 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 4, 17, 256)   262400      activation_661[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 4, 17, 256)   1024        conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_662 (Activation)     (None, 4, 17, 256)   0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 4, 17, 256)   590080      activation_662[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 4, 17, 256)   1024        conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_663 (Activation)     (None, 4, 17, 256)   0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 4, 17, 1024)  263168      activation_663[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 4, 17, 1024)  0           add_233[0][0]                    \n",
      "                                                                 conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 4, 17, 1024)  4096        add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_664 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_673[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 4, 17, 256)   262400      activation_664[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 4, 17, 256)   1024        conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_665 (Activation)     (None, 4, 17, 256)   0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, 4, 17, 256)   590080      activation_665[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 4, 17, 256)   1024        conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_666 (Activation)     (None, 4, 17, 256)   0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 4, 17, 1024)  263168      activation_666[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 4, 17, 1024)  0           add_234[0][0]                    \n",
      "                                                                 conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 4, 17, 1024)  4096        add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_667 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_676[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 4, 17, 256)   262400      activation_667[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 4, 17, 256)   1024        conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_668 (Activation)     (None, 4, 17, 256)   0           batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 4, 17, 256)   590080      activation_668[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 4, 17, 256)   1024        conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_669 (Activation)     (None, 4, 17, 256)   0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 4, 17, 1024)  263168      activation_669[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 4, 17, 1024)  0           add_235[0][0]                    \n",
      "                                                                 conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 4, 17, 1024)  4096        add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_670 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 4, 17, 256)   262400      activation_670[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 4, 17, 256)   1024        conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 4, 17, 256)   0           batch_normalization_680[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 4, 17, 256)   590080      activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 4, 17, 256)   1024        conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 4, 17, 256)   0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 4, 17, 1024)  263168      activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 4, 17, 1024)  0           add_236[0][0]                    \n",
      "                                                                 conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 4, 17, 1024)  4096        add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 4, 17, 256)   262400      activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 4, 17, 256)   1024        conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 4, 17, 256)   0           batch_normalization_683[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 4, 17, 256)   590080      activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 4, 17, 256)   1024        conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_675 (Activation)     (None, 4, 17, 256)   0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 4, 17, 1024)  263168      activation_675[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 4, 17, 1024)  0           add_237[0][0]                    \n",
      "                                                                 conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 4, 17, 1024)  4096        add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_676 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 4, 17, 256)   262400      activation_676[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 4, 17, 256)   1024        conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_677 (Activation)     (None, 4, 17, 256)   0           batch_normalization_686[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 4, 17, 256)   590080      activation_677[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 4, 17, 256)   1024        conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_678 (Activation)     (None, 4, 17, 256)   0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 4, 17, 1024)  263168      activation_678[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 4, 17, 1024)  0           add_238[0][0]                    \n",
      "                                                                 conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 4, 17, 1024)  4096        add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_679 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 4, 17, 256)   262400      activation_679[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, 4, 17, 256)   1024        conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_680 (Activation)     (None, 4, 17, 256)   0           batch_normalization_689[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 4, 17, 256)   590080      activation_680[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, 4, 17, 256)   1024        conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_681 (Activation)     (None, 4, 17, 256)   0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 4, 17, 1024)  263168      activation_681[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 4, 17, 1024)  0           add_239[0][0]                    \n",
      "                                                                 conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 4, 17, 1024)  4096        add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_682 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 4, 17, 256)   262400      activation_682[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 4, 17, 256)   1024        conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_683 (Activation)     (None, 4, 17, 256)   0           batch_normalization_692[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 4, 17, 256)   590080      activation_683[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 4, 17, 256)   1024        conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_684 (Activation)     (None, 4, 17, 256)   0           batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, 4, 17, 1024)  263168      activation_684[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_241 (Add)                   (None, 4, 17, 1024)  0           add_240[0][0]                    \n",
      "                                                                 conv2d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 4, 17, 1024)  4096        add_241[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_685 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, 4, 17, 256)   262400      activation_685[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_695 (BatchN (None, 4, 17, 256)   1024        conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_686 (Activation)     (None, 4, 17, 256)   0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 4, 17, 256)   590080      activation_686[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 4, 17, 256)   1024        conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 4, 17, 256)   0           batch_normalization_696[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 4, 17, 1024)  263168      activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_242 (Add)                   (None, 4, 17, 1024)  0           add_241[0][0]                    \n",
      "                                                                 conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 4, 17, 1024)  4096        add_242[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 4, 17, 256)   262400      activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 4, 17, 256)   1024        conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, 4, 17, 256)   0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 4, 17, 256)   590080      activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_699 (BatchN (None, 4, 17, 256)   1024        conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, 4, 17, 256)   0           batch_normalization_699[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 4, 17, 1024)  263168      activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_243 (Add)                   (None, 4, 17, 1024)  0           add_242[0][0]                    \n",
      "                                                                 conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_700 (BatchN (None, 4, 17, 1024)  4096        add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_700[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 2, 9, 512)    524800      activation_691[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_701 (BatchN (None, 2, 9, 512)    2048        conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_692 (Activation)     (None, 2, 9, 512)    0           batch_normalization_701[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 2, 9, 512)    2359808     activation_692[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_702 (BatchN (None, 2, 9, 512)    2048        conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_693 (Activation)     (None, 2, 9, 512)    0           batch_normalization_702[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 2, 9, 2048)   2099200     add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_244 (Add)                   (None, 2, 9, 2048)   0           conv2d_730[0][0]                 \n",
      "                                                                 conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_703 (BatchN (None, 2, 9, 2048)   8192        add_244[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_694 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_703[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 2, 9, 512)    1049088     activation_694[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_704 (BatchN (None, 2, 9, 512)    2048        conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_695 (Activation)     (None, 2, 9, 512)    0           batch_normalization_704[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 2, 9, 512)    2359808     activation_695[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_705 (BatchN (None, 2, 9, 512)    2048        conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_696 (Activation)     (None, 2, 9, 512)    0           batch_normalization_705[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_696[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_245 (Add)                   (None, 2, 9, 2048)   0           add_244[0][0]                    \n",
      "                                                                 conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_706 (BatchN (None, 2, 9, 2048)   8192        add_245[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_697 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_706[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 2, 9, 512)    1049088     activation_697[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_707 (BatchN (None, 2, 9, 512)    2048        conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_698 (Activation)     (None, 2, 9, 512)    0           batch_normalization_707[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 2, 9, 512)    2359808     activation_698[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_708 (BatchN (None, 2, 9, 512)    2048        conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_699 (Activation)     (None, 2, 9, 512)    0           batch_normalization_708[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_246 (Add)                   (None, 2, 9, 2048)   0           add_245[0][0]                    \n",
      "                                                                 conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_709 (BatchN (None, 2, 9, 2048)   8192        add_246[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_700 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_709[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 1, 1, 2048)   0           activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2048)         0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 47)           96303       flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_710 (BatchN (None, 47)           188         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 47)           0           batch_normalization_710[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 41)           1968        dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 58,447,771\n",
      "Trainable params: 58,303,933\n",
      "Non-trainable params: 143,838\n",
      "__________________________________________________________________________________________________\n",
      "BS: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "510/510 [==============================] - 144s 283ms/step - loss: 1.5820 - acc: 0.7837 - val_loss: 1.0387 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78706, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 125s 245ms/step - loss: 1.5668 - acc: 0.7854 - val_loss: 1.0589 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.78706\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 126s 246ms/step - loss: 1.5513 - acc: 0.7915 - val_loss: 1.0237 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.78706 to 0.79245, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 126s 246ms/step - loss: 1.5502 - acc: 0.7918 - val_loss: 1.0181 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79245\n",
      "Epoch 5/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5493 - acc: 0.7887 - val_loss: 1.0083 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79245\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5396 - acc: 0.7946 - val_loss: 0.9966 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79245 to 0.79784, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5473 - acc: 0.7947 - val_loss: 1.0191 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79784\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5263 - acc: 0.7978 - val_loss: 0.9450 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.79784 to 0.80323, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5138 - acc: 0.7998 - val_loss: 0.9219 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80323 to 0.80863, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 10/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5235 - acc: 0.7931 - val_loss: 0.9734 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80863\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5214 - acc: 0.7987 - val_loss: 0.9823 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80863\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5134 - acc: 0.7982 - val_loss: 1.0186 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80863\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4969 - acc: 0.8031 - val_loss: 0.9594 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80863\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4977 - acc: 0.7988 - val_loss: 0.9902 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80863\n",
      "Epoch 15/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.5003 - acc: 0.7970 - val_loss: 0.9424 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.80863 to 0.80863, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4775 - acc: 0.8042 - val_loss: 1.0309 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80863\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4959 - acc: 0.7955 - val_loss: 0.9105 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80863\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4802 - acc: 0.8061 - val_loss: 0.9426 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80863\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4946 - acc: 0.8000 - val_loss: 0.9939 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80863\n",
      "Epoch 20/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4911 - acc: 0.8028 - val_loss: 0.9498 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80863\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4748 - acc: 0.8058 - val_loss: 0.8956 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80863\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4658 - acc: 0.8067 - val_loss: 0.8936 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.80863 to 0.81132, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4683 - acc: 0.8099 - val_loss: 0.9340 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81132\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4653 - acc: 0.8044 - val_loss: 0.8870 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81132\n",
      "Epoch 25/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4602 - acc: 0.8100 - val_loss: 0.9458 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81132\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4632 - acc: 0.8075 - val_loss: 0.9231 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.81132 to 0.81402, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4555 - acc: 0.8100 - val_loss: 0.8548 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81402\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4457 - acc: 0.8045 - val_loss: 0.8687 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81402\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4444 - acc: 0.8100 - val_loss: 0.8975 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81402\n",
      "Epoch 30/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4283 - acc: 0.8165 - val_loss: 0.9104 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81402\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4412 - acc: 0.8142 - val_loss: 0.8866 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.81402 to 0.82210, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4369 - acc: 0.8164 - val_loss: 1.0269 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82210\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4322 - acc: 0.8151 - val_loss: 0.8913 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.82210 to 0.82480, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4301 - acc: 0.8145 - val_loss: 0.8525 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.82480 to 0.82749, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 35/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4230 - acc: 0.8142 - val_loss: 0.8903 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82749\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4280 - acc: 0.8136 - val_loss: 0.8973 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82749\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4246 - acc: 0.8148 - val_loss: 0.8958 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.82749 to 0.83019, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4344 - acc: 0.8130 - val_loss: 0.8671 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83019\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4084 - acc: 0.8219 - val_loss: 0.8903 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83019\n",
      "Epoch 40/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4133 - acc: 0.8165 - val_loss: 0.8996 - val_acc: 0.8221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83019\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4247 - acc: 0.8143 - val_loss: 0.8840 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83019\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4015 - acc: 0.8227 - val_loss: 0.8144 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.83019 to 0.84097, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4019 - acc: 0.8224 - val_loss: 0.9323 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84097\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4075 - acc: 0.8202 - val_loss: 0.8899 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84097\n",
      "Epoch 45/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3868 - acc: 0.8296 - val_loss: 0.8271 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84097\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.4068 - acc: 0.8178 - val_loss: 0.9028 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84097\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3993 - acc: 0.8181 - val_loss: 0.8820 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84097\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3888 - acc: 0.8202 - val_loss: 0.8324 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84097\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3966 - acc: 0.8238 - val_loss: 0.8574 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84097\n",
      "Epoch 50/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3922 - acc: 0.8267 - val_loss: 0.8984 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84097\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3991 - acc: 0.8180 - val_loss: 0.9311 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84097\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3857 - acc: 0.8286 - val_loss: 0.8570 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84097\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3918 - acc: 0.8230 - val_loss: 0.8517 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84097\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3763 - acc: 0.8253 - val_loss: 0.8857 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84097\n",
      "Epoch 55/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3706 - acc: 0.8273 - val_loss: 0.8830 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84097\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3668 - acc: 0.8316 - val_loss: 0.9331 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84097\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3784 - acc: 0.8265 - val_loss: 0.7941 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84097\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3699 - acc: 0.8302 - val_loss: 0.8307 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84097\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3799 - acc: 0.8276 - val_loss: 0.8008 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84097\n",
      "Epoch 60/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3769 - acc: 0.8246 - val_loss: 0.8592 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84097\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3730 - acc: 0.8306 - val_loss: 0.8061 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.84097 to 0.84636, saving model to model/mfcc7/LGD_fold4_self_resnet.h5\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3704 - acc: 0.8286 - val_loss: 1.0292 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84636\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3703 - acc: 0.8266 - val_loss: 0.8095 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84636\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3451 - acc: 0.8343 - val_loss: 0.8221 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84636\n",
      "Epoch 65/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3417 - acc: 0.8366 - val_loss: 0.8131 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84636\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3247 - acc: 0.8421 - val_loss: 0.7897 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84636\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 126s 247ms/step - loss: 1.3205 - acc: 0.8446 - val_loss: 0.7910 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84636\n",
      "Epoch 68/3000\n",
      "111/510 [=====>........................] - ETA: 1:37 - loss: 1.3397 - acc: 0.8364"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-338dba80f4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_semi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY_semi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===train semi_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel_semi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unverified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_semi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_semi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# MFCC7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#6=>0.87062\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7074cf8a78b4>\u001b[0m in \u001b[0;36mtrain_unverified\u001b[0;34m(X_semi, Y_semi, fold)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#                         use_multiprocessing = True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m#                         batch_size=batchSize,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;31m#                         initial_epoch = int(patien/20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                        )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "#     print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "#     model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(X_semi,Y_semi,fold)\n",
    "# MFCC7\n",
    "#6=>0.87062\n",
    "#0=>0.87062 (X) 需要在一次fine tune\n",
    "#1=>0.87332\n",
    "#2=>0.87332\n",
    "#3=>0.90296\n",
    "#7=>0.87332\n",
    "#4=>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(8162, 64, 431, 1) (8162, 41)\n",
      "===train semi_6===\n",
      "semi loading: model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 32, 216, 64)  3200        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 32, 216, 64)  256         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 32, 216, 64)  0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 16, 108, 64)  4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 16, 108, 64)  256         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 16, 108, 64)  0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 16, 108, 64)  36928       activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 16, 108, 64)  256         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 16, 108, 64)  0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 16, 108, 256) 16640       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 16, 108, 256) 16640       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 16, 108, 256) 0           conv2d_218[0][0]                 \n",
      "                                                                 conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 16, 108, 256) 1024        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 16, 108, 256) 0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 16, 108, 64)  16448       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 16, 108, 64)  256         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 16, 108, 64)  0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 16, 108, 64)  36928       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 16, 108, 64)  256         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 16, 108, 64)  0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 16, 108, 256) 16640       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 108, 256) 0           add_74[0][0]                     \n",
      "                                                                 conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 16, 108, 256) 1024        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 16, 108, 256) 0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 16, 108, 64)  16448       activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 16, 108, 64)  256         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 16, 108, 64)  0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 16, 108, 64)  36928       activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 16, 108, 64)  256         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 16, 108, 64)  0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 16, 108, 256) 16640       activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 16, 108, 256) 0           add_75[0][0]                     \n",
      "                                                                 conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 16, 108, 256) 1024        add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 16, 108, 256) 0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 8, 54, 128)   32896       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 8, 54, 128)   512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 8, 54, 128)   0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 8, 54, 128)   147584      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 8, 54, 128)   512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 8, 54, 128)   0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 8, 54, 512)   131584      add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 8, 54, 512)   66048       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 8, 54, 512)   0           conv2d_228[0][0]                 \n",
      "                                                                 conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 8, 54, 512)   2048        add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 8, 54, 512)   0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 8, 54, 128)   65664       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 54, 128)   512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 8, 54, 128)   0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 8, 54, 128)   147584      activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 8, 54, 128)   512         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 8, 54, 128)   0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 8, 54, 512)   66048       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 8, 54, 512)   0           add_77[0][0]                     \n",
      "                                                                 conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 8, 54, 512)   2048        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 8, 54, 512)   0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 8, 54, 128)   65664       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 8, 54, 128)   512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 8, 54, 128)   0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 8, 54, 128)   147584      activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 8, 54, 128)   512         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 8, 54, 128)   0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 8, 54, 512)   66048       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 8, 54, 512)   0           add_78[0][0]                     \n",
      "                                                                 conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 8, 54, 512)   2048        add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 8, 54, 512)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 8, 54, 128)   65664       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 8, 54, 128)   512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 8, 54, 128)   0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 8, 54, 128)   147584      activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 8, 54, 128)   512         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 8, 54, 128)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 8, 54, 512)   66048       activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 8, 54, 512)   0           add_79[0][0]                     \n",
      "                                                                 conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 8, 54, 512)   2048        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 8, 54, 512)   0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 8, 54, 128)   65664       activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 8, 54, 128)   512         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 8, 54, 128)   0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 8, 54, 128)   147584      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 8, 54, 128)   512         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 8, 54, 128)   0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 8, 54, 512)   66048       activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 8, 54, 512)   0           add_80[0][0]                     \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 8, 54, 512)   2048        add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 8, 54, 512)   0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 8, 54, 128)   65664       activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 8, 54, 128)   512         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 8, 54, 128)   0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 8, 54, 128)   147584      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 8, 54, 128)   512         conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 8, 54, 128)   0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 8, 54, 512)   66048       activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 8, 54, 512)   0           add_81[0][0]                     \n",
      "                                                                 conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 8, 54, 512)   2048        add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 8, 54, 512)   0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 8, 54, 128)   65664       activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 8, 54, 128)   512         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 8, 54, 128)   0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 8, 54, 128)   147584      activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 8, 54, 128)   512         conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 8, 54, 128)   0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 8, 54, 512)   66048       activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 8, 54, 512)   0           add_82[0][0]                     \n",
      "                                                                 conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 8, 54, 512)   2048        add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 8, 54, 512)   0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 8, 54, 128)   65664       activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 8, 54, 128)   512         conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 8, 54, 128)   0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 8, 54, 128)   147584      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 8, 54, 128)   512         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 8, 54, 128)   0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 8, 54, 512)   66048       activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 8, 54, 512)   0           add_83[0][0]                     \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 8, 54, 512)   2048        add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 8, 54, 512)   0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 4, 27, 256)   131328      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 4, 27, 256)   1024        conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 4, 27, 256)   0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 4, 27, 256)   590080      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 4, 27, 256)   1024        conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 4, 27, 256)   0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 4, 27, 1024)  525312      add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 4, 27, 1024)  263168      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 4, 27, 1024)  0           conv2d_253[0][0]                 \n",
      "                                                                 conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 4, 27, 1024)  4096        add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 4, 27, 256)   262400      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 4, 27, 256)   1024        conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 4, 27, 256)   0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 4, 27, 256)   590080      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 4, 27, 256)   1024        conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 4, 27, 256)   0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 4, 27, 1024)  263168      activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 4, 27, 1024)  0           add_85[0][0]                     \n",
      "                                                                 conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 4, 27, 1024)  4096        add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 4, 27, 256)   262400      activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 4, 27, 256)   1024        conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 4, 27, 256)   0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 4, 27, 256)   590080      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 4, 27, 256)   1024        conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 4, 27, 256)   0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 4, 27, 1024)  263168      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 4, 27, 1024)  0           add_86[0][0]                     \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 4, 27, 1024)  4096        add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 4, 27, 256)   262400      activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 4, 27, 256)   1024        conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 4, 27, 256)   0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 4, 27, 256)   590080      activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 4, 27, 256)   1024        conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 4, 27, 256)   0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, 4, 27, 1024)  263168      activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 4, 27, 1024)  0           add_87[0][0]                     \n",
      "                                                                 conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 4, 27, 1024)  4096        add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 4, 27, 256)   262400      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 4, 27, 256)   1024        conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 4, 27, 256)   0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 4, 27, 256)   590080      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 4, 27, 256)   1024        conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 4, 27, 256)   0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, 4, 27, 1024)  263168      activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 4, 27, 1024)  0           add_88[0][0]                     \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 4, 27, 1024)  4096        add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, 4, 27, 256)   262400      activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 4, 27, 256)   1024        conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 4, 27, 256)   0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, 4, 27, 256)   590080      activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 4, 27, 256)   1024        conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 4, 27, 256)   0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, 4, 27, 1024)  263168      activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 4, 27, 1024)  0           add_89[0][0]                     \n",
      "                                                                 conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 4, 27, 1024)  4096        add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, 4, 27, 256)   262400      activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 4, 27, 256)   1024        conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 4, 27, 256)   0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, 4, 27, 256)   590080      activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 4, 27, 256)   1024        conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 4, 27, 256)   0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, 4, 27, 1024)  263168      activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 4, 27, 1024)  0           add_90[0][0]                     \n",
      "                                                                 conv2d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 4, 27, 1024)  4096        add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, 4, 27, 256)   262400      activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 4, 27, 256)   1024        conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 4, 27, 256)   0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, 4, 27, 256)   590080      activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 4, 27, 256)   1024        conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 4, 27, 256)   0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, 4, 27, 1024)  263168      activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 4, 27, 1024)  0           add_91[0][0]                     \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 4, 27, 1024)  4096        add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, 4, 27, 256)   262400      activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, 4, 27, 256)   1024        conv2d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 4, 27, 256)   0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, 4, 27, 256)   590080      activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 4, 27, 256)   1024        conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 4, 27, 256)   0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, 4, 27, 1024)  263168      activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 4, 27, 1024)  0           add_92[0][0]                     \n",
      "                                                                 conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 4, 27, 1024)  4096        add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, 4, 27, 256)   262400      activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, 4, 27, 256)   1024        conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 4, 27, 256)   0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 4, 27, 256)   590080      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, 4, 27, 256)   1024        conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 4, 27, 256)   0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 4, 27, 1024)  263168      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 4, 27, 1024)  0           add_93[0][0]                     \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, 4, 27, 1024)  4096        add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, 4, 27, 256)   262400      activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 4, 27, 256)   1024        conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 4, 27, 256)   0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 4, 27, 256)   590080      activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 4, 27, 256)   1024        conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 4, 27, 256)   0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)             (None, 4, 27, 1024)  263168      activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 4, 27, 1024)  0           add_94[0][0]                     \n",
      "                                                                 conv2d_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 4, 27, 1024)  4096        add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)             (None, 4, 27, 256)   262400      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 4, 27, 256)   1024        conv2d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 4, 27, 256)   0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 4, 27, 256)   590080      activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 4, 27, 256)   1024        conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 4, 27, 256)   0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 4, 27, 1024)  263168      activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 4, 27, 1024)  0           add_95[0][0]                     \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 4, 27, 1024)  4096        add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 4, 27, 256)   262400      activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 4, 27, 256)   1024        conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 4, 27, 256)   0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 4, 27, 256)   590080      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 4, 27, 256)   1024        conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 4, 27, 256)   0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 4, 27, 1024)  263168      activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 4, 27, 1024)  0           add_96[0][0]                     \n",
      "                                                                 conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 4, 27, 1024)  4096        add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 4, 27, 256)   262400      activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 4, 27, 256)   1024        conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 4, 27, 256)   0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 4, 27, 256)   590080      activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 4, 27, 256)   1024        conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 4, 27, 256)   0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 4, 27, 1024)  263168      activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 4, 27, 1024)  0           add_97[0][0]                     \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 4, 27, 1024)  4096        add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 4, 27, 256)   262400      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 4, 27, 256)   1024        conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 4, 27, 256)   0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 4, 27, 256)   590080      activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 4, 27, 256)   1024        conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 4, 27, 256)   0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 4, 27, 1024)  263168      activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 4, 27, 1024)  0           add_98[0][0]                     \n",
      "                                                                 conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 4, 27, 1024)  4096        add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 4, 27, 256)   262400      activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 4, 27, 256)   1024        conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 4, 27, 256)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 4, 27, 256)   590080      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 4, 27, 256)   1024        conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 4, 27, 256)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 4, 27, 1024)  263168      activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 4, 27, 1024)  0           add_99[0][0]                     \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 4, 27, 1024)  4096        add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 4, 27, 256)   262400      activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 4, 27, 256)   1024        conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 4, 27, 256)   0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 4, 27, 256)   590080      activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 4, 27, 256)   1024        conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 4, 27, 256)   0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 4, 27, 1024)  263168      activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 4, 27, 1024)  0           add_100[0][0]                    \n",
      "                                                                 conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 4, 27, 1024)  4096        add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 4, 27, 256)   262400      activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 4, 27, 256)   1024        conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 4, 27, 256)   0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 4, 27, 256)   590080      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 4, 27, 256)   1024        conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 4, 27, 256)   0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 4, 27, 1024)  263168      activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 4, 27, 1024)  0           add_101[0][0]                    \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 4, 27, 1024)  4096        add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 4, 27, 256)   262400      activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 4, 27, 256)   1024        conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 4, 27, 256)   0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 4, 27, 256)   590080      activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 4, 27, 256)   1024        conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 4, 27, 256)   0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 4, 27, 1024)  263168      activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 4, 27, 1024)  0           add_102[0][0]                    \n",
      "                                                                 conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 4, 27, 1024)  4096        add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 4, 27, 256)   262400      activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 4, 27, 256)   1024        conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 4, 27, 256)   0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 4, 27, 256)   590080      activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 4, 27, 256)   1024        conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 4, 27, 256)   0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 4, 27, 1024)  263168      activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 4, 27, 1024)  0           add_103[0][0]                    \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 4, 27, 1024)  4096        add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 4, 27, 256)   262400      activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 4, 27, 256)   1024        conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 4, 27, 256)   0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 4, 27, 256)   590080      activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 4, 27, 256)   1024        conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 4, 27, 256)   0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 4, 27, 1024)  263168      activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 4, 27, 1024)  0           add_104[0][0]                    \n",
      "                                                                 conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 4, 27, 1024)  4096        add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 4, 27, 256)   262400      activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 4, 27, 256)   1024        conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 4, 27, 256)   0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 4, 27, 256)   590080      activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 4, 27, 256)   1024        conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 4, 27, 256)   0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 4, 27, 1024)  263168      activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 4, 27, 1024)  0           add_105[0][0]                    \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 4, 27, 1024)  4096        add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 4, 27, 256)   262400      activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 4, 27, 256)   1024        conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 4, 27, 256)   0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 4, 27, 256)   590080      activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 4, 27, 256)   1024        conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 4, 27, 256)   0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 4, 27, 1024)  263168      activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 4, 27, 1024)  0           add_106[0][0]                    \n",
      "                                                                 conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 4, 27, 1024)  4096        add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 4, 27, 256)   262400      activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 4, 27, 256)   1024        conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 4, 27, 256)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 4, 27, 256)   590080      activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 4, 27, 256)   1024        conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 4, 27, 256)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 4, 27, 1024)  263168      activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 4, 27, 1024)  0           add_107[0][0]                    \n",
      "                                                                 conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 4, 27, 1024)  4096        add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 4, 27, 256)   262400      activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 4, 27, 256)   1024        conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 4, 27, 256)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 4, 27, 256)   590080      activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 4, 27, 256)   1024        conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 4, 27, 256)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 4, 27, 1024)  263168      activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 4, 27, 1024)  0           add_108[0][0]                    \n",
      "                                                                 conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 4, 27, 1024)  4096        add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 4, 27, 256)   262400      activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 4, 27, 256)   1024        conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 4, 27, 256)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 4, 27, 256)   590080      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 4, 27, 256)   1024        conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 4, 27, 256)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 4, 27, 1024)  263168      activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 4, 27, 1024)  0           add_109[0][0]                    \n",
      "                                                                 conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 4, 27, 1024)  4096        add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 4, 27, 256)   262400      activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 4, 27, 256)   1024        conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 4, 27, 256)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 4, 27, 256)   590080      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 4, 27, 256)   1024        conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 4, 27, 256)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 4, 27, 1024)  263168      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 4, 27, 1024)  0           add_110[0][0]                    \n",
      "                                                                 conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 4, 27, 1024)  4096        add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 4, 27, 256)   262400      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 4, 27, 256)   1024        conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 4, 27, 256)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 4, 27, 256)   590080      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 4, 27, 256)   1024        conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 4, 27, 256)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 4, 27, 1024)  263168      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 4, 27, 1024)  0           add_111[0][0]                    \n",
      "                                                                 conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 4, 27, 1024)  4096        add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 4, 27, 256)   262400      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 4, 27, 256)   1024        conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 4, 27, 256)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 4, 27, 256)   590080      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 4, 27, 256)   1024        conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 4, 27, 256)   0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 4, 27, 1024)  263168      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 4, 27, 1024)  0           add_112[0][0]                    \n",
      "                                                                 conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 4, 27, 1024)  4096        add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 4, 27, 256)   262400      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 4, 27, 256)   1024        conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 27, 256)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 4, 27, 256)   590080      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 4, 27, 256)   1024        conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 27, 256)   0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 4, 27, 1024)  263168      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 4, 27, 1024)  0           add_113[0][0]                    \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 4, 27, 1024)  4096        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 4, 27, 256)   262400      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 27, 256)   1024        conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 27, 256)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 4, 27, 256)   590080      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 27, 256)   1024        conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 27, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 4, 27, 1024)  263168      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 4, 27, 1024)  0           add_114[0][0]                    \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 27, 1024)  4096        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 4, 27, 256)   262400      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 27, 256)   1024        conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 27, 256)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 4, 27, 256)   590080      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 27, 256)   1024        conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 27, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 4, 27, 1024)  263168      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 4, 27, 1024)  0           add_115[0][0]                    \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 27, 1024)  4096        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 27, 256)   262400      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 27, 256)   1024        conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 27, 256)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 27, 256)   590080      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 27, 256)   1024        conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 27, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 27, 1024)  263168      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 4, 27, 1024)  0           add_116[0][0]                    \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 27, 1024)  4096        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 27, 256)   262400      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 27, 256)   1024        conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 27, 256)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 27, 256)   590080      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 27, 256)   1024        conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 27, 256)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 27, 1024)  263168      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 4, 27, 1024)  0           add_117[0][0]                    \n",
      "                                                                 conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 27, 1024)  4096        add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 27, 256)   262400      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 27, 256)   1024        conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 27, 256)   0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 27, 256)   590080      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 27, 256)   1024        conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 27, 256)   0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 27, 1024)  263168      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 4, 27, 1024)  0           add_118[0][0]                    \n",
      "                                                                 conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 27, 1024)  4096        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 27, 256)   262400      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 27, 256)   1024        conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 27, 256)   0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 27, 256)   590080      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 27, 256)   1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 27, 256)   0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 27, 1024)  263168      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 4, 27, 1024)  0           add_119[0][0]                    \n",
      "                                                                 conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 27, 1024)  4096        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 2, 14, 512)   524800      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 2, 14, 512)   2048        conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 2, 14, 512)   0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 2, 14, 512)   2359808     activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 2, 14, 512)   2048        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 2, 14, 512)   0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 2, 14, 2048)  2099200     add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 2, 14, 2048)  0           conv2d_362[0][0]                 \n",
      "                                                                 conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 2, 14, 2048)  8192        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 2, 14, 512)   1049088     activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 2, 14, 512)   2048        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 2, 14, 512)   0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 2, 14, 512)   2359808     activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 2, 14, 512)   2048        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 2, 14, 512)   0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 2, 14, 2048)  0           add_121[0][0]                    \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 2, 14, 2048)  8192        add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 2, 14, 512)   1049088     activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 2, 14, 512)   2048        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 2, 14, 512)   0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 2, 14, 512)   2359808     activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 2, 14, 512)   2048        conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 2, 14, 512)   0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 2, 14, 2048)  0           add_122[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 2, 14, 2048)  8192        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 2048)   0           activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 52)           106548      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 52)           208         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 52)           0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 41)           2173        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 58,458,241\n",
      "Trainable params: 58,314,393\n",
      "Non-trainable params: 143,848\n",
      "__________________________________________________________________________________________________\n",
      "BS: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "510/510 [==============================] - 181s 355ms/step - loss: 1.4374 - acc: 0.8197 - val_loss: 0.7829 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85175, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 162s 317ms/step - loss: 1.4192 - acc: 0.8301 - val_loss: 0.8296 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.85175\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.4061 - acc: 0.8274 - val_loss: 0.7856 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85175 to 0.85175, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.4015 - acc: 0.8269 - val_loss: 0.7647 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85175\n",
      "Epoch 5/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3901 - acc: 0.8307 - val_loss: 0.7758 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85175 to 0.85714, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3873 - acc: 0.8292 - val_loss: 0.7912 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85714\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3735 - acc: 0.8336 - val_loss: 0.8265 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85714\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3768 - acc: 0.8357 - val_loss: 0.7654 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.85714 to 0.85984, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3786 - acc: 0.8311 - val_loss: 0.7734 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85984\n",
      "Epoch 10/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3732 - acc: 0.8344 - val_loss: 0.7703 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85984\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3616 - acc: 0.8320 - val_loss: 0.7559 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85984\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3813 - acc: 0.8289 - val_loss: 0.8302 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85984\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3641 - acc: 0.8349 - val_loss: 0.7583 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85984\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3769 - acc: 0.8300 - val_loss: 0.7876 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85984\n",
      "Epoch 15/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3677 - acc: 0.8331 - val_loss: 0.7610 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85984\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3449 - acc: 0.8373 - val_loss: 0.7838 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85984\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3627 - acc: 0.8366 - val_loss: 0.7753 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.85984 to 0.86253, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3593 - acc: 0.8346 - val_loss: 0.7706 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86253\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3631 - acc: 0.8347 - val_loss: 0.7444 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.86253 to 0.86253, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 20/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3382 - acc: 0.8357 - val_loss: 0.7342 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.86253 to 0.87062, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3510 - acc: 0.8403 - val_loss: 0.8192 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87062\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3453 - acc: 0.8344 - val_loss: 0.7643 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87062\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3513 - acc: 0.8387 - val_loss: 0.7750 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87062\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3429 - acc: 0.8395 - val_loss: 0.7471 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87062\n",
      "Epoch 25/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3493 - acc: 0.8338 - val_loss: 0.7580 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87062\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3325 - acc: 0.8406 - val_loss: 0.7289 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87062\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3320 - acc: 0.8429 - val_loss: 0.7612 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87062\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3285 - acc: 0.8398 - val_loss: 0.7535 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87062\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3331 - acc: 0.8375 - val_loss: 0.7455 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87062\n",
      "Epoch 30/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3272 - acc: 0.8407 - val_loss: 0.7833 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87062\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3270 - acc: 0.8415 - val_loss: 0.7709 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87062\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3262 - acc: 0.8355 - val_loss: 0.7528 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87062\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3357 - acc: 0.8416 - val_loss: 0.7830 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87062\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.3255 - acc: 0.8406 - val_loss: 0.7576 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87062\n",
      "Epoch 35/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3210 - acc: 0.8448 - val_loss: 0.7984 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87062\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3205 - acc: 0.8415 - val_loss: 0.7249 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87062\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3148 - acc: 0.8352 - val_loss: 0.7452 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87062\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3196 - acc: 0.8419 - val_loss: 0.7749 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.87062 to 0.87332, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.3070 - acc: 0.8413 - val_loss: 0.7673 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87332\n",
      "Epoch 40/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3263 - acc: 0.8449 - val_loss: 0.7282 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.87332 to 0.87601, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 41/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3091 - acc: 0.8412 - val_loss: 0.7326 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87601\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3162 - acc: 0.8379 - val_loss: 0.7538 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87601\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3102 - acc: 0.8433 - val_loss: 0.7590 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87601\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3013 - acc: 0.8437 - val_loss: 0.7374 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87601\n",
      "Epoch 45/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2971 - acc: 0.8455 - val_loss: 0.7104 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87601\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2989 - acc: 0.8448 - val_loss: 0.7475 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87601\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2959 - acc: 0.8441 - val_loss: 0.7916 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87601\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.3150 - acc: 0.8424 - val_loss: 0.7404 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87601\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2925 - acc: 0.8415 - val_loss: 0.7614 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87601\n",
      "Epoch 50/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2891 - acc: 0.8449 - val_loss: 0.7255 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87601\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2912 - acc: 0.8445 - val_loss: 0.7699 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87601\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2929 - acc: 0.8382 - val_loss: 0.7729 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87601\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2939 - acc: 0.8444 - val_loss: 0.7467 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87601\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2817 - acc: 0.8426 - val_loss: 0.7308 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87601\n",
      "Epoch 55/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2902 - acc: 0.8460 - val_loss: 0.7670 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87601\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2861 - acc: 0.8482 - val_loss: 0.7936 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87601\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2685 - acc: 0.8479 - val_loss: 0.7567 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87601\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2715 - acc: 0.8485 - val_loss: 0.8005 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87601\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2828 - acc: 0.8461 - val_loss: 0.7558 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87601\n",
      "Epoch 60/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2738 - acc: 0.8490 - val_loss: 0.7595 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87601\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2783 - acc: 0.8452 - val_loss: 0.7441 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87601\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.2746 - acc: 0.8477 - val_loss: 0.7196 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87601\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 163s 319ms/step - loss: 1.2700 - acc: 0.8494 - val_loss: 0.7612 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87601\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 162s 319ms/step - loss: 1.2784 - acc: 0.8442 - val_loss: 0.7267 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87601\n",
      "Epoch 65/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2535 - acc: 0.8499 - val_loss: 0.7383 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87601\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2532 - acc: 0.8513 - val_loss: 0.7285 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87601\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2468 - acc: 0.8509 - val_loss: 0.7244 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87601\n",
      "Epoch 68/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2418 - acc: 0.8539 - val_loss: 0.7134 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87601\n",
      "Epoch 69/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2358 - acc: 0.8498 - val_loss: 0.7076 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87601\n",
      "Epoch 70/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2504 - acc: 0.8499 - val_loss: 0.7292 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87601\n",
      "Epoch 71/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2426 - acc: 0.8556 - val_loss: 0.6896 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.87601 to 0.87871, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 72/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2436 - acc: 0.8478 - val_loss: 0.7246 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87871\n",
      "Epoch 73/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2400 - acc: 0.8534 - val_loss: 0.7000 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87871\n",
      "Epoch 74/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2516 - acc: 0.8531 - val_loss: 0.6815 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87871\n",
      "Epoch 75/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2320 - acc: 0.8536 - val_loss: 0.6952 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.87871 to 0.88140, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 76/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2357 - acc: 0.8532 - val_loss: 0.6981 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88140\n",
      "Epoch 77/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2331 - acc: 0.8507 - val_loss: 0.6934 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88140\n",
      "Epoch 78/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2351 - acc: 0.8542 - val_loss: 0.7012 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88140\n",
      "Epoch 79/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2316 - acc: 0.8523 - val_loss: 0.7194 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88140\n",
      "Epoch 80/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2337 - acc: 0.8525 - val_loss: 0.6947 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88140\n",
      "Epoch 81/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2288 - acc: 0.8521 - val_loss: 0.7077 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88140\n",
      "Epoch 82/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2311 - acc: 0.8501 - val_loss: 0.7298 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88140\n",
      "Epoch 83/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2254 - acc: 0.8536 - val_loss: 0.7152 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88140\n",
      "Epoch 84/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2286 - acc: 0.8502 - val_loss: 0.6867 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88140\n",
      "Epoch 85/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2424 - acc: 0.8508 - val_loss: 0.7177 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88140\n",
      "Epoch 86/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2329 - acc: 0.8529 - val_loss: 0.6589 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.88140 to 0.88679, saving model to model/mfcc6/LGD_fold6_self_resnet.h5\n",
      "Epoch 87/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2227 - acc: 0.8543 - val_loss: 0.7074 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88679\n",
      "Epoch 88/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2184 - acc: 0.8575 - val_loss: 0.7005 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88679\n",
      "Epoch 89/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2158 - acc: 0.8577 - val_loss: 0.7000 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88679\n",
      "Epoch 90/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2208 - acc: 0.8547 - val_loss: 0.6840 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88679\n",
      "Epoch 91/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2188 - acc: 0.8556 - val_loss: 0.6759 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88679\n",
      "Epoch 92/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2239 - acc: 0.8552 - val_loss: 0.7090 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88679\n",
      "Epoch 93/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2158 - acc: 0.8575 - val_loss: 0.7094 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88679\n",
      "Epoch 94/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2190 - acc: 0.8559 - val_loss: 0.6883 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88679\n",
      "Epoch 95/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2245 - acc: 0.8501 - val_loss: 0.6834 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88679\n",
      "Epoch 96/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2114 - acc: 0.8578 - val_loss: 0.7057 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88679\n",
      "Epoch 97/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2112 - acc: 0.8581 - val_loss: 0.6894 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88679\n",
      "Epoch 98/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2126 - acc: 0.8559 - val_loss: 0.6810 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88679\n",
      "Epoch 99/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2206 - acc: 0.8562 - val_loss: 0.7144 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.88679\n",
      "Epoch 100/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2069 - acc: 0.8566 - val_loss: 0.7190 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.88679\n",
      "Epoch 101/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2123 - acc: 0.8566 - val_loss: 0.7155 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.88679\n",
      "Epoch 102/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2060 - acc: 0.8548 - val_loss: 0.7061 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.88679\n",
      "Epoch 103/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2158 - acc: 0.8563 - val_loss: 0.7054 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.88679\n",
      "Epoch 104/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2079 - acc: 0.8585 - val_loss: 0.6719 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.88679\n",
      "Epoch 105/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2053 - acc: 0.8563 - val_loss: 0.6820 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.88679\n",
      "Epoch 106/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2088 - acc: 0.8552 - val_loss: 0.7001 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.88679\n",
      "Epoch 107/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2097 - acc: 0.8535 - val_loss: 0.7000 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.88679\n",
      "Epoch 108/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.1936 - acc: 0.8594 - val_loss: 0.6972 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.88679\n",
      "Epoch 109/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.1883 - acc: 0.8576 - val_loss: 0.6907 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.88679\n",
      "Epoch 110/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2138 - acc: 0.8525 - val_loss: 0.6970 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.88679\n",
      "Epoch 111/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2080 - acc: 0.8573 - val_loss: 0.7135 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.88679\n",
      "Epoch 112/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2036 - acc: 0.8540 - val_loss: 0.7084 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.88679\n",
      "Epoch 113/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2017 - acc: 0.8532 - val_loss: 0.6964 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.88679\n",
      "Epoch 114/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2056 - acc: 0.8589 - val_loss: 0.6787 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.88679\n",
      "Epoch 115/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2015 - acc: 0.8591 - val_loss: 0.7078 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.88679\n",
      "Epoch 116/3000\n",
      "510/510 [==============================] - 162s 318ms/step - loss: 1.2000 - acc: 0.8575 - val_loss: 0.6831 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.88679\n",
      "Epoch 00116: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(8162, 64, 431, 1) (8162, 41)\n",
      "===train semi_7===\n",
      "semi loading: model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 32, 216, 64)  3200        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 32, 216, 64)  256         conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 32, 216, 64)  0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 16, 108, 64)  4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 16, 108, 64)  256         conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 16, 108, 64)  0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 16, 108, 64)  36928       activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 16, 108, 64)  256         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 16, 108, 64)  0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 16, 108, 256) 16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 16, 108, 256) 16640       activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 16, 108, 256) 0           conv2d_429[0][0]                 \n",
      "                                                                 conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 16, 108, 256) 1024        add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 16, 108, 256) 0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 16, 108, 64)  16448       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 16, 108, 64)  256         conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 16, 108, 64)  0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 16, 108, 64)  36928       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 16, 108, 64)  256         conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 16, 108, 64)  0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 16, 108, 256) 16640       activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 16, 108, 256) 0           add_148[0][0]                    \n",
      "                                                                 conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 16, 108, 256) 1024        add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 16, 108, 256) 0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 16, 108, 64)  16448       activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 16, 108, 64)  256         conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 16, 108, 64)  0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 16, 108, 64)  36928       activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 16, 108, 64)  256         conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 16, 108, 64)  0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 16, 108, 256) 16640       activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 16, 108, 256) 0           add_149[0][0]                    \n",
      "                                                                 conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 16, 108, 256) 1024        add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 16, 108, 256) 0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 8, 54, 128)   32896       activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 8, 54, 128)   512         conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 8, 54, 128)   0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 8, 54, 128)   147584      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 8, 54, 128)   512         conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 8, 54, 128)   0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 8, 54, 512)   131584      add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 8, 54, 512)   66048       activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 8, 54, 512)   0           conv2d_439[0][0]                 \n",
      "                                                                 conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 8, 54, 512)   2048        add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 8, 54, 512)   0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 8, 54, 128)   65664       activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 8, 54, 128)   512         conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 8, 54, 128)   0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 8, 54, 128)   147584      activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 8, 54, 128)   512         conv2d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 8, 54, 128)   0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 8, 54, 512)   66048       activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_152 (Add)                   (None, 8, 54, 512)   0           add_151[0][0]                    \n",
      "                                                                 conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 8, 54, 512)   2048        add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 8, 54, 512)   0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 8, 54, 128)   65664       activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 8, 54, 128)   512         conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 8, 54, 128)   0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 8, 54, 128)   147584      activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 8, 54, 128)   512         conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 8, 54, 128)   0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 8, 54, 512)   66048       activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_153 (Add)                   (None, 8, 54, 512)   0           add_152[0][0]                    \n",
      "                                                                 conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 8, 54, 512)   2048        add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 8, 54, 512)   0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 8, 54, 128)   65664       activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 8, 54, 128)   512         conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 8, 54, 128)   0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 8, 54, 128)   147584      activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 8, 54, 128)   512         conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 8, 54, 128)   0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 8, 54, 512)   66048       activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_154 (Add)                   (None, 8, 54, 512)   0           add_153[0][0]                    \n",
      "                                                                 conv2d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 8, 54, 512)   2048        add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 8, 54, 512)   0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 4, 27, 256)   131328      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 4, 27, 256)   1024        conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 4, 27, 256)   0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 4, 27, 256)   590080      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 4, 27, 256)   1024        conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 4, 27, 256)   0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 4, 27, 1024)  525312      add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 4, 27, 1024)  263168      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_155 (Add)                   (None, 4, 27, 1024)  0           conv2d_452[0][0]                 \n",
      "                                                                 conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 4, 27, 1024)  4096        add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 4, 27, 256)   262400      activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 4, 27, 256)   1024        conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 4, 27, 256)   0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 4, 27, 256)   590080      activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 4, 27, 256)   1024        conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 4, 27, 256)   0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 4, 27, 1024)  263168      activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_156 (Add)                   (None, 4, 27, 1024)  0           add_155[0][0]                    \n",
      "                                                                 conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 4, 27, 1024)  4096        add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 4, 27, 256)   262400      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 4, 27, 256)   1024        conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 4, 27, 256)   0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 4, 27, 256)   590080      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 4, 27, 256)   1024        conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 4, 27, 256)   0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 4, 27, 1024)  263168      activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, 4, 27, 1024)  0           add_156[0][0]                    \n",
      "                                                                 conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 4, 27, 1024)  4096        add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 4, 27, 256)   262400      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 4, 27, 256)   1024        conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 4, 27, 256)   0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 4, 27, 256)   590080      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 4, 27, 256)   1024        conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 4, 27, 256)   0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 4, 27, 1024)  263168      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, 4, 27, 1024)  0           add_157[0][0]                    \n",
      "                                                                 conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 4, 27, 1024)  4096        add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 4, 27, 256)   262400      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 4, 27, 256)   1024        conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 4, 27, 256)   0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 4, 27, 256)   590080      activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 4, 27, 256)   1024        conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 4, 27, 256)   0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 4, 27, 1024)  263168      activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_159 (Add)                   (None, 4, 27, 1024)  0           add_158[0][0]                    \n",
      "                                                                 conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 4, 27, 1024)  4096        add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 4, 27, 256)   262400      activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 4, 27, 256)   1024        conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 4, 27, 256)   0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 4, 27, 256)   590080      activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 4, 27, 256)   1024        conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 4, 27, 256)   0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 4, 27, 1024)  263168      activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, 4, 27, 1024)  0           add_159[0][0]                    \n",
      "                                                                 conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 4, 27, 1024)  4096        add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 2, 14, 512)   524800      activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 2, 14, 512)   2048        conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 2, 14, 512)   0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 2, 14, 512)   2359808     activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 2, 14, 512)   2048        conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 2, 14, 512)   0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 2, 14, 2048)  2099200     add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, 2, 14, 2048)  0           conv2d_471[0][0]                 \n",
      "                                                                 conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 2, 14, 2048)  8192        add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 2, 14, 512)   1049088     activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 2, 14, 512)   2048        conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 2, 14, 512)   0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 2, 14, 512)   2359808     activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 2, 14, 512)   2048        conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 2, 14, 512)   0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, 2, 14, 2048)  0           add_161[0][0]                    \n",
      "                                                                 conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 2, 14, 2048)  8192        add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 2, 14, 512)   1049088     activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 2, 14, 512)   2048        conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 2, 14, 512)   0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 2, 14, 512)   2359808     activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 2, 14, 512)   2048        conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 2, 14, 512)   0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, 2, 14, 2048)  0           add_162[0][0]                    \n",
      "                                                                 conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 2, 14, 2048)  8192        add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1, 1, 2048)   0           activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 45)           92205       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 45)           180         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 45)           0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 41)           1886        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 23,660,351\n",
      "Trainable params: 23,614,821\n",
      "Non-trainable params: 45,530\n",
      "__________________________________________________________________________________________________\n",
      "BS: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "510/510 [==============================] - 77s 151ms/step - loss: 1.4310 - acc: 0.7795 - val_loss: 0.7599 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87062, saving model to model/mfcc6/LGD_fold7_self_resnet.h5\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 71s 139ms/step - loss: 1.3636 - acc: 0.8020 - val_loss: 0.7726 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.87062\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.3529 - acc: 0.8094 - val_loss: 0.7523 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87062\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.3119 - acc: 0.8165 - val_loss: 0.7465 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87062 to 0.88140, saving model to model/mfcc6/LGD_fold7_self_resnet.h5\n",
      "Epoch 5/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.3069 - acc: 0.8200 - val_loss: 0.7420 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.88140\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2950 - acc: 0.8251 - val_loss: 0.7473 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88140\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.3028 - acc: 0.8279 - val_loss: 0.7367 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88140\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2873 - acc: 0.8324 - val_loss: 0.8085 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88140\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2837 - acc: 0.8306 - val_loss: 0.7618 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88140\n",
      "Epoch 10/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2918 - acc: 0.8244 - val_loss: 0.8509 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88140\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2765 - acc: 0.8330 - val_loss: 0.7765 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88140\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2746 - acc: 0.8306 - val_loss: 0.7678 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88140\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2714 - acc: 0.8310 - val_loss: 0.7626 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88140\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2643 - acc: 0.8361 - val_loss: 0.7424 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88140\n",
      "Epoch 15/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2667 - acc: 0.8349 - val_loss: 0.7768 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88140\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2502 - acc: 0.8377 - val_loss: 0.7172 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88140\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2491 - acc: 0.8369 - val_loss: 0.7191 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88140\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2559 - acc: 0.8365 - val_loss: 0.7982 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88140\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2435 - acc: 0.8403 - val_loss: 0.7641 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88140\n",
      "Epoch 20/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2379 - acc: 0.8398 - val_loss: 0.7376 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88140\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2488 - acc: 0.8351 - val_loss: 0.7706 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88140\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2536 - acc: 0.8323 - val_loss: 0.7549 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88140\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2483 - acc: 0.8305 - val_loss: 0.7722 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88140\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2378 - acc: 0.8406 - val_loss: 0.7755 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88140\n",
      "Epoch 25/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2379 - acc: 0.8377 - val_loss: 0.7640 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88140\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2295 - acc: 0.8430 - val_loss: 0.7534 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.88140 to 0.88410, saving model to model/mfcc6/LGD_fold7_self_resnet.h5\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2338 - acc: 0.8447 - val_loss: 0.7137 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88410\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2228 - acc: 0.8461 - val_loss: 0.6925 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88410\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2287 - acc: 0.8442 - val_loss: 0.7104 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88410\n",
      "Epoch 30/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2168 - acc: 0.8445 - val_loss: 0.7557 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88410\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2122 - acc: 0.8464 - val_loss: 0.7514 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88410\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2176 - acc: 0.8433 - val_loss: 0.7446 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88410\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2244 - acc: 0.8417 - val_loss: 0.7391 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88410\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2153 - acc: 0.8442 - val_loss: 0.7265 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88410\n",
      "Epoch 35/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2203 - acc: 0.8464 - val_loss: 0.7061 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88410\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2085 - acc: 0.8444 - val_loss: 0.7393 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88410\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2249 - acc: 0.8387 - val_loss: 0.7565 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88410\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2112 - acc: 0.8433 - val_loss: 0.7119 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88410\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2221 - acc: 0.8412 - val_loss: 0.7024 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88410\n",
      "Epoch 40/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2140 - acc: 0.8442 - val_loss: 0.7301 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88410\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2144 - acc: 0.8453 - val_loss: 0.7242 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88410\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2057 - acc: 0.8456 - val_loss: 0.7025 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88410\n",
      "Epoch 43/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2000 - acc: 0.8519 - val_loss: 0.7077 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.88410 to 0.89218, saving model to model/mfcc6/LGD_fold7_self_resnet.h5\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1942 - acc: 0.8477 - val_loss: 0.7035 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89218\n",
      "Epoch 45/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2082 - acc: 0.8480 - val_loss: 0.7402 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89218\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1951 - acc: 0.8447 - val_loss: 0.7386 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89218\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1940 - acc: 0.8490 - val_loss: 0.7325 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89218\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2013 - acc: 0.8461 - val_loss: 0.7518 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89218\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.2052 - acc: 0.8464 - val_loss: 0.7815 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89218\n",
      "Epoch 50/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1948 - acc: 0.8512 - val_loss: 0.7445 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89218\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1991 - acc: 0.8469 - val_loss: 0.7404 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89218\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1905 - acc: 0.8491 - val_loss: 0.7279 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89218\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1950 - acc: 0.8461 - val_loss: 0.7590 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89218\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1874 - acc: 0.8480 - val_loss: 0.7733 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89218\n",
      "Epoch 55/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1888 - acc: 0.8468 - val_loss: 0.7218 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89218\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1907 - acc: 0.8483 - val_loss: 0.7065 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89218\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1856 - acc: 0.8506 - val_loss: 0.7126 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89218\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 71s 140ms/step - loss: 1.1773 - acc: 0.8507 - val_loss: 0.7830 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89218\n",
      "Epoch 00058: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(8162, 64, 431, 1) (8162, 41)\n",
      "===train semi_8===\n",
      "semi loading: model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 32, 216, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 32, 216, 64)  256         conv2d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 32, 216, 64)  0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 16, 108, 64)  0           activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 16, 108, 64)  36928       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 16, 108, 64)  256         conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 16, 108, 64)  0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 16, 108, 64)  36928       activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_255 (Add)                   (None, 16, 108, 64)  0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 16, 108, 64)  256         add_255[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 16, 108, 64)  0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 16, 108, 64)  36928       activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 16, 108, 64)  256         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 16, 108, 64)  0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 16, 108, 64)  36928       activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_256 (Add)                   (None, 16, 108, 64)  0           add_255[0][0]                    \n",
      "                                                                 conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 16, 108, 64)  256         add_256[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 16, 108, 64)  0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 16, 108, 64)  36928       activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 16, 108, 64)  256         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 16, 108, 64)  0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 16, 108, 64)  36928       activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_257 (Add)                   (None, 16, 108, 64)  0           add_256[0][0]                    \n",
      "                                                                 conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 16, 108, 64)  256         add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 16, 108, 64)  0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 8, 54, 128)   73856       activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 8, 54, 128)   512         conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 8, 54, 128)   0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 8, 54, 128)   8320        add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 8, 54, 128)   147584      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_258 (Add)                   (None, 8, 54, 128)   0           conv2d_766[0][0]                 \n",
      "                                                                 conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 8, 54, 128)   512         add_258[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 8, 54, 128)   0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 8, 54, 128)   147584      activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 8, 54, 128)   512         conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 8, 54, 128)   0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 8, 54, 128)   147584      activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_259 (Add)                   (None, 8, 54, 128)   0           add_258[0][0]                    \n",
      "                                                                 conv2d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 8, 54, 128)   512         add_259[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 8, 54, 128)   0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 8, 54, 128)   147584      activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 8, 54, 128)   512         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 8, 54, 128)   0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 8, 54, 128)   147584      activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_260 (Add)                   (None, 8, 54, 128)   0           add_259[0][0]                    \n",
      "                                                                 conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 8, 54, 128)   512         add_260[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 8, 54, 128)   0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 8, 54, 128)   147584      activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 8, 54, 128)   512         conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 8, 54, 128)   0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 8, 54, 128)   147584      activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_261 (Add)                   (None, 8, 54, 128)   0           add_260[0][0]                    \n",
      "                                                                 conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 8, 54, 128)   512         add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 8, 54, 128)   0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 4, 27, 256)   295168      activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 4, 27, 256)   1024        conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 4, 27, 256)   0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 4, 27, 256)   33024       add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 4, 27, 256)   590080      activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_262 (Add)                   (None, 4, 27, 256)   0           conv2d_775[0][0]                 \n",
      "                                                                 conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 4, 27, 256)   1024        add_262[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 4, 27, 256)   0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 4, 27, 256)   590080      activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 4, 27, 256)   1024        conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 4, 27, 256)   0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 4, 27, 256)   590080      activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_263 (Add)                   (None, 4, 27, 256)   0           add_262[0][0]                    \n",
      "                                                                 conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 4, 27, 256)   1024        add_263[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 4, 27, 256)   0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 4, 27, 256)   590080      activation_736[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 4, 27, 256)   1024        conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 4, 27, 256)   0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 4, 27, 256)   590080      activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_264 (Add)                   (None, 4, 27, 256)   0           add_263[0][0]                    \n",
      "                                                                 conv2d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 4, 27, 256)   1024        add_264[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 4, 27, 256)   0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 4, 27, 256)   590080      activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 4, 27, 256)   1024        conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 4, 27, 256)   0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 4, 27, 256)   590080      activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_265 (Add)                   (None, 4, 27, 256)   0           add_264[0][0]                    \n",
      "                                                                 conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 4, 27, 256)   1024        add_265[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 4, 27, 256)   0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 4, 27, 256)   590080      activation_740[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 4, 27, 256)   1024        conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_741 (Activation)     (None, 4, 27, 256)   0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 4, 27, 256)   590080      activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_266 (Add)                   (None, 4, 27, 256)   0           add_265[0][0]                    \n",
      "                                                                 conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 4, 27, 256)   1024        add_266[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 4, 27, 256)   0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 4, 27, 256)   590080      activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 4, 27, 256)   1024        conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 4, 27, 256)   0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 4, 27, 256)   590080      activation_743[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_267 (Add)                   (None, 4, 27, 256)   0           add_266[0][0]                    \n",
      "                                                                 conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 4, 27, 256)   1024        add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 4, 27, 256)   0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 2, 14, 512)   1180160     activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 2, 14, 512)   2048        conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 2, 14, 512)   0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 2, 14, 512)   131584      add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 2, 14, 512)   2359808     activation_745[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_268 (Add)                   (None, 2, 14, 512)   0           conv2d_788[0][0]                 \n",
      "                                                                 conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 2, 14, 512)   2048        add_268[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 2, 14, 512)   0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 2, 14, 512)   2359808     activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 2, 14, 512)   2048        conv2d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 2, 14, 512)   0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 2, 14, 512)   2359808     activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_269 (Add)                   (None, 2, 14, 512)   0           add_268[0][0]                    \n",
      "                                                                 conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 2, 14, 512)   2048        add_269[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 2, 14, 512)   0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 2, 14, 512)   2359808     activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 2, 14, 512)   2048        conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 2, 14, 512)   0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 2, 14, 512)   2359808     activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_270 (Add)                   (None, 2, 14, 512)   0           add_269[0][0]                    \n",
      "                                                                 conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 2, 14, 512)   2048        add_270[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 2, 14, 512)   0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 512)    0           activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 512)          0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 45)           23085       flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 45)           180         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 45)           0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           1886        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,325,503\n",
      "Trainable params: 21,310,181\n",
      "Non-trainable params: 15,322\n",
      "__________________________________________________________________________________________________\n",
      "BS: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "510/510 [==============================] - 49s 96ms/step - loss: 1.1593 - acc: 0.8505 - val_loss: 0.7261 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86523, saving model to model/mfcc6/LGD_fold8_self_resnet.h5\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1354 - acc: 0.8637 - val_loss: 0.7392 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86523 to 0.86792, saving model to model/mfcc6/LGD_fold8_self_resnet.h5\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1262 - acc: 0.8653 - val_loss: 0.7651 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.86792\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1185 - acc: 0.8646 - val_loss: 0.7429 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86792\n",
      "Epoch 5/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1184 - acc: 0.8649 - val_loss: 0.7500 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86792\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1091 - acc: 0.8652 - val_loss: 0.7276 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86792\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1127 - acc: 0.8678 - val_loss: 0.7196 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86792\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1001 - acc: 0.8700 - val_loss: 0.7582 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86792\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.1053 - acc: 0.8659 - val_loss: 0.7588 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86792\n",
      "Epoch 10/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0922 - acc: 0.8645 - val_loss: 0.7802 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86792\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0810 - acc: 0.8714 - val_loss: 0.7554 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86792\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0921 - acc: 0.8706 - val_loss: 0.7510 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86792\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0954 - acc: 0.8696 - val_loss: 0.7675 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86792\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0965 - acc: 0.8721 - val_loss: 0.7786 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.86792 to 0.87062, saving model to model/mfcc6/LGD_fold8_self_resnet.h5\n",
      "Epoch 15/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0743 - acc: 0.8714 - val_loss: 0.7242 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87062\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0733 - acc: 0.8689 - val_loss: 0.7355 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.87062 to 0.87601, saving model to model/mfcc6/LGD_fold8_self_resnet.h5\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0730 - acc: 0.8727 - val_loss: 0.7341 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87601\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0736 - acc: 0.8724 - val_loss: 0.8081 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87601\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0693 - acc: 0.8724 - val_loss: 0.7212 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87601\n",
      "Epoch 20/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0700 - acc: 0.8732 - val_loss: 0.7306 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87601\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0638 - acc: 0.8740 - val_loss: 0.7478 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87601\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0592 - acc: 0.8718 - val_loss: 0.7344 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87601\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0659 - acc: 0.8711 - val_loss: 0.7722 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87601\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0604 - acc: 0.8699 - val_loss: 0.7365 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87601\n",
      "Epoch 25/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0630 - acc: 0.8726 - val_loss: 0.7813 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87601\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0578 - acc: 0.8724 - val_loss: 0.8088 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87601\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0604 - acc: 0.8705 - val_loss: 0.8029 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87601\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0518 - acc: 0.8763 - val_loss: 0.7995 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87601\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0581 - acc: 0.8722 - val_loss: 0.8168 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87601\n",
      "Epoch 30/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0536 - acc: 0.8730 - val_loss: 0.7979 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87601\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0463 - acc: 0.8719 - val_loss: 0.7817 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87601\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0501 - acc: 0.8737 - val_loss: 0.7506 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87601\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0468 - acc: 0.8711 - val_loss: 0.7774 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87601\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0504 - acc: 0.8735 - val_loss: 0.7773 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87601\n",
      "Epoch 35/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0388 - acc: 0.8705 - val_loss: 0.7618 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87601\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0431 - acc: 0.8754 - val_loss: 0.7722 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87601\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 44s 87ms/step - loss: 1.0374 - acc: 0.8773 - val_loss: 0.7871 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.87601 to 0.88140, saving model to model/mfcc6/LGD_fold8_self_resnet.h5\n",
      "Epoch 00037: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(8162, 64, 431, 1) (8162, 41)\n",
      "===train semi_9===\n",
      "semi loading: model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 216, 64)  3200        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 216, 64)  256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 216, 64)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 108, 64)  4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 108, 64)  256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 108, 64)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 108, 64)  36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 108, 64)  256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 108, 64)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 108, 256) 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 108, 256) 16640       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 108, 256) 0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 108, 256) 1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 108, 256) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 108, 64)  16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 108, 64)  256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 108, 64)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 108, 64)  36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 108, 64)  256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 108, 64)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 108, 256) 16640       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 108, 256) 0           add_25[0][0]                     \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 108, 256) 1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 108, 256) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 108, 64)  16448       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 108, 64)  256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 108, 64)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 108, 64)  36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 108, 64)  256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 108, 64)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 108, 256) 16640       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 108, 256) 0           add_26[0][0]                     \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 108, 256) 1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 108, 256) 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 54, 128)   32896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 54, 128)   512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 54, 128)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 54, 128)   147584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 54, 128)   512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 54, 128)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 54, 512)   131584      add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 54, 512)   66048       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 54, 512)   0           conv2d_71[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 54, 512)   2048        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 54, 512)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 54, 128)   65664       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 54, 128)   512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 54, 128)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 54, 128)   147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 54, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 54, 128)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 54, 512)   66048       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 54, 512)   0           add_28[0][0]                     \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 54, 512)   2048        add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 54, 512)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 54, 128)   65664       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 54, 128)   512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 54, 128)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 54, 128)   147584      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 54, 128)   512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 54, 128)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 54, 512)   66048       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 54, 512)   0           add_29[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 54, 512)   2048        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 54, 512)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 54, 128)   65664       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 54, 128)   512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 54, 128)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 54, 128)   147584      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 54, 128)   512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 54, 128)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 54, 512)   66048       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 54, 512)   0           add_30[0][0]                     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 54, 512)   2048        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 54, 512)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 27, 256)   131328      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 27, 256)   1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 27, 256)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 27, 256)   590080      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 27, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 27, 256)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 27, 1024)  525312      add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 27, 1024)  263168      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 4, 27, 1024)  0           conv2d_84[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 27, 1024)  4096        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 27, 256)   262400      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 27, 256)   1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 27, 256)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 27, 256)   590080      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 27, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 27, 256)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 27, 1024)  263168      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 27, 1024)  0           add_32[0][0]                     \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 27, 1024)  4096        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 27, 256)   262400      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 27, 256)   1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 27, 256)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 27, 256)   590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 27, 256)   1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 27, 256)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 27, 1024)  263168      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 27, 1024)  0           add_33[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 27, 1024)  4096        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 27, 256)   262400      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 27, 256)   1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 27, 256)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 27, 256)   590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 27, 256)   1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 27, 256)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 27, 1024)  263168      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 27, 1024)  0           add_34[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 27, 1024)  4096        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 27, 256)   262400      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 27, 256)   1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 27, 256)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 27, 256)   590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 27, 256)   1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 27, 256)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 27, 1024)  263168      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4, 27, 1024)  0           add_35[0][0]                     \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 27, 1024)  4096        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 4, 27, 256)   262400      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 27, 256)   1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 27, 256)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 4, 27, 256)   590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 27, 256)   1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 27, 256)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 4, 27, 1024)  263168      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 4, 27, 1024)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 27, 1024)  4096        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 14, 512)   524800      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 2, 14, 512)   2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 14, 512)   0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 14, 512)   2359808     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 2, 14, 512)   2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 14, 512)   0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 14, 2048)  2099200     add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 2, 14, 2048)  0           conv2d_103[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 14, 2048)  8192        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 14, 512)   1049088     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 14, 512)   2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 14, 512)   0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 14, 512)   2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 14, 512)   2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 14, 512)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 2, 14, 2048)  0           add_38[0][0]                     \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 14, 2048)  8192        add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 14, 512)   1049088     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 14, 512)   2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 14, 512)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 14, 512)   2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 14, 512)   2048        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 14, 512)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 2, 14, 2048)  0           add_39[0][0]                     \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 14, 2048)  8192        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 2048)   0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 43)           88107       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 43)           172         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 43)           0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 41)           1804        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,656,163\n",
      "Trainable params: 23,610,637\n",
      "Non-trainable params: 45,526\n",
      "__________________________________________________________________________________________________\n",
      "BS: 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "510/510 [==============================] - 79s 155ms/step - loss: 1.1155 - acc: 0.8737 - val_loss: 0.8043 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85175, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 2/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0972 - acc: 0.8795 - val_loss: 0.8274 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.85175\n",
      "Epoch 3/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0876 - acc: 0.8815 - val_loss: 0.9236 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.85175\n",
      "Epoch 4/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0823 - acc: 0.8820 - val_loss: 0.7738 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85175 to 0.85175, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 5/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0839 - acc: 0.8810 - val_loss: 0.7718 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85175 to 0.86253, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 6/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0676 - acc: 0.8839 - val_loss: 0.8165 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86253\n",
      "Epoch 7/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0808 - acc: 0.8822 - val_loss: 0.7876 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86253\n",
      "Epoch 8/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0716 - acc: 0.8849 - val_loss: 0.7570 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86253\n",
      "Epoch 9/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0673 - acc: 0.8831 - val_loss: 0.7761 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86253\n",
      "Epoch 10/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0644 - acc: 0.8852 - val_loss: 0.8126 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86253\n",
      "Epoch 11/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0514 - acc: 0.8873 - val_loss: 0.7689 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.86253 to 0.87601, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 12/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0522 - acc: 0.8892 - val_loss: 0.7788 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87601\n",
      "Epoch 13/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0484 - acc: 0.8873 - val_loss: 0.7928 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87601\n",
      "Epoch 14/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0534 - acc: 0.8839 - val_loss: 0.8528 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87601\n",
      "Epoch 15/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0470 - acc: 0.8881 - val_loss: 0.7536 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87601\n",
      "Epoch 16/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0498 - acc: 0.8885 - val_loss: 0.7943 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87601\n",
      "Epoch 17/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0385 - acc: 0.8897 - val_loss: 0.7486 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87601\n",
      "Epoch 18/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0289 - acc: 0.8922 - val_loss: 0.7631 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87601\n",
      "Epoch 19/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0389 - acc: 0.8863 - val_loss: 0.7600 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87601\n",
      "Epoch 20/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0273 - acc: 0.8876 - val_loss: 0.7222 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87601\n",
      "Epoch 21/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0325 - acc: 0.8846 - val_loss: 0.7816 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.87601 to 0.87871, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 22/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0258 - acc: 0.8881 - val_loss: 0.7785 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87871\n",
      "Epoch 23/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0329 - acc: 0.8839 - val_loss: 0.7455 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87871\n",
      "Epoch 24/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0264 - acc: 0.8860 - val_loss: 0.7212 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87871\n",
      "Epoch 25/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0233 - acc: 0.8890 - val_loss: 0.7379 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.87871 to 0.88140, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 26/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0104 - acc: 0.8912 - val_loss: 0.7638 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88140\n",
      "Epoch 27/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0196 - acc: 0.8917 - val_loss: 0.7502 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88140\n",
      "Epoch 28/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0189 - acc: 0.8854 - val_loss: 0.7629 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88140\n",
      "Epoch 29/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0050 - acc: 0.8903 - val_loss: 0.7221 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88140\n",
      "Epoch 30/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0090 - acc: 0.8922 - val_loss: 0.7369 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88140\n",
      "Epoch 31/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0085 - acc: 0.8888 - val_loss: 0.7190 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88140\n",
      "Epoch 32/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0028 - acc: 0.8918 - val_loss: 0.7086 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88140\n",
      "Epoch 33/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0060 - acc: 0.8908 - val_loss: 0.7860 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88140\n",
      "Epoch 34/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9998 - acc: 0.8910 - val_loss: 0.7705 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88140\n",
      "Epoch 35/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 1.0040 - acc: 0.8923 - val_loss: 0.8071 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88140\n",
      "Epoch 36/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9936 - acc: 0.8930 - val_loss: 0.7448 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88140\n",
      "Epoch 37/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9968 - acc: 0.8867 - val_loss: 0.8050 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88140\n",
      "Epoch 38/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9951 - acc: 0.8918 - val_loss: 0.7714 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88140\n",
      "Epoch 39/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9864 - acc: 0.8926 - val_loss: 0.8213 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88140\n",
      "Epoch 40/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9913 - acc: 0.8907 - val_loss: 0.7145 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88140\n",
      "Epoch 41/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9886 - acc: 0.8901 - val_loss: 0.7679 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88140\n",
      "Epoch 42/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9996 - acc: 0.8896 - val_loss: 0.6928 - val_acc: 0.8625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88140\n",
      "Epoch 43/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9842 - acc: 0.8917 - val_loss: 0.7940 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88140\n",
      "Epoch 44/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9838 - acc: 0.8912 - val_loss: 0.7027 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88140\n",
      "Epoch 45/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9916 - acc: 0.8931 - val_loss: 0.7129 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88140 to 0.88679, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 46/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9882 - acc: 0.8869 - val_loss: 0.7848 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88679\n",
      "Epoch 47/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9892 - acc: 0.8918 - val_loss: 0.6931 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88679\n",
      "Epoch 48/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9821 - acc: 0.8971 - val_loss: 0.7951 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88679\n",
      "Epoch 49/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9769 - acc: 0.8956 - val_loss: 0.6673 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.88679 to 0.88949, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 50/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9766 - acc: 0.8947 - val_loss: 0.7289 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88949\n",
      "Epoch 51/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9759 - acc: 0.8930 - val_loss: 0.6623 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88949\n",
      "Epoch 52/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9844 - acc: 0.8912 - val_loss: 0.7139 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88949\n",
      "Epoch 53/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9782 - acc: 0.8956 - val_loss: 0.7387 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88949\n",
      "Epoch 54/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9718 - acc: 0.8921 - val_loss: 0.7787 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88949\n",
      "Epoch 55/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9787 - acc: 0.8965 - val_loss: 0.6985 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88949\n",
      "Epoch 56/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9729 - acc: 0.8906 - val_loss: 0.7273 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88949\n",
      "Epoch 57/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9642 - acc: 0.8933 - val_loss: 0.8334 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88949\n",
      "Epoch 58/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9772 - acc: 0.8922 - val_loss: 0.7573 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88949\n",
      "Epoch 59/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9661 - acc: 0.8925 - val_loss: 0.7772 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88949\n",
      "Epoch 60/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9767 - acc: 0.8950 - val_loss: 0.7976 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88949\n",
      "Epoch 61/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9684 - acc: 0.8904 - val_loss: 0.6879 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.88949 to 0.89488, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 62/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9629 - acc: 0.8942 - val_loss: 0.7549 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89488\n",
      "Epoch 63/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9661 - acc: 0.8936 - val_loss: 0.7079 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89488\n",
      "Epoch 64/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9594 - acc: 0.8983 - val_loss: 0.6568 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89488\n",
      "Epoch 65/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9707 - acc: 0.8952 - val_loss: 0.6605 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89488\n",
      "Epoch 66/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9614 - acc: 0.8952 - val_loss: 0.7032 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89488\n",
      "Epoch 67/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9558 - acc: 0.8967 - val_loss: 0.8080 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89488\n",
      "Epoch 68/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9715 - acc: 0.8913 - val_loss: 0.6584 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89488\n",
      "Epoch 69/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9561 - acc: 0.8924 - val_loss: 0.7028 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89488\n",
      "Epoch 70/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9564 - acc: 0.8943 - val_loss: 0.6502 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89488\n",
      "Epoch 71/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9520 - acc: 0.8938 - val_loss: 0.6698 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89488\n",
      "Epoch 72/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9635 - acc: 0.8928 - val_loss: 0.6846 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89488\n",
      "Epoch 73/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9534 - acc: 0.8960 - val_loss: 0.6918 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89488\n",
      "Epoch 74/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9579 - acc: 0.8955 - val_loss: 0.7286 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89488\n",
      "Epoch 75/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9605 - acc: 0.8949 - val_loss: 0.7328 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89488\n",
      "Epoch 76/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9525 - acc: 0.8952 - val_loss: 0.7351 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89488\n",
      "Epoch 77/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9520 - acc: 0.8991 - val_loss: 0.6288 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89488\n",
      "Epoch 78/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9567 - acc: 0.8936 - val_loss: 0.6802 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89488\n",
      "Epoch 79/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9244 - acc: 0.9031 - val_loss: 0.7182 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89488\n",
      "Epoch 80/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9175 - acc: 0.9037 - val_loss: 0.6453 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89488\n",
      "Epoch 81/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9069 - acc: 0.9077 - val_loss: 0.6373 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89488\n",
      "Epoch 82/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9025 - acc: 0.9025 - val_loss: 0.5945 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89488\n",
      "Epoch 83/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9028 - acc: 0.9031 - val_loss: 0.6234 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89488\n",
      "Epoch 84/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9047 - acc: 0.9042 - val_loss: 0.6129 - val_acc: 0.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_acc improved from 0.89488 to 0.90027, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 85/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9007 - acc: 0.9029 - val_loss: 0.6478 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90027\n",
      "Epoch 86/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8953 - acc: 0.9048 - val_loss: 0.6333 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90027\n",
      "Epoch 87/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9012 - acc: 0.9023 - val_loss: 0.6440 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90027\n",
      "Epoch 88/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8904 - acc: 0.9035 - val_loss: 0.6541 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90027\n",
      "Epoch 89/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8890 - acc: 0.9028 - val_loss: 0.6000 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.90027 to 0.90027, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 90/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.9007 - acc: 0.9025 - val_loss: 0.6110 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90027\n",
      "Epoch 91/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8949 - acc: 0.9021 - val_loss: 0.6177 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90027\n",
      "Epoch 92/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8915 - acc: 0.9034 - val_loss: 0.6084 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90027\n",
      "Epoch 93/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8854 - acc: 0.9051 - val_loss: 0.6262 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90027\n",
      "Epoch 94/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8854 - acc: 0.9053 - val_loss: 0.5874 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90027\n",
      "Epoch 95/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8881 - acc: 0.9074 - val_loss: 0.6177 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90027\n",
      "Epoch 96/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8844 - acc: 0.8980 - val_loss: 0.6191 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90027\n",
      "Epoch 97/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8865 - acc: 0.9010 - val_loss: 0.5920 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90027\n",
      "Epoch 98/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8806 - acc: 0.9050 - val_loss: 0.6020 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90027\n",
      "Epoch 99/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8806 - acc: 0.9093 - val_loss: 0.6315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90027\n",
      "Epoch 100/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8907 - acc: 0.9007 - val_loss: 0.6510 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90027\n",
      "Epoch 101/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8775 - acc: 0.9048 - val_loss: 0.6117 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90027\n",
      "Epoch 102/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8804 - acc: 0.9054 - val_loss: 0.6198 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90027\n",
      "Epoch 103/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8831 - acc: 0.9051 - val_loss: 0.6257 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90027\n",
      "Epoch 104/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8794 - acc: 0.9102 - val_loss: 0.6331 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90027\n",
      "Epoch 105/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8756 - acc: 0.9066 - val_loss: 0.5873 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90027\n",
      "Epoch 106/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8845 - acc: 0.9032 - val_loss: 0.6266 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90027\n",
      "Epoch 107/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8677 - acc: 0.9078 - val_loss: 0.5976 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.90027 to 0.90566, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 108/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8761 - acc: 0.9014 - val_loss: 0.6499 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90566\n",
      "Epoch 109/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8717 - acc: 0.9038 - val_loss: 0.5848 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90566\n",
      "Epoch 110/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8789 - acc: 0.9044 - val_loss: 0.6247 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90566\n",
      "Epoch 111/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8707 - acc: 0.9007 - val_loss: 0.6036 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90566\n",
      "Epoch 112/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8672 - acc: 0.9073 - val_loss: 0.6394 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90566\n",
      "Epoch 113/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8744 - acc: 0.9050 - val_loss: 0.6418 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90566\n",
      "Epoch 114/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8689 - acc: 0.9037 - val_loss: 0.6274 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90566\n",
      "Epoch 115/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8787 - acc: 0.9059 - val_loss: 0.6122 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90566\n",
      "Epoch 116/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8611 - acc: 0.9049 - val_loss: 0.6492 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90566\n",
      "Epoch 117/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8632 - acc: 0.9073 - val_loss: 0.6322 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90566\n",
      "Epoch 118/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8716 - acc: 0.9017 - val_loss: 0.5696 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90566\n",
      "Epoch 119/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8720 - acc: 0.9067 - val_loss: 0.5706 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90566\n",
      "Epoch 120/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8711 - acc: 0.9055 - val_loss: 0.6435 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90566\n",
      "Epoch 121/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8670 - acc: 0.9020 - val_loss: 0.6351 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90566\n",
      "Epoch 122/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8653 - acc: 0.9060 - val_loss: 0.6432 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90566\n",
      "Epoch 123/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8560 - acc: 0.9063 - val_loss: 0.5419 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90566\n",
      "Epoch 124/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8552 - acc: 0.9045 - val_loss: 0.5742 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90566\n",
      "Epoch 125/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8587 - acc: 0.9066 - val_loss: 0.5948 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90566\n",
      "Epoch 126/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8692 - acc: 0.9028 - val_loss: 0.6109 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90566\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8612 - acc: 0.9081 - val_loss: 0.6037 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90566\n",
      "Epoch 128/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8610 - acc: 0.9078 - val_loss: 0.5861 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90566\n",
      "Epoch 129/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8637 - acc: 0.9033 - val_loss: 0.5972 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90566\n",
      "Epoch 130/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8556 - acc: 0.9079 - val_loss: 0.5473 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00130: val_acc improved from 0.90566 to 0.91375, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 131/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8564 - acc: 0.9080 - val_loss: 0.5977 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91375\n",
      "Epoch 132/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8411 - acc: 0.9071 - val_loss: 0.5667 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91375\n",
      "Epoch 133/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8425 - acc: 0.9064 - val_loss: 0.5870 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91375\n",
      "Epoch 134/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8427 - acc: 0.9072 - val_loss: 0.5536 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91375\n",
      "Epoch 135/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8331 - acc: 0.9124 - val_loss: 0.5667 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91375\n",
      "Epoch 136/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8330 - acc: 0.9108 - val_loss: 0.5688 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91375\n",
      "Epoch 137/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8344 - acc: 0.9068 - val_loss: 0.5799 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91375\n",
      "Epoch 138/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8363 - acc: 0.9030 - val_loss: 0.5423 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91375\n",
      "Epoch 139/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8302 - acc: 0.9088 - val_loss: 0.5589 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91375\n",
      "Epoch 140/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8265 - acc: 0.9102 - val_loss: 0.5240 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91375\n",
      "Epoch 141/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8286 - acc: 0.9099 - val_loss: 0.5407 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91375\n",
      "Epoch 142/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8271 - acc: 0.9107 - val_loss: 0.5504 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91375\n",
      "Epoch 143/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8268 - acc: 0.9105 - val_loss: 0.5709 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91375\n",
      "Epoch 144/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8325 - acc: 0.9079 - val_loss: 0.5779 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91375\n",
      "Epoch 145/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8273 - acc: 0.9100 - val_loss: 0.5513 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91375\n",
      "Epoch 146/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8202 - acc: 0.9121 - val_loss: 0.5759 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91375\n",
      "Epoch 147/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8302 - acc: 0.9081 - val_loss: 0.5393 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91375\n",
      "Epoch 148/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8255 - acc: 0.9096 - val_loss: 0.6093 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91375\n",
      "Epoch 149/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8264 - acc: 0.9081 - val_loss: 0.6031 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91375\n",
      "Epoch 150/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8210 - acc: 0.9110 - val_loss: 0.5905 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91375\n",
      "Epoch 151/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8267 - acc: 0.9094 - val_loss: 0.5647 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.91375\n",
      "Epoch 152/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8147 - acc: 0.9093 - val_loss: 0.5770 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.91375\n",
      "Epoch 153/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8205 - acc: 0.9140 - val_loss: 0.5417 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.91375\n",
      "Epoch 154/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8163 - acc: 0.9105 - val_loss: 0.5200 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.91375\n",
      "Epoch 155/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8179 - acc: 0.9107 - val_loss: 0.5598 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.91375\n",
      "Epoch 156/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8253 - acc: 0.9080 - val_loss: 0.5965 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.91375\n",
      "Epoch 157/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8225 - acc: 0.9114 - val_loss: 0.5276 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.91375\n",
      "Epoch 158/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8256 - acc: 0.9100 - val_loss: 0.5830 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.91375\n",
      "Epoch 159/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8229 - acc: 0.9034 - val_loss: 0.5863 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.91375\n",
      "Epoch 160/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8129 - acc: 0.9094 - val_loss: 0.5748 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.91375\n",
      "Epoch 161/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8085 - acc: 0.9119 - val_loss: 0.5654 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.91375\n",
      "Epoch 162/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8146 - acc: 0.9116 - val_loss: 0.5777 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.91375\n",
      "Epoch 163/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8053 - acc: 0.9128 - val_loss: 0.5436 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00163: val_acc improved from 0.91375 to 0.91644, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 164/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8094 - acc: 0.9156 - val_loss: 0.5469 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.91644\n",
      "Epoch 165/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8039 - acc: 0.9096 - val_loss: 0.5715 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.91644\n",
      "Epoch 166/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8143 - acc: 0.9105 - val_loss: 0.5441 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.91644\n",
      "Epoch 167/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8070 - acc: 0.9083 - val_loss: 0.5398 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.91644\n",
      "Epoch 168/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8081 - acc: 0.9100 - val_loss: 0.5445 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.91644\n",
      "Epoch 169/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8040 - acc: 0.9108 - val_loss: 0.5226 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.91644\n",
      "Epoch 170/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8100 - acc: 0.9085 - val_loss: 0.5105 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.91644\n",
      "Epoch 171/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8074 - acc: 0.9108 - val_loss: 0.5280 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.91644\n",
      "Epoch 172/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8082 - acc: 0.9120 - val_loss: 0.5259 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.91644\n",
      "Epoch 173/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8020 - acc: 0.9129 - val_loss: 0.5184 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.91644\n",
      "Epoch 174/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8023 - acc: 0.9103 - val_loss: 0.5092 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.91644\n",
      "Epoch 175/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8045 - acc: 0.9139 - val_loss: 0.5194 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.91644\n",
      "Epoch 176/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8086 - acc: 0.9098 - val_loss: 0.5144 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.91644\n",
      "Epoch 177/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7969 - acc: 0.9110 - val_loss: 0.5180 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.91644\n",
      "Epoch 178/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8019 - acc: 0.9087 - val_loss: 0.5246 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.91644\n",
      "Epoch 179/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7957 - acc: 0.9155 - val_loss: 0.5178 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.91644\n",
      "Epoch 180/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7976 - acc: 0.9115 - val_loss: 0.5096 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.91644\n",
      "Epoch 181/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7969 - acc: 0.9154 - val_loss: 0.5216 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00181: val_acc improved from 0.91644 to 0.91914, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 182/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7964 - acc: 0.9126 - val_loss: 0.5165 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.91914\n",
      "Epoch 183/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7965 - acc: 0.9117 - val_loss: 0.5151 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.91914\n",
      "Epoch 184/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8016 - acc: 0.9113 - val_loss: 0.5181 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.91914\n",
      "Epoch 185/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8007 - acc: 0.9150 - val_loss: 0.5195 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.91914\n",
      "Epoch 186/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8012 - acc: 0.9161 - val_loss: 0.5227 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.91914\n",
      "Epoch 187/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7985 - acc: 0.9119 - val_loss: 0.5186 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.91914\n",
      "Epoch 188/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8018 - acc: 0.9093 - val_loss: 0.5078 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.91914\n",
      "Epoch 189/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.8005 - acc: 0.9085 - val_loss: 0.5162 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.91914\n",
      "Epoch 190/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7885 - acc: 0.9141 - val_loss: 0.5191 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.91914\n",
      "Epoch 191/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7949 - acc: 0.9140 - val_loss: 0.5263 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.91914\n",
      "Epoch 192/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7927 - acc: 0.9157 - val_loss: 0.5305 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.91914\n",
      "Epoch 193/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7983 - acc: 0.9109 - val_loss: 0.5224 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.91914\n",
      "Epoch 194/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7939 - acc: 0.9109 - val_loss: 0.5254 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.91914\n",
      "Epoch 195/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7977 - acc: 0.9156 - val_loss: 0.5291 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.91914\n",
      "Epoch 196/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7966 - acc: 0.9125 - val_loss: 0.5224 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.91914\n",
      "Epoch 197/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7963 - acc: 0.9119 - val_loss: 0.5172 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.91914\n",
      "Epoch 198/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7876 - acc: 0.9159 - val_loss: 0.5238 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.91914\n",
      "Epoch 199/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7978 - acc: 0.9138 - val_loss: 0.5235 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.91914\n",
      "Epoch 200/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7951 - acc: 0.9117 - val_loss: 0.5334 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.91914\n",
      "Epoch 201/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7934 - acc: 0.9093 - val_loss: 0.5282 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.91914\n",
      "Epoch 202/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7943 - acc: 0.9154 - val_loss: 0.5295 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.91914\n",
      "Epoch 203/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7831 - acc: 0.9147 - val_loss: 0.5338 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.91914\n",
      "Epoch 204/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7908 - acc: 0.9172 - val_loss: 0.5498 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.91914\n",
      "Epoch 205/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7855 - acc: 0.9137 - val_loss: 0.5356 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.91914\n",
      "Epoch 206/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7867 - acc: 0.9142 - val_loss: 0.5282 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.91914\n",
      "Epoch 207/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7881 - acc: 0.9164 - val_loss: 0.5402 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.91914\n",
      "Epoch 208/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7904 - acc: 0.9178 - val_loss: 0.5279 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.91914\n",
      "Epoch 209/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7890 - acc: 0.9139 - val_loss: 0.5247 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00209: val_acc improved from 0.91914 to 0.92453, saving model to model/mfcc6/LGD_fold9_self_resnet.h5\n",
      "Epoch 210/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7862 - acc: 0.9205 - val_loss: 0.5279 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.92453\n",
      "Epoch 211/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7900 - acc: 0.9159 - val_loss: 0.5290 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.92453\n",
      "Epoch 212/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7923 - acc: 0.9122 - val_loss: 0.5269 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.92453\n",
      "Epoch 213/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7884 - acc: 0.9112 - val_loss: 0.5213 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.92453\n",
      "Epoch 214/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7959 - acc: 0.9149 - val_loss: 0.5161 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.92453\n",
      "Epoch 215/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7858 - acc: 0.9157 - val_loss: 0.5282 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.92453\n",
      "Epoch 216/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7923 - acc: 0.9162 - val_loss: 0.5337 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.92453\n",
      "Epoch 217/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7955 - acc: 0.9097 - val_loss: 0.5381 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.92453\n",
      "Epoch 218/3000\n",
      "510/510 [==============================] - 73s 143ms/step - loss: 0.7929 - acc: 0.9130 - val_loss: 0.5297 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.92453\n",
      "Epoch 00218: early stopping\n"
     ]
    }
   ],
   "source": [
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "#     print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "#     model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(X_semi,Y_semi,fold)\n",
    "\n",
    "#MFCC6\n",
    "#0=>0.83288\n",
    "#1=>0.89757\n",
    "#2=>0.8625\n",
    "#3=>0.83288 (X)重頭train，重新load在tune\n",
    "#4=>0.89218\n",
    "#5=> 0.26146重頭train(X)設定大一點的lr\n",
    "#6=>0.88679\n",
    "#7=>0.89218\n",
    "#8=>0.8814 (patience太小)需要在fine tune\n",
    "#9=>0.92453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC7\n",
    "#0=> 0.8814\n",
    "#1=> 0.85714\n",
    "#2=> 0.87062\n",
    "#3=> 0.88949\n",
    "#4=> 0.82749\n",
    "#5=> 0.84906\n",
    "#6=> 0.85445\n",
    "#7=> 0.84906\n",
    "#8=> 0.84906\n",
    "#9=> 0.90566\n",
    "\n",
    "#====================\n",
    "#0=>0.84367(resnet3_semi)/0.83288(not semi)\n",
    "#1=>0.84367(resnet3_semi)/0.81132(not semi)\n",
    "#2=>0.84097(resnet2_semi)/0.81671(not semi)\n",
    "#3=>0.88410(resnet1_semi)/0.84367\n",
    "#4=>0.82210(resnet4_semi)/0.74663(not semi)\n",
    "#5 => 0.82749(resnet4_semi)/0.77628\n",
    "#6 => 0.85445(resnet2_semi)/0.78437\n",
    "#7 => 0.82749(resnet3_semi)/0.76280\n",
    "#8 => 0.83288(resnet3_semi)/0.78706\n",
    "# 9=> 0.90566(resnet1_semi)/0.87332\n",
    "\n",
    "# 0=>0.88140 (co)\n",
    "# 1 => 0.85714 (co)\n",
    "# 2=>0.87062 (co)\n",
    "# 3=>0.88949 (co)\n",
    "# 4=>0.82749 (co)\n",
    "# 5=>0.84367 (co)\n",
    "# 6=>0.84906(X) (semi)\n",
    "# 7=>0.84906 (co)\n",
    "# 8=>0.83019(X) (smi)\n",
    "# 9=>0.90566 (co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC6\n",
    "#0=> 0.81941\n",
    "#1=> 0.8841\n",
    "#2=> 0.83558\n",
    "#3=> 0.85984\n",
    "#4=> 0.84367\n",
    "#5=> 0.85175\n",
    "#6=> 0.86792\n",
    "#7=> 0.88949\n",
    "#8=> 0.8841\n",
    "#9=> 0.87332\n",
    "\n",
    "#===============\n",
    "# 0=> 0.81941 (not semi)\n",
    "# 1=>0.83019 (semi)\n",
    "# 2=>0.81941 (semi)\n",
    "# 3=>0.85984 (resnet1_not semi)、0.78437\n",
    "# 4=>0.84367 (renet1_not semi)、0.81132\n",
    "# 5=> 0.85175(resnet3_semi)、0.82749(not semi)\n",
    "#6 => 0.85904(resnet4_semi)、0.79784(not semi)\n",
    "# 7=>0.88949(resnet2_semi)、0.83558(not semi)\n",
    "# 8=>0.83558(resnet1 not semi)\n",
    "# 9=>0.86792(resnet2_semi)、0.8112(not semi)\n",
    "\n",
    "# 0 => 0.81941\n",
    "#1=>0.88410\n",
    "#2=>0.83558\n",
    "# 3=>0.84367 (X)\n",
    "# 4+>0.86523 (X)\n",
    "# 5=> (X) (前五個fold街從頭train)\n",
    "\n",
    "# 3=>0.85445 (x)\n",
    "# 4=>0.82110 (x)\n",
    "# 5=>0.84636 (X)\n",
    "# 6=>0.86792\n",
    "# 7=>0.88949\n",
    "# 8=>0.88410\n",
    "# 9=>0.87332"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
