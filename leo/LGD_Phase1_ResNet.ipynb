{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from collections import Counter\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "# import pandas as pd\n",
    "import glob\n",
    "from sklearn.semi_supervised import *\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import pandas as pd # data frame\n",
    "import numpy as np # matrix math\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "import matplotlib.pyplot as plt # to view graphs\n",
    "import wave\n",
    "from math import log, floor\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import KMeans\n",
    "import sys, os\n",
    "import random,math\n",
    "from tqdm import tqdm ##\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "from keras.utils import *\n",
    "from sklearn.ensemble import *\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "from logHandler import Logger\n",
    "from utils import readCSV, getPath, writePickle,readPickle\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import resnet\n",
    "from random_eraser import get_random_eraser\n",
    "from mixup_generator import MixupGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def normalize(X_train, X_val):\n",
    "    X_train = (X_train - mean)/(std)\n",
    "#     X_train = (X_train - min_)/range_\n",
    "    X_val = (X_val - mean)/(std)\n",
    "#     X_val = (X_val - min_)/range_\n",
    "\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# min_ = np.swapaxes(min_,0,1)\n",
    "# mean = np.swapaxes(mean,0,1)\n",
    "# range_ = np.swapaxes(range_,0,1)\n",
    "# std = np.swapaxes(std,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_valid(X_train,Y_train,X_valid,Y_valid,fold):\n",
    "    model = [resnet.ResnetBuilder.build_resnet_18((1, X_train.shape[1], X_train.shape[2]), 41),\n",
    "             resnet.ResnetBuilder.build_resnet_34((1,X_train.shape[1], X_train.shape[2]), 41),\n",
    "             resnet.ResnetBuilder.build_resnet_50((1, X_train.shape[1], X_train.shape[2]), 41) ,]\n",
    "#              resnet.ResnetBuilder.build_resnet_101((1, X_train.shape[1], X_train.shape[2]), 41),]\n",
    "#              resnet.ResnetBuilder.build_resnet_152((1, X_train.shape[1], X_train.shape[2]), 41)]\n",
    "    kk = random.randint(0, 2)#4)\n",
    "    model = model[kk]\n",
    "    print('using resnet model: '+str(kk))\n",
    "\n",
    "    model.summary()\n",
    "    if kk>=3:\n",
    "        batchSize=[32]\n",
    "    elif kk>2:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif kk>=1:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64,128,256]\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=100\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    if not os.path.exists(saveD):\n",
    "        os.makedirs(saveD)\n",
    "    opt = Adam()#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        width_shift_range=0.1*random.random(),\n",
    "        height_shift_range=0.1*random.random(),\n",
    "        shear_range=0.05*random.random(),\n",
    "        preprocessing_function=get_random_eraser(p=0.82+0.13*random.random(),v_l=np.min(X_train), v_h=np.max(X_train)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_train)\n",
    "    generator = MixupGenerator(X_train, Y_train, alpha=1, \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    print('using resnet model: '+str(kk))\n",
    "    if not os.path.exists(logD):\n",
    "        os.makedirs(logD)\n",
    "    history = History()\n",
    "    callback=[\n",
    "    #     ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/2),\n",
    "    #                                   min_lr=1e-4,mode='min', cooldown=1 ),\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_resnet'+str(kk)+'-.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_resnet'+str(kk)),\n",
    "        history#,batch_size=batch_size, write_graph=True, write_grads=False, write_images=True)\n",
    "    ]\n",
    "\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=X_train.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch\n",
    "                       )\n",
    "    return model,kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semi_data(X_train,Y_train):\n",
    "    X_un_ver = np.load('feature/'+feature_type+'/semi/fbank4/X_un_ver.npy')\n",
    "    X_test_ver = np.load('feature/'+feature_type+'/semi/fbank4/X_test_ver.npy')\n",
    "    X = np.concatenate((X_un_ver,X_test_ver))\n",
    "    Y_un_ver = np.load('feature/'+feature_type+'/semi/fbank4/Y_un_ver.npy')\n",
    "    Y_test_ver = np.load('feature/'+feature_type+'/semi/fbank4/Y_test_ver.npy')\n",
    "    Y = np.concatenate((Y_un_ver,Y_test_ver))\n",
    "    Y = to_categorical(Y,num_classes=41)\n",
    "    X_semi = np.concatenate((X_train,X))\n",
    "    Y_semi = np.concatenate((Y_train,Y))\n",
    "    X_semi , Y_semi = _shuffle(X_semi,Y_semi)\n",
    "    print(X_semi.shape , Y_semi.shape)\n",
    "    return X_semi , Y_semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = resnet.ResnetBuilder.build_resnet_50((1, 64, 431), 41)\n",
    "# model.summary()\n",
    "def train_unverified(model,X_semi,Y_semi,fold,kk):\n",
    "    name = glob.glob('model/'+feature_type+'/'+'LGD_fold'+str(fold)+'_resnet'+str(kk)+'-**')[0]\n",
    "    print('semi loading: '+ name)\n",
    "    model = load_model(name)\n",
    "    if kk==4:\n",
    "        batchSize=[32]\n",
    "    elif kk>=2:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif kk>=1:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64,128,256]\n",
    "#     batchSize=[32,64,128,256] ##ERR?\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=100\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    opt = Adam(lr=0.0001,decay=1e-6)#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "#         featurewise_std_normalization=True,\n",
    "        width_shift_range=0.05+0.15*random.random(),\n",
    "        height_shift_range=0.05+0.15*random.random(),\n",
    "        shear_range=0.084375+0.253125*random.random(),\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(X_semi), v_h=np.max(X_semi)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_semi)\n",
    "    test_datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True)\n",
    "    generator = MixupGenerator(X_semi, Y_semi, alpha=0.4+0.6*random.random(), \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "    \n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    history = History()\n",
    "    callback=[\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/10),min_lr=4e-6,\n",
    "                          mode='min', cooldown=1,verbose=1 ), #0.2,/25\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_semi_resnet'+str(kk)+'.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,\n",
    "                        mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_semi_resnet'+str(kk)),\n",
    "        history\n",
    "    ]\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=X_semi.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch,\n",
    "                        initial_epoch = int(patien/25)\n",
    "                       )\n",
    "#     model.fit(X_semi,Y_semi,\n",
    "#               shuffle=True,\n",
    "#               callbacks=callback, \n",
    "#               class_weight='auto',\n",
    "#               validation_data=(X_valid,Y_valid),\n",
    "#               batch_size=batchSize,\n",
    "#               epochs=epoch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'mfcc7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized parameters\n",
    "mean = np.load('feature/'+feature_type+'/mean.npy')\n",
    "std = np.load('feature/'+feature_type+'/std.npy')\n",
    "min_ = np.load('feature/'+feature_type+'/min.npy')\n",
    "range_ = np.load('feature/'+feature_type+'/range.npy')\n",
    "\n",
    "\n",
    "base_path = 'feature/'+feature_type+'/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/'+feature_type+'/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10\n",
    "\n",
    "val_set_num = [7,8,9]#str(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold6_mfcc7===\n",
      "using resnet model: 2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 30, 130, 64)  3200        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 30, 130, 64)  256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 30, 130, 64)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 15, 65, 64)   0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 15, 65, 64)   4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 15, 65, 64)   256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 15, 65, 64)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 15, 65, 64)   36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 15, 65, 64)   256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 15, 65, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 15, 65, 256)  16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 15, 65, 256)  16640       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 15, 65, 256)  0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 15, 65, 256)  1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 15, 65, 256)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 15, 65, 64)   16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 15, 65, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 15, 65, 64)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 15, 65, 64)   36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 15, 65, 64)   256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 15, 65, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 15, 65, 256)  16640       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 15, 65, 256)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 15, 65, 256)  1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 15, 65, 256)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 15, 65, 64)   16448       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 15, 65, 64)   256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 15, 65, 64)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 15, 65, 64)   36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 15, 65, 64)   256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 15, 65, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 15, 65, 256)  16640       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 15, 65, 256)  0           add_26[0][0]                     \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 15, 65, 256)  1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 15, 65, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 33, 128)   32896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 33, 128)   512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 33, 128)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 33, 128)   147584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 33, 128)   512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 33, 128)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 33, 512)   131584      add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 33, 512)   66048       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 33, 512)   0           conv2d_71[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 33, 512)   2048        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 33, 512)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 33, 128)   65664       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 33, 128)   512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 33, 128)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 33, 128)   147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 33, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 33, 128)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 33, 512)   66048       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 33, 512)   0           add_28[0][0]                     \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 33, 512)   2048        add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 33, 512)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 33, 128)   65664       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 33, 128)   512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 33, 128)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 33, 128)   147584      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 33, 128)   512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 33, 128)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 33, 512)   66048       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 33, 512)   0           add_29[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 33, 512)   2048        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 33, 512)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 33, 128)   65664       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 33, 128)   512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 33, 128)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 33, 128)   147584      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 33, 128)   512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 33, 128)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 33, 512)   66048       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 33, 512)   0           add_30[0][0]                     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 33, 512)   2048        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 33, 512)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 17, 256)   131328      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 17, 256)   1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 17, 256)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 17, 256)   590080      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 17, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 17, 256)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 17, 1024)  525312      add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 17, 1024)  263168      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 4, 17, 1024)  0           conv2d_84[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 17, 1024)  4096        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 17, 256)   262400      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 17, 256)   1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 17, 256)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 17, 256)   590080      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 17, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 17, 256)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 17, 1024)  263168      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 17, 1024)  0           add_32[0][0]                     \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 17, 1024)  4096        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 17, 256)   262400      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 17, 256)   1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 17, 256)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 17, 256)   590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 17, 256)   1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 17, 256)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 17, 1024)  263168      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 17, 1024)  0           add_33[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 17, 1024)  4096        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 17, 256)   262400      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 17, 256)   1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 17, 256)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 17, 256)   590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 17, 256)   1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 17, 256)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 17, 1024)  263168      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 17, 1024)  0           add_34[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 17, 1024)  4096        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 17, 256)   262400      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 17, 256)   1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 17, 256)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 17, 256)   590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 17, 256)   1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 17, 256)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 17, 1024)  263168      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4, 17, 1024)  0           add_35[0][0]                     \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 17, 1024)  4096        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 4, 17, 256)   262400      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 17, 256)   1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 17, 256)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 4, 17, 256)   590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 17, 256)   1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 17, 256)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 4, 17, 1024)  263168      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 4, 17, 1024)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 17, 1024)  4096        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 17, 1024)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 9, 512)    524800      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 2, 9, 512)    2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 9, 512)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 9, 512)    2359808     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 2, 9, 512)    2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 9, 512)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 9, 2048)   2099200     add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 2, 9, 2048)   0           conv2d_103[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 9, 2048)   8192        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 9, 512)    1049088     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 9, 512)    2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 9, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 9, 512)    2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 9, 512)    2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 9, 512)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 2, 9, 2048)   0           add_38[0][0]                     \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 9, 2048)   8192        add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 9, 512)    1049088     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 9, 512)    2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 9, 512)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 9, 512)    2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 9, 512)    2048        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 9, 512)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 2, 9, 2048)   0           add_39[0][0]                     \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 9, 2048)   8192        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 9, 2048)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 2048)   0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 51)           104499      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 51)           204         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 51)           0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 41)           2132        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,672,915\n",
      "Trainable params: 23,627,373\n",
      "Non-trainable params: 45,542\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "52/52 [==============================] - 16s 302ms/step - loss: 9.0078 - acc: 0.0604 - val_loss: 8.8012 - val_acc: 0.0647\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.06469, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 2/3000\n",
      "52/52 [==============================] - 9s 165ms/step - loss: 7.7923 - acc: 0.0739 - val_loss: 7.1946 - val_acc: 0.0997\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.06469 to 0.09973, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 3/3000\n",
      "52/52 [==============================] - 9s 165ms/step - loss: 6.8076 - acc: 0.0995 - val_loss: 6.8510 - val_acc: 0.0782\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.09973\n",
      "Epoch 4/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 6.0597 - acc: 0.1187 - val_loss: 6.1507 - val_acc: 0.0485\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.09973\n",
      "Epoch 5/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 5.5071 - acc: 0.1385 - val_loss: 5.0723 - val_acc: 0.1725\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.09973 to 0.17251, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 6/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 5.0812 - acc: 0.1611 - val_loss: 5.1313 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.17251\n",
      "Epoch 7/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.7786 - acc: 0.1782 - val_loss: 4.7712 - val_acc: 0.1402\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.17251\n",
      "Epoch 8/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.4948 - acc: 0.2025 - val_loss: 4.4422 - val_acc: 0.1617\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.17251\n",
      "Epoch 9/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.3373 - acc: 0.1926 - val_loss: 4.0359 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.17251 to 0.22102, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 10/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.1431 - acc: 0.2148 - val_loss: 4.6654 - val_acc: 0.1914\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.22102\n",
      "Epoch 11/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 4.0411 - acc: 0.2248 - val_loss: 3.5581 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.22102 to 0.28571, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 12/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.8676 - acc: 0.2566 - val_loss: 3.3080 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.28571 to 0.33154, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 13/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.8171 - acc: 0.2530 - val_loss: 3.8054 - val_acc: 0.2237\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.33154\n",
      "Epoch 14/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.6785 - acc: 0.2629 - val_loss: 3.2006 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.33154\n",
      "Epoch 15/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.5947 - acc: 0.2803 - val_loss: 3.1565 - val_acc: 0.2884\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.33154\n",
      "Epoch 16/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.5340 - acc: 0.2885 - val_loss: 3.4855 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.33154\n",
      "Epoch 17/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.5057 - acc: 0.2891 - val_loss: 3.0113 - val_acc: 0.3073\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.33154\n",
      "Epoch 18/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.4127 - acc: 0.2990 - val_loss: 3.7160 - val_acc: 0.2264\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.33154\n",
      "Epoch 19/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.3926 - acc: 0.3008 - val_loss: 3.2824 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.33154\n",
      "Epoch 20/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.2901 - acc: 0.3206 - val_loss: 2.9585 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.33154\n",
      "Epoch 21/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.2705 - acc: 0.3173 - val_loss: 2.9076 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.33154\n",
      "Epoch 22/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.2673 - acc: 0.3176 - val_loss: 2.8678 - val_acc: 0.3585\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.33154 to 0.35849, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 23/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1713 - acc: 0.3468 - val_loss: 2.7876 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.35849\n",
      "Epoch 24/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1698 - acc: 0.3534 - val_loss: 2.9918 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.35849\n",
      "Epoch 25/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.1316 - acc: 0.3588 - val_loss: 2.4975 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.35849 to 0.42318, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 26/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.1095 - acc: 0.3477 - val_loss: 2.5587 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.42318\n",
      "Epoch 27/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 3.0763 - acc: 0.3660 - val_loss: 2.4178 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.42318 to 0.46092, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 28/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.0530 - acc: 0.3840 - val_loss: 3.0256 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.46092\n",
      "Epoch 29/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 3.0075 - acc: 0.3864 - val_loss: 2.5218 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.46092\n",
      "Epoch 30/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9867 - acc: 0.3873 - val_loss: 2.7858 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.46092\n",
      "Epoch 31/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9579 - acc: 0.4075 - val_loss: 2.4868 - val_acc: 0.4043\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.46092\n",
      "Epoch 32/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9484 - acc: 0.4099 - val_loss: 2.3469 - val_acc: 0.4582\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.46092\n",
      "Epoch 33/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9649 - acc: 0.3861 - val_loss: 2.7358 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.46092\n",
      "Epoch 34/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9280 - acc: 0.3984 - val_loss: 2.3958 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.46092\n",
      "Epoch 35/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.9139 - acc: 0.4084 - val_loss: 2.6968 - val_acc: 0.3208\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.46092\n",
      "Epoch 36/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8619 - acc: 0.4366 - val_loss: 2.2023 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.46092 to 0.47709, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 37/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8699 - acc: 0.4252 - val_loss: 2.1951 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.47709 to 0.48248, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 38/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8501 - acc: 0.4453 - val_loss: 3.1784 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.48248\n",
      "Epoch 39/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8751 - acc: 0.4288 - val_loss: 3.1952 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.48248\n",
      "Epoch 40/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8411 - acc: 0.4348 - val_loss: 2.6161 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.48248\n",
      "Epoch 41/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.8229 - acc: 0.4414 - val_loss: 2.5088 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.48248\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7563 - acc: 0.4567 - val_loss: 2.3626 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.48248\n",
      "Epoch 43/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7926 - acc: 0.4555 - val_loss: 2.2227 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.48248\n",
      "Epoch 44/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7858 - acc: 0.4444 - val_loss: 2.5051 - val_acc: 0.3747\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.48248\n",
      "Epoch 45/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7206 - acc: 0.4627 - val_loss: 2.0551 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.48248 to 0.52291, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 46/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7352 - acc: 0.4769 - val_loss: 2.2166 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.52291\n",
      "Epoch 47/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7277 - acc: 0.4675 - val_loss: 2.2076 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.52291\n",
      "Epoch 48/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7058 - acc: 0.4901 - val_loss: 2.1263 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.52291\n",
      "Epoch 49/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6871 - acc: 0.4814 - val_loss: 2.2381 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.52291\n",
      "Epoch 50/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6934 - acc: 0.4766 - val_loss: 2.4774 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.52291\n",
      "Epoch 51/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6935 - acc: 0.4688 - val_loss: 2.0119 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.52291\n",
      "Epoch 52/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.7077 - acc: 0.4700 - val_loss: 2.1969 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.52291\n",
      "Epoch 53/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6741 - acc: 0.4787 - val_loss: 2.0153 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.52291\n",
      "Epoch 54/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6455 - acc: 0.4937 - val_loss: 1.9388 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.52291 to 0.53908, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 55/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.6448 - acc: 0.4970 - val_loss: 2.4240 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.53908\n",
      "Epoch 56/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6161 - acc: 0.5057 - val_loss: 2.1865 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.53908\n",
      "Epoch 57/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.6176 - acc: 0.5117 - val_loss: 1.9232 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.53908\n",
      "Epoch 58/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6244 - acc: 0.5117 - val_loss: 2.3630 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.53908\n",
      "Epoch 59/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6506 - acc: 0.4967 - val_loss: 2.0702 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.53908\n",
      "Epoch 60/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6286 - acc: 0.5093 - val_loss: 1.9066 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.53908\n",
      "Epoch 61/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5787 - acc: 0.5156 - val_loss: 2.1767 - val_acc: 0.4744\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.53908\n",
      "Epoch 62/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.5913 - acc: 0.5081 - val_loss: 2.5118 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.53908\n",
      "Epoch 63/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.6107 - acc: 0.5015 - val_loss: 2.7139 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.53908\n",
      "Epoch 64/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5745 - acc: 0.5207 - val_loss: 1.8559 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.53908 to 0.53908, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 65/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5921 - acc: 0.5099 - val_loss: 1.9643 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.53908\n",
      "Epoch 66/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5575 - acc: 0.5228 - val_loss: 1.7995 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.53908 to 0.57951, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 67/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5362 - acc: 0.5388 - val_loss: 2.1406 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.57951\n",
      "Epoch 68/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5347 - acc: 0.5394 - val_loss: 1.9245 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.57951\n",
      "Epoch 69/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5274 - acc: 0.5288 - val_loss: 2.1583 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.57951\n",
      "Epoch 70/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.5453 - acc: 0.5328 - val_loss: 2.0253 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.57951\n",
      "Epoch 71/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4910 - acc: 0.5508 - val_loss: 2.1252 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.57951\n",
      "Epoch 72/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5504 - acc: 0.5319 - val_loss: 2.0971 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.57951\n",
      "Epoch 73/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5318 - acc: 0.5367 - val_loss: 1.9899 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.57951\n",
      "Epoch 74/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4989 - acc: 0.5532 - val_loss: 1.6394 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.57951 to 0.61456, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 75/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4842 - acc: 0.5487 - val_loss: 1.7656 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.61456\n",
      "Epoch 76/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5371 - acc: 0.5291 - val_loss: 1.7239 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.61456\n",
      "Epoch 77/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5124 - acc: 0.5361 - val_loss: 1.7698 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.61456\n",
      "Epoch 78/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.5017 - acc: 0.5487 - val_loss: 1.9586 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.61456\n",
      "Epoch 79/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.4712 - acc: 0.5529 - val_loss: 1.6697 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.61456\n",
      "Epoch 80/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.4605 - acc: 0.5523 - val_loss: 1.5836 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.61456 to 0.61725, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 81/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4961 - acc: 0.5547 - val_loss: 1.7742 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.61725\n",
      "Epoch 82/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4774 - acc: 0.5586 - val_loss: 1.6207 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.61725 to 0.61995, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 83/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4501 - acc: 0.5571 - val_loss: 1.6488 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.61995\n",
      "Epoch 84/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4187 - acc: 0.5691 - val_loss: 1.6502 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.61995 to 0.63342, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 85/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4170 - acc: 0.5673 - val_loss: 1.7660 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.63342\n",
      "Epoch 86/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4438 - acc: 0.5634 - val_loss: 1.8612 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.63342\n",
      "Epoch 87/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4751 - acc: 0.5493 - val_loss: 2.3727 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.63342\n",
      "Epoch 88/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4273 - acc: 0.5628 - val_loss: 1.9449 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.63342\n",
      "Epoch 89/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3896 - acc: 0.5862 - val_loss: 1.7312 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.63342\n",
      "Epoch 90/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3998 - acc: 0.5679 - val_loss: 1.7836 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.63342\n",
      "Epoch 91/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4281 - acc: 0.5748 - val_loss: 2.1083 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.63342\n",
      "Epoch 92/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4173 - acc: 0.5613 - val_loss: 1.6336 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.63342\n",
      "Epoch 93/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3887 - acc: 0.5904 - val_loss: 1.7706 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.63342\n",
      "Epoch 94/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4042 - acc: 0.5772 - val_loss: 1.6930 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.63342\n",
      "Epoch 95/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4080 - acc: 0.5799 - val_loss: 1.9109 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.63342\n",
      "Epoch 96/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3989 - acc: 0.5838 - val_loss: 1.8001 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.63342\n",
      "Epoch 97/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.4197 - acc: 0.5679 - val_loss: 1.8148 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.63342\n",
      "Epoch 98/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4006 - acc: 0.5688 - val_loss: 1.8318 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.63342\n",
      "Epoch 99/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.3876 - acc: 0.5847 - val_loss: 2.2235 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.63342\n",
      "Epoch 100/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.4130 - acc: 0.5947 - val_loss: 1.9869 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.63342\n",
      "Epoch 101/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3758 - acc: 0.5938 - val_loss: 1.7337 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.63342\n",
      "Epoch 102/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3630 - acc: 0.5959 - val_loss: 1.7142 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.63342\n",
      "Epoch 103/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3577 - acc: 0.6010 - val_loss: 2.3889 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.63342\n",
      "Epoch 104/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3719 - acc: 0.5877 - val_loss: 2.0028 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.63342\n",
      "Epoch 105/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3756 - acc: 0.5944 - val_loss: 2.2543 - val_acc: 0.4555\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.63342\n",
      "Epoch 106/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3569 - acc: 0.5998 - val_loss: 1.7596 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.63342\n",
      "Epoch 107/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3370 - acc: 0.5826 - val_loss: 1.5411 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.63342 to 0.63342, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 108/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3126 - acc: 0.6187 - val_loss: 1.6769 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.63342\n",
      "Epoch 109/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3468 - acc: 0.5919 - val_loss: 1.6184 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.63342\n",
      "Epoch 110/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3255 - acc: 0.6076 - val_loss: 2.6025 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.63342\n",
      "Epoch 111/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3442 - acc: 0.5971 - val_loss: 2.0368 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.63342\n",
      "Epoch 112/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3252 - acc: 0.6055 - val_loss: 1.6933 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.63342\n",
      "Epoch 113/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3456 - acc: 0.6010 - val_loss: 1.8006 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.63342\n",
      "Epoch 114/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3243 - acc: 0.6169 - val_loss: 1.5640 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.63342\n",
      "Epoch 115/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3475 - acc: 0.6001 - val_loss: 1.6311 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.63342\n",
      "Epoch 116/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2903 - acc: 0.6190 - val_loss: 2.2205 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.63342\n",
      "Epoch 117/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3082 - acc: 0.6118 - val_loss: 1.5214 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00117: val_acc improved from 0.63342 to 0.64420, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 118/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3326 - acc: 0.5998 - val_loss: 1.7747 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.64420\n",
      "Epoch 119/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.2695 - acc: 0.6262 - val_loss: 1.8849 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.64420\n",
      "Epoch 120/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2977 - acc: 0.6151 - val_loss: 1.5967 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00120: val_acc improved from 0.64420 to 0.65229, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 121/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3139 - acc: 0.6064 - val_loss: 1.7825 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.65229\n",
      "Epoch 122/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2890 - acc: 0.6196 - val_loss: 1.9102 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.65229\n",
      "Epoch 123/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2794 - acc: 0.6343 - val_loss: 1.7520 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.65229\n",
      "Epoch 124/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2883 - acc: 0.6133 - val_loss: 1.7666 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.65229\n",
      "Epoch 125/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.3234 - acc: 0.6013 - val_loss: 1.5330 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.65229\n",
      "Epoch 126/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2843 - acc: 0.6283 - val_loss: 1.8783 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.65229\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2800 - acc: 0.6181 - val_loss: 1.5761 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.65229\n",
      "Epoch 128/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3150 - acc: 0.6142 - val_loss: 1.4272 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.65229 to 0.67655, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 129/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.3006 - acc: 0.6253 - val_loss: 2.2490 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.67655\n",
      "Epoch 130/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2788 - acc: 0.6196 - val_loss: 1.5255 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00130: val_acc improved from 0.67655 to 0.67925, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 131/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2993 - acc: 0.6247 - val_loss: 2.0299 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.67925\n",
      "Epoch 132/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.2719 - acc: 0.6232 - val_loss: 1.8330 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.67925\n",
      "Epoch 133/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2984 - acc: 0.6262 - val_loss: 2.3189 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.67925\n",
      "Epoch 134/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2864 - acc: 0.6214 - val_loss: 1.6195 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.67925\n",
      "Epoch 135/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2325 - acc: 0.6487 - val_loss: 1.9097 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.67925\n",
      "Epoch 136/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2592 - acc: 0.6298 - val_loss: 2.1827 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.67925\n",
      "Epoch 137/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2475 - acc: 0.6328 - val_loss: 1.4780 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.67925\n",
      "Epoch 138/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2445 - acc: 0.6352 - val_loss: 1.7472 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.67925\n",
      "Epoch 139/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2161 - acc: 0.6457 - val_loss: 1.4732 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00139: val_acc improved from 0.67925 to 0.69811, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 140/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2352 - acc: 0.6334 - val_loss: 2.6811 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.69811\n",
      "Epoch 141/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2137 - acc: 0.6529 - val_loss: 1.7002 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.69811\n",
      "Epoch 142/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.2374 - acc: 0.6454 - val_loss: 1.4700 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.69811\n",
      "Epoch 143/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2270 - acc: 0.6487 - val_loss: 1.5221 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.69811\n",
      "Epoch 144/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2415 - acc: 0.6340 - val_loss: 1.5359 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.69811\n",
      "Epoch 145/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2164 - acc: 0.6406 - val_loss: 1.4921 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.69811\n",
      "Epoch 146/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2294 - acc: 0.6484 - val_loss: 1.3070 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00146: val_acc improved from 0.69811 to 0.71429, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 147/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2073 - acc: 0.6523 - val_loss: 1.5837 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.71429\n",
      "Epoch 148/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2273 - acc: 0.6328 - val_loss: 1.5851 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.71429\n",
      "Epoch 149/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2226 - acc: 0.6313 - val_loss: 1.8332 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.71429\n",
      "Epoch 150/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2020 - acc: 0.6502 - val_loss: 1.8781 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.71429\n",
      "Epoch 151/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2270 - acc: 0.6421 - val_loss: 1.8926 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.71429\n",
      "Epoch 152/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1937 - acc: 0.6538 - val_loss: 1.4920 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.71429\n",
      "Epoch 153/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2190 - acc: 0.6433 - val_loss: 1.4252 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.71429\n",
      "Epoch 154/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1719 - acc: 0.6734 - val_loss: 1.7099 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.71429\n",
      "Epoch 155/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2116 - acc: 0.6424 - val_loss: 1.5435 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.71429\n",
      "Epoch 156/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2036 - acc: 0.6544 - val_loss: 1.8498 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.71429\n",
      "Epoch 157/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1871 - acc: 0.6544 - val_loss: 1.7439 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.71429\n",
      "Epoch 158/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1813 - acc: 0.6466 - val_loss: 1.6299 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.71429\n",
      "Epoch 159/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1743 - acc: 0.6641 - val_loss: 2.0368 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.71429\n",
      "Epoch 160/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1658 - acc: 0.6532 - val_loss: 1.4182 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.71429\n",
      "Epoch 161/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1776 - acc: 0.6617 - val_loss: 1.2761 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00161: val_acc improved from 0.71429 to 0.73854, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 162/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.2001 - acc: 0.6623 - val_loss: 1.6829 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.73854\n",
      "Epoch 163/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1867 - acc: 0.6490 - val_loss: 1.6841 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.73854\n",
      "Epoch 164/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1782 - acc: 0.6608 - val_loss: 1.5158 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.73854\n",
      "Epoch 165/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1578 - acc: 0.6638 - val_loss: 1.5450 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.73854\n",
      "Epoch 166/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1941 - acc: 0.6463 - val_loss: 1.6504 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.73854\n",
      "Epoch 167/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1662 - acc: 0.6725 - val_loss: 1.5405 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.73854\n",
      "Epoch 168/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1472 - acc: 0.6830 - val_loss: 2.1457 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.73854\n",
      "Epoch 169/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1699 - acc: 0.6698 - val_loss: 1.4664 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.73854\n",
      "Epoch 170/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1680 - acc: 0.6731 - val_loss: 1.6261 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.73854\n",
      "Epoch 171/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1360 - acc: 0.6863 - val_loss: 1.4699 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.73854\n",
      "Epoch 172/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.1501 - acc: 0.6728 - val_loss: 1.6077 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.73854\n",
      "Epoch 173/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1486 - acc: 0.6695 - val_loss: 1.5463 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.73854\n",
      "Epoch 174/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1287 - acc: 0.6698 - val_loss: 1.4311 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.73854\n",
      "Epoch 175/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1964 - acc: 0.6641 - val_loss: 1.4057 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.73854\n",
      "Epoch 176/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1522 - acc: 0.6644 - val_loss: 1.4480 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.73854\n",
      "Epoch 177/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1193 - acc: 0.6926 - val_loss: 1.6824 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.73854\n",
      "Epoch 178/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1355 - acc: 0.6590 - val_loss: 1.5295 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.73854\n",
      "Epoch 179/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1325 - acc: 0.6836 - val_loss: 1.6720 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.73854\n",
      "Epoch 180/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1449 - acc: 0.6707 - val_loss: 1.5943 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.73854\n",
      "Epoch 181/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1488 - acc: 0.6674 - val_loss: 1.3472 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.73854\n",
      "Epoch 182/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1423 - acc: 0.6713 - val_loss: 1.7502 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.73854\n",
      "Epoch 183/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1157 - acc: 0.6803 - val_loss: 1.7693 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.73854\n",
      "Epoch 184/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1416 - acc: 0.6713 - val_loss: 1.5800 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.73854\n",
      "Epoch 185/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1131 - acc: 0.6860 - val_loss: 1.4801 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.73854\n",
      "Epoch 186/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0909 - acc: 0.6998 - val_loss: 1.9365 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.73854\n",
      "Epoch 187/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1417 - acc: 0.6797 - val_loss: 1.3623 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.73854\n",
      "Epoch 188/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1330 - acc: 0.6698 - val_loss: 1.9821 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.73854\n",
      "Epoch 189/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.1625 - acc: 0.6689 - val_loss: 1.5488 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.73854\n",
      "Epoch 190/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0999 - acc: 0.6911 - val_loss: 1.6300 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.73854\n",
      "Epoch 191/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0861 - acc: 0.7016 - val_loss: 1.4566 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.73854\n",
      "Epoch 192/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0743 - acc: 0.6890 - val_loss: 1.4044 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.73854\n",
      "Epoch 193/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1019 - acc: 0.6965 - val_loss: 1.4930 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.73854\n",
      "Epoch 194/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0903 - acc: 0.6860 - val_loss: 1.6816 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.73854\n",
      "Epoch 195/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1271 - acc: 0.6809 - val_loss: 1.4972 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.73854\n",
      "Epoch 196/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0927 - acc: 0.6842 - val_loss: 1.7708 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.73854\n",
      "Epoch 197/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0794 - acc: 0.6902 - val_loss: 1.3936 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.73854\n",
      "Epoch 198/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1039 - acc: 0.6878 - val_loss: 1.4930 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.73854\n",
      "Epoch 199/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0987 - acc: 0.6875 - val_loss: 1.5455 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.73854\n",
      "Epoch 200/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0753 - acc: 0.6833 - val_loss: 1.5880 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.73854\n",
      "Epoch 201/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1070 - acc: 0.6809 - val_loss: 1.3933 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.73854\n",
      "Epoch 202/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0492 - acc: 0.7019 - val_loss: 1.5277 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.73854\n",
      "Epoch 203/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0581 - acc: 0.7082 - val_loss: 1.6518 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.73854\n",
      "Epoch 204/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0582 - acc: 0.6998 - val_loss: 1.5264 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.73854\n",
      "Epoch 205/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0887 - acc: 0.6974 - val_loss: 1.7066 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.73854\n",
      "Epoch 206/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0690 - acc: 0.6953 - val_loss: 1.6554 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.73854\n",
      "Epoch 207/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0809 - acc: 0.7010 - val_loss: 1.3386 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.73854\n",
      "Epoch 208/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.1010 - acc: 0.6977 - val_loss: 1.3980 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.73854\n",
      "Epoch 209/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0605 - acc: 0.6932 - val_loss: 1.3727 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.73854\n",
      "Epoch 210/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0765 - acc: 0.6839 - val_loss: 1.3705 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.73854\n",
      "Epoch 211/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0864 - acc: 0.6884 - val_loss: 1.5457 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.73854\n",
      "Epoch 212/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0625 - acc: 0.7028 - val_loss: 1.5837 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.73854\n",
      "Epoch 213/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0982 - acc: 0.6767 - val_loss: 1.8036 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.73854\n",
      "Epoch 214/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0587 - acc: 0.6944 - val_loss: 1.3289 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.73854\n",
      "Epoch 215/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0285 - acc: 0.7097 - val_loss: 1.6219 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.73854\n",
      "Epoch 216/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0809 - acc: 0.6845 - val_loss: 1.4737 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.73854\n",
      "Epoch 217/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0758 - acc: 0.6974 - val_loss: 1.7397 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.73854\n",
      "Epoch 218/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0728 - acc: 0.6923 - val_loss: 1.5653 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.73854\n",
      "Epoch 219/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0615 - acc: 0.6947 - val_loss: 1.2256 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.73854\n",
      "Epoch 220/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0452 - acc: 0.6995 - val_loss: 1.2453 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.73854\n",
      "Epoch 221/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0667 - acc: 0.6989 - val_loss: 1.3980 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.73854\n",
      "Epoch 222/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0557 - acc: 0.7034 - val_loss: 1.3150 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.73854\n",
      "Epoch 223/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0564 - acc: 0.7094 - val_loss: 1.1846 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00223: val_acc improved from 0.73854 to 0.75472, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 224/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 2.0289 - acc: 0.7067 - val_loss: 1.6076 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.75472\n",
      "Epoch 225/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0418 - acc: 0.7025 - val_loss: 1.7365 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.75472\n",
      "Epoch 226/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0578 - acc: 0.7022 - val_loss: 2.0155 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.75472\n",
      "Epoch 227/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0297 - acc: 0.7127 - val_loss: 1.6915 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.75472\n",
      "Epoch 228/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0353 - acc: 0.7106 - val_loss: 1.5815 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.75472\n",
      "Epoch 229/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0576 - acc: 0.7085 - val_loss: 1.6210 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.75472\n",
      "Epoch 230/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0415 - acc: 0.7007 - val_loss: 1.8627 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.75472\n",
      "Epoch 231/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0569 - acc: 0.7070 - val_loss: 1.6277 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.75472\n",
      "Epoch 232/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0104 - acc: 0.7248 - val_loss: 1.6340 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.75472\n",
      "Epoch 233/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0355 - acc: 0.7151 - val_loss: 1.4085 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.75472\n",
      "Epoch 234/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0559 - acc: 0.6923 - val_loss: 1.8373 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.75472\n",
      "Epoch 235/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0562 - acc: 0.7028 - val_loss: 1.4631 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.75472\n",
      "Epoch 236/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0405 - acc: 0.7172 - val_loss: 1.3639 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.75472\n",
      "Epoch 237/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0442 - acc: 0.7028 - val_loss: 1.6474 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.75472\n",
      "Epoch 238/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0226 - acc: 0.7233 - val_loss: 1.2074 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.75472\n",
      "Epoch 239/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0584 - acc: 0.7043 - val_loss: 2.1377 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.75472\n",
      "Epoch 240/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0060 - acc: 0.7269 - val_loss: 1.3005 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.75472\n",
      "Epoch 241/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0033 - acc: 0.7181 - val_loss: 1.3599 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.75472\n",
      "Epoch 242/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0363 - acc: 0.7100 - val_loss: 1.2934 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.75472\n",
      "Epoch 243/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0279 - acc: 0.7230 - val_loss: 1.5538 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.75472\n",
      "Epoch 244/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9943 - acc: 0.7139 - val_loss: 1.3820 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.75472\n",
      "Epoch 245/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 2.0102 - acc: 0.7257 - val_loss: 1.3939 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.75472\n",
      "Epoch 246/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9960 - acc: 0.7145 - val_loss: 1.3716 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.75472\n",
      "Epoch 247/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0222 - acc: 0.7215 - val_loss: 1.5226 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.75472\n",
      "Epoch 248/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9983 - acc: 0.7251 - val_loss: 1.4802 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.75472\n",
      "Epoch 249/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0333 - acc: 0.7055 - val_loss: 1.5901 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.75472\n",
      "Epoch 250/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0179 - acc: 0.7157 - val_loss: 1.7575 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.75472\n",
      "Epoch 251/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9906 - acc: 0.7124 - val_loss: 1.3221 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.75472\n",
      "Epoch 252/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9941 - acc: 0.7368 - val_loss: 1.4486 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.75472\n",
      "Epoch 253/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9833 - acc: 0.7353 - val_loss: 1.4627 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.75472\n",
      "Epoch 254/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0107 - acc: 0.7191 - val_loss: 1.7021 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.75472\n",
      "Epoch 255/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0227 - acc: 0.7296 - val_loss: 1.5135 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.75472\n",
      "Epoch 256/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9625 - acc: 0.7350 - val_loss: 1.4742 - val_acc: 0.6873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00256: val_acc did not improve from 0.75472\n",
      "Epoch 257/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9667 - acc: 0.7275 - val_loss: 1.6442 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.75472\n",
      "Epoch 258/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 1.9630 - acc: 0.7254 - val_loss: 1.5204 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.75472\n",
      "Epoch 259/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9764 - acc: 0.7287 - val_loss: 1.3257 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.75472\n",
      "Epoch 260/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0032 - acc: 0.7130 - val_loss: 1.5825 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.75472\n",
      "Epoch 261/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9750 - acc: 0.7290 - val_loss: 1.5924 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.75472\n",
      "Epoch 262/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 2.0059 - acc: 0.7206 - val_loss: 1.4657 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.75472\n",
      "Epoch 263/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9981 - acc: 0.7218 - val_loss: 1.5934 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.75472\n",
      "Epoch 264/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9919 - acc: 0.7347 - val_loss: 1.9532 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.75472\n",
      "Epoch 265/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9876 - acc: 0.7236 - val_loss: 1.3770 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.75472\n",
      "Epoch 266/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9798 - acc: 0.7314 - val_loss: 1.7180 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.75472\n",
      "Epoch 267/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9871 - acc: 0.7269 - val_loss: 1.4601 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.75472\n",
      "Epoch 268/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9801 - acc: 0.7197 - val_loss: 1.3920 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.75472\n",
      "Epoch 269/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9879 - acc: 0.7266 - val_loss: 1.6073 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.75472\n",
      "Epoch 270/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9960 - acc: 0.7221 - val_loss: 1.5377 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.75472\n",
      "Epoch 271/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9789 - acc: 0.7338 - val_loss: 1.6030 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.75472\n",
      "Epoch 272/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9529 - acc: 0.7410 - val_loss: 1.5359 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.75472\n",
      "Epoch 273/3000\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 1.9685 - acc: 0.7269 - val_loss: 1.3707 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.75472\n",
      "Epoch 274/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9854 - acc: 0.7305 - val_loss: 1.4053 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.75472\n",
      "Epoch 275/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9833 - acc: 0.7311 - val_loss: 1.5784 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.75472\n",
      "Epoch 276/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9735 - acc: 0.7296 - val_loss: 1.3263 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.75472\n",
      "Epoch 277/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9634 - acc: 0.7272 - val_loss: 1.2789 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.75472\n",
      "Epoch 278/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9360 - acc: 0.7473 - val_loss: 1.5227 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.75472\n",
      "Epoch 279/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9588 - acc: 0.7371 - val_loss: 1.9124 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.75472\n",
      "Epoch 280/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9537 - acc: 0.7257 - val_loss: 1.8593 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.75472\n",
      "Epoch 281/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9641 - acc: 0.7305 - val_loss: 1.4890 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.75472\n",
      "Epoch 282/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9563 - acc: 0.7254 - val_loss: 1.6478 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.75472\n",
      "Epoch 283/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9715 - acc: 0.7317 - val_loss: 1.3524 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.75472\n",
      "Epoch 284/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9571 - acc: 0.7314 - val_loss: 1.4591 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.75472\n",
      "Epoch 285/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9734 - acc: 0.7302 - val_loss: 1.1844 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.75472\n",
      "Epoch 286/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9525 - acc: 0.7431 - val_loss: 1.6666 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.75472\n",
      "Epoch 287/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9739 - acc: 0.7287 - val_loss: 1.3025 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.75472\n",
      "Epoch 288/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9474 - acc: 0.7368 - val_loss: 1.2204 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.75472\n",
      "Epoch 289/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9717 - acc: 0.7293 - val_loss: 1.5128 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.75472\n",
      "Epoch 290/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9448 - acc: 0.7380 - val_loss: 1.3667 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.75472\n",
      "Epoch 291/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9392 - acc: 0.7401 - val_loss: 1.2777 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.75472\n",
      "Epoch 292/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9497 - acc: 0.7353 - val_loss: 1.3100 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.75472\n",
      "Epoch 293/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9511 - acc: 0.7494 - val_loss: 1.7024 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.75472\n",
      "Epoch 294/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9532 - acc: 0.7359 - val_loss: 1.5818 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.75472\n",
      "Epoch 295/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9413 - acc: 0.7428 - val_loss: 1.2527 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.75472\n",
      "Epoch 296/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9148 - acc: 0.7473 - val_loss: 1.3989 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.75472\n",
      "Epoch 297/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9377 - acc: 0.7413 - val_loss: 1.4955 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.75472\n",
      "Epoch 298/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9257 - acc: 0.7491 - val_loss: 1.3775 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.75472\n",
      "Epoch 299/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9525 - acc: 0.7467 - val_loss: 1.4076 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.75472\n",
      "Epoch 300/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9121 - acc: 0.7455 - val_loss: 1.8934 - val_acc: 0.5445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_acc did not improve from 0.75472\n",
      "Epoch 301/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9486 - acc: 0.7476 - val_loss: 1.4373 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.75472\n",
      "Epoch 302/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9343 - acc: 0.7437 - val_loss: 1.2516 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.75472\n",
      "Epoch 303/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9653 - acc: 0.7251 - val_loss: 1.2921 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.75472\n",
      "Epoch 304/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9360 - acc: 0.7365 - val_loss: 1.4472 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.75472\n",
      "Epoch 305/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9468 - acc: 0.7299 - val_loss: 1.1882 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.75472\n",
      "Epoch 306/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8945 - acc: 0.7587 - val_loss: 1.2375 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.75472\n",
      "Epoch 307/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9359 - acc: 0.7449 - val_loss: 1.4035 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.75472\n",
      "Epoch 308/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9190 - acc: 0.7458 - val_loss: 1.4114 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.75472\n",
      "Epoch 309/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9059 - acc: 0.7521 - val_loss: 1.3555 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.75472\n",
      "Epoch 310/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9384 - acc: 0.7413 - val_loss: 1.5425 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.75472\n",
      "Epoch 311/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9146 - acc: 0.7512 - val_loss: 1.1305 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00311: val_acc improved from 0.75472 to 0.78437, saving model to model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 312/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9340 - acc: 0.7395 - val_loss: 1.1232 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.78437\n",
      "Epoch 313/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9383 - acc: 0.7410 - val_loss: 1.3640 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.78437\n",
      "Epoch 314/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9364 - acc: 0.7305 - val_loss: 1.4005 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.78437\n",
      "Epoch 315/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9400 - acc: 0.7323 - val_loss: 1.2315 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.78437\n",
      "Epoch 316/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9185 - acc: 0.7446 - val_loss: 1.4016 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.78437\n",
      "Epoch 317/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9419 - acc: 0.7416 - val_loss: 1.2714 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.78437\n",
      "Epoch 318/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9275 - acc: 0.7347 - val_loss: 1.5025 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.78437\n",
      "Epoch 319/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8924 - acc: 0.7578 - val_loss: 1.3672 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.78437\n",
      "Epoch 320/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9215 - acc: 0.7485 - val_loss: 1.5658 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.78437\n",
      "Epoch 321/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9232 - acc: 0.7473 - val_loss: 1.2987 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.78437\n",
      "Epoch 322/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9306 - acc: 0.7452 - val_loss: 1.5355 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.78437\n",
      "Epoch 323/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9130 - acc: 0.7578 - val_loss: 1.4953 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.78437\n",
      "Epoch 324/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9056 - acc: 0.7491 - val_loss: 1.3570 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.78437\n",
      "Epoch 325/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8896 - acc: 0.7515 - val_loss: 1.3942 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.78437\n",
      "Epoch 326/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9399 - acc: 0.7425 - val_loss: 1.3895 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.78437\n",
      "Epoch 327/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9140 - acc: 0.7494 - val_loss: 1.8051 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.78437\n",
      "Epoch 328/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9032 - acc: 0.7542 - val_loss: 1.2058 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.78437\n",
      "Epoch 329/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9160 - acc: 0.7491 - val_loss: 1.2455 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.78437\n",
      "Epoch 330/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8971 - acc: 0.7518 - val_loss: 1.4719 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.78437\n",
      "Epoch 331/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9251 - acc: 0.7422 - val_loss: 1.2728 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.78437\n",
      "Epoch 332/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9059 - acc: 0.7479 - val_loss: 1.3279 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.78437\n",
      "Epoch 333/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9113 - acc: 0.7539 - val_loss: 1.2210 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.78437\n",
      "Epoch 334/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9021 - acc: 0.7413 - val_loss: 1.3885 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.78437\n",
      "Epoch 335/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9120 - acc: 0.7554 - val_loss: 1.6344 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.78437\n",
      "Epoch 336/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9075 - acc: 0.7476 - val_loss: 1.6357 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.78437\n",
      "Epoch 337/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8894 - acc: 0.7584 - val_loss: 1.1828 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.78437\n",
      "Epoch 338/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9199 - acc: 0.7587 - val_loss: 1.3664 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.78437\n",
      "Epoch 339/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8869 - acc: 0.7632 - val_loss: 2.0296 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.78437\n",
      "Epoch 340/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9338 - acc: 0.7479 - val_loss: 1.3554 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.78437\n",
      "Epoch 341/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9092 - acc: 0.7491 - val_loss: 1.2951 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.78437\n",
      "Epoch 342/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9084 - acc: 0.7437 - val_loss: 1.5286 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.78437\n",
      "Epoch 343/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8889 - acc: 0.7566 - val_loss: 1.7760 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.78437\n",
      "Epoch 344/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9225 - acc: 0.7527 - val_loss: 1.5387 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.78437\n",
      "Epoch 345/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8721 - acc: 0.7635 - val_loss: 1.1735 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.78437\n",
      "Epoch 346/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.9062 - acc: 0.7527 - val_loss: 1.5853 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.78437\n",
      "Epoch 347/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9140 - acc: 0.7533 - val_loss: 1.5410 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.78437\n",
      "Epoch 348/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8747 - acc: 0.7566 - val_loss: 1.4062 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.78437\n",
      "Epoch 349/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8860 - acc: 0.7575 - val_loss: 1.3570 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.78437\n",
      "Epoch 350/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8975 - acc: 0.7461 - val_loss: 1.2302 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.78437\n",
      "Epoch 351/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8896 - acc: 0.7605 - val_loss: 1.4106 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.78437\n",
      "Epoch 352/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8951 - acc: 0.7527 - val_loss: 1.8326 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.78437\n",
      "Epoch 353/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8704 - acc: 0.7599 - val_loss: 1.2223 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.78437\n",
      "Epoch 354/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8483 - acc: 0.7758 - val_loss: 1.4831 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.78437\n",
      "Epoch 355/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8671 - acc: 0.7548 - val_loss: 1.1950 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.78437\n",
      "Epoch 356/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8632 - acc: 0.7620 - val_loss: 1.7094 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.78437\n",
      "Epoch 357/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9128 - acc: 0.7557 - val_loss: 1.2252 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.78437\n",
      "Epoch 358/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8998 - acc: 0.7518 - val_loss: 1.7532 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.78437\n",
      "Epoch 359/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8982 - acc: 0.7443 - val_loss: 1.2971 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.78437\n",
      "Epoch 360/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8651 - acc: 0.7608 - val_loss: 1.3090 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.78437\n",
      "Epoch 361/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8786 - acc: 0.7608 - val_loss: 1.2693 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.78437\n",
      "Epoch 362/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8981 - acc: 0.7641 - val_loss: 1.4752 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.78437\n",
      "Epoch 363/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8608 - acc: 0.7671 - val_loss: 1.3857 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.78437\n",
      "Epoch 364/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8979 - acc: 0.7596 - val_loss: 1.5792 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.78437\n",
      "Epoch 365/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8698 - acc: 0.7581 - val_loss: 1.4988 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.78437\n",
      "Epoch 366/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8405 - acc: 0.7668 - val_loss: 1.8383 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.78437\n",
      "Epoch 367/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8727 - acc: 0.7659 - val_loss: 1.3737 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.78437\n",
      "Epoch 368/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8766 - acc: 0.7614 - val_loss: 1.3314 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.78437\n",
      "Epoch 369/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8755 - acc: 0.7662 - val_loss: 1.3059 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.78437\n",
      "Epoch 370/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8594 - acc: 0.7590 - val_loss: 1.2743 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.78437\n",
      "Epoch 371/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9000 - acc: 0.7431 - val_loss: 1.4582 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.78437\n",
      "Epoch 372/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8837 - acc: 0.7587 - val_loss: 1.8025 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.78437\n",
      "Epoch 373/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8835 - acc: 0.7638 - val_loss: 1.9964 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.78437\n",
      "Epoch 374/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8763 - acc: 0.7623 - val_loss: 1.2459 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.78437\n",
      "Epoch 375/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8801 - acc: 0.7599 - val_loss: 1.6808 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.78437\n",
      "Epoch 376/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8806 - acc: 0.7575 - val_loss: 1.2495 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.78437\n",
      "Epoch 377/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8515 - acc: 0.7608 - val_loss: 1.5058 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.78437\n",
      "Epoch 378/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8747 - acc: 0.7659 - val_loss: 1.9892 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.78437\n",
      "Epoch 379/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8737 - acc: 0.7605 - val_loss: 1.5133 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.78437\n",
      "Epoch 380/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8694 - acc: 0.7623 - val_loss: 1.3081 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.78437\n",
      "Epoch 381/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8503 - acc: 0.7698 - val_loss: 1.5720 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.78437\n",
      "Epoch 382/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8476 - acc: 0.7623 - val_loss: 1.4492 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.78437\n",
      "Epoch 383/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8687 - acc: 0.7575 - val_loss: 1.4458 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.78437\n",
      "Epoch 384/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8628 - acc: 0.7569 - val_loss: 1.1993 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.78437\n",
      "Epoch 385/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8469 - acc: 0.7644 - val_loss: 1.4546 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.78437\n",
      "Epoch 386/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8835 - acc: 0.7551 - val_loss: 1.7115 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.78437\n",
      "Epoch 387/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8621 - acc: 0.7599 - val_loss: 1.3058 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.78437\n",
      "Epoch 388/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8586 - acc: 0.7602 - val_loss: 1.7172 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.78437\n",
      "Epoch 389/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8674 - acc: 0.7647 - val_loss: 1.6238 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.78437\n",
      "Epoch 390/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.9079 - acc: 0.7476 - val_loss: 1.3803 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.78437\n",
      "Epoch 391/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8765 - acc: 0.7545 - val_loss: 1.2727 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.78437\n",
      "Epoch 392/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8502 - acc: 0.7665 - val_loss: 1.4175 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.78437\n",
      "Epoch 393/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8420 - acc: 0.7656 - val_loss: 1.4988 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.78437\n",
      "Epoch 394/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8388 - acc: 0.7773 - val_loss: 1.2329 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.78437\n",
      "Epoch 395/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8460 - acc: 0.7686 - val_loss: 1.2715 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.78437\n",
      "Epoch 396/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8373 - acc: 0.7822 - val_loss: 1.4598 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.78437\n",
      "Epoch 397/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8607 - acc: 0.7608 - val_loss: 1.3958 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.78437\n",
      "Epoch 398/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8455 - acc: 0.7764 - val_loss: 1.4083 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.78437\n",
      "Epoch 399/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8611 - acc: 0.7728 - val_loss: 1.7200 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.78437\n",
      "Epoch 400/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8609 - acc: 0.7605 - val_loss: 1.4832 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.78437\n",
      "Epoch 401/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8586 - acc: 0.7686 - val_loss: 1.2815 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.78437\n",
      "Epoch 402/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8374 - acc: 0.7692 - val_loss: 1.1837 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.78437\n",
      "Epoch 403/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8422 - acc: 0.7689 - val_loss: 1.2669 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.78437\n",
      "Epoch 404/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8665 - acc: 0.7545 - val_loss: 1.3666 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.78437\n",
      "Epoch 405/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8783 - acc: 0.7635 - val_loss: 1.5081 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.78437\n",
      "Epoch 406/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8508 - acc: 0.7650 - val_loss: 1.3435 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.78437\n",
      "Epoch 407/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8218 - acc: 0.7635 - val_loss: 1.2239 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.78437\n",
      "Epoch 408/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8493 - acc: 0.7704 - val_loss: 1.6202 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.78437\n",
      "Epoch 409/3000\n",
      "52/52 [==============================] - 9s 166ms/step - loss: 1.8524 - acc: 0.7650 - val_loss: 1.4229 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.78437\n",
      "Epoch 410/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8487 - acc: 0.7608 - val_loss: 1.8951 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.78437\n",
      "Epoch 411/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8423 - acc: 0.7686 - val_loss: 1.6097 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.78437\n",
      "Epoch 412/3000\n",
      "52/52 [==============================] - 9s 167ms/step - loss: 1.8572 - acc: 0.7581 - val_loss: 1.3260 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.78437\n",
      "Epoch 00412: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_6===\n",
      "semi loading: model/mfcc7/LGD_fold6_resnet2-.h5\n",
      "Epoch 5/3000\n",
      "106/106 [==============================] - 18s 168ms/step - loss: 1.9593 - acc: 0.7164 - val_loss: 0.9806 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.82210, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 6/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8859 - acc: 0.7379 - val_loss: 0.9923 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82210\n",
      "Epoch 7/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8495 - acc: 0.7500 - val_loss: 1.0046 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82210\n",
      "Epoch 8/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8313 - acc: 0.7577 - val_loss: 0.9608 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82210 to 0.83288, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 9/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8170 - acc: 0.7683 - val_loss: 0.9443 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83288\n",
      "Epoch 10/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8022 - acc: 0.7656 - val_loss: 0.9749 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83288\n",
      "Epoch 11/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7952 - acc: 0.7698 - val_loss: 0.9576 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83288\n",
      "Epoch 12/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.8134 - acc: 0.7612 - val_loss: 0.9263 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83288 to 0.83827, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 13/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7978 - acc: 0.7633 - val_loss: 0.9556 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83827\n",
      "Epoch 14/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7776 - acc: 0.7765 - val_loss: 0.9807 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83827\n",
      "Epoch 15/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7659 - acc: 0.7780 - val_loss: 0.9202 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83827\n",
      "Epoch 16/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7892 - acc: 0.7677 - val_loss: 0.9262 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.83827 to 0.84367, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 17/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7712 - acc: 0.7683 - val_loss: 0.9003 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84367\n",
      "Epoch 18/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7413 - acc: 0.7807 - val_loss: 0.9254 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84367\n",
      "Epoch 19/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7613 - acc: 0.7706 - val_loss: 0.9767 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84367\n",
      "Epoch 20/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7410 - acc: 0.7916 - val_loss: 0.9688 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84367\n",
      "Epoch 21/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7343 - acc: 0.7910 - val_loss: 0.9361 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84367\n",
      "Epoch 22/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7389 - acc: 0.7863 - val_loss: 0.9688 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84367\n",
      "Epoch 23/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7437 - acc: 0.7774 - val_loss: 0.9533 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84367\n",
      "Epoch 24/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7413 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84367\n",
      "Epoch 25/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7035 - acc: 0.8078 - val_loss: 0.9418 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.84367 to 0.84636, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 26/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6995 - acc: 0.7913 - val_loss: 0.9274 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84636\n",
      "Epoch 27/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7336 - acc: 0.7780 - val_loss: 0.9166 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84636\n",
      "Epoch 28/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6886 - acc: 0.7877 - val_loss: 0.9178 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84636\n",
      "Epoch 29/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7124 - acc: 0.7824 - val_loss: 0.9044 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84636\n",
      "Epoch 30/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7278 - acc: 0.7869 - val_loss: 0.8978 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84636\n",
      "Epoch 31/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7193 - acc: 0.7851 - val_loss: 0.9155 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84636\n",
      "Epoch 32/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7097 - acc: 0.7880 - val_loss: 0.9354 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84636\n",
      "Epoch 33/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6808 - acc: 0.7901 - val_loss: 0.9159 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84636\n",
      "Epoch 34/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7093 - acc: 0.7883 - val_loss: 0.9447 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84636\n",
      "Epoch 35/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.7078 - acc: 0.7904 - val_loss: 0.9341 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84636\n",
      "Epoch 36/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6973 - acc: 0.7922 - val_loss: 0.9159 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84636\n",
      "Epoch 37/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6938 - acc: 0.7919 - val_loss: 0.9144 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84636\n",
      "Epoch 38/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6969 - acc: 0.7963 - val_loss: 0.9131 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84636\n",
      "Epoch 39/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6910 - acc: 0.8010 - val_loss: 0.8801 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84636\n",
      "Epoch 40/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6716 - acc: 0.7880 - val_loss: 0.9151 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84636\n",
      "Epoch 41/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6695 - acc: 0.8034 - val_loss: 0.9094 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84636\n",
      "Epoch 42/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6599 - acc: 0.8010 - val_loss: 0.8965 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84636\n",
      "Epoch 43/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6850 - acc: 0.7948 - val_loss: 0.9399 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84636\n",
      "Epoch 44/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6741 - acc: 0.7889 - val_loss: 0.8536 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84636\n",
      "Epoch 45/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6892 - acc: 0.7983 - val_loss: 0.9524 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84636\n",
      "Epoch 46/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6933 - acc: 0.7889 - val_loss: 0.8587 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84636\n",
      "Epoch 47/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6670 - acc: 0.7966 - val_loss: 0.9131 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84636\n",
      "Epoch 48/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6649 - acc: 0.7963 - val_loss: 0.9215 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84636\n",
      "Epoch 49/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6694 - acc: 0.7942 - val_loss: 0.9298 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84636\n",
      "Epoch 50/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6605 - acc: 0.7992 - val_loss: 0.9141 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84636\n",
      "Epoch 51/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6542 - acc: 0.8072 - val_loss: 0.8789 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84636\n",
      "Epoch 52/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6906 - acc: 0.7871 - val_loss: 0.9240 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84636\n",
      "Epoch 53/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6667 - acc: 0.7983 - val_loss: 0.9065 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84636\n",
      "Epoch 54/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6600 - acc: 0.7975 - val_loss: 0.8703 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84636\n",
      "Epoch 55/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6450 - acc: 0.8104 - val_loss: 0.9047 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84636\n",
      "Epoch 56/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.6636 - acc: 0.7975 - val_loss: 0.9042 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84636\n",
      "Epoch 57/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6469 - acc: 0.7998 - val_loss: 0.8972 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84636\n",
      "Epoch 58/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6569 - acc: 0.7966 - val_loss: 0.9071 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84636\n",
      "Epoch 59/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6473 - acc: 0.7948 - val_loss: 0.8987 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84636\n",
      "Epoch 60/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6375 - acc: 0.8019 - val_loss: 0.9196 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84636\n",
      "Epoch 61/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6349 - acc: 0.8066 - val_loss: 0.9291 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84636\n",
      "Epoch 62/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6530 - acc: 0.8075 - val_loss: 0.9018 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84636\n",
      "Epoch 63/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6502 - acc: 0.8022 - val_loss: 0.9213 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84636\n",
      "Epoch 64/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6244 - acc: 0.8131 - val_loss: 0.9009 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84636\n",
      "Epoch 65/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6425 - acc: 0.7945 - val_loss: 0.8687 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84636\n",
      "Epoch 66/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6607 - acc: 0.8010 - val_loss: 0.8674 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84636\n",
      "Epoch 67/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6170 - acc: 0.8160 - val_loss: 0.8571 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84636\n",
      "Epoch 68/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6367 - acc: 0.8037 - val_loss: 0.8915 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84636\n",
      "Epoch 69/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6451 - acc: 0.7992 - val_loss: 0.8814 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84636\n",
      "Epoch 70/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6532 - acc: 0.8087 - val_loss: 0.9165 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84636\n",
      "Epoch 71/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6078 - acc: 0.7922 - val_loss: 0.8813 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84636\n",
      "Epoch 72/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6317 - acc: 0.8119 - val_loss: 0.9038 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84636\n",
      "Epoch 73/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6041 - acc: 0.8287 - val_loss: 0.8455 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84636\n",
      "Epoch 74/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6099 - acc: 0.7995 - val_loss: 0.8765 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84636\n",
      "Epoch 75/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6183 - acc: 0.8116 - val_loss: 0.8918 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84636\n",
      "Epoch 76/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6301 - acc: 0.8025 - val_loss: 0.8724 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84636\n",
      "Epoch 77/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6090 - acc: 0.8045 - val_loss: 0.9156 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84636\n",
      "Epoch 78/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6225 - acc: 0.7957 - val_loss: 0.8668 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84636\n",
      "Epoch 79/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6098 - acc: 0.8048 - val_loss: 0.9584 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84636\n",
      "Epoch 80/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6313 - acc: 0.8081 - val_loss: 0.9107 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84636\n",
      "Epoch 81/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6200 - acc: 0.8122 - val_loss: 0.8731 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84636\n",
      "Epoch 82/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6179 - acc: 0.8069 - val_loss: 0.8716 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.84636\n",
      "Epoch 83/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6028 - acc: 0.8178 - val_loss: 0.8547 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.84636\n",
      "Epoch 84/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6059 - acc: 0.8045 - val_loss: 0.8380 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.84636\n",
      "Epoch 85/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5728 - acc: 0.8249 - val_loss: 0.8538 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.84636\n",
      "Epoch 86/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5942 - acc: 0.8128 - val_loss: 0.8579 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.84636\n",
      "Epoch 87/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6193 - acc: 0.8060 - val_loss: 0.8540 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.84636\n",
      "Epoch 88/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6233 - acc: 0.8090 - val_loss: 0.8986 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.84636\n",
      "Epoch 89/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6174 - acc: 0.8154 - val_loss: 0.8501 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.84636\n",
      "Epoch 90/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6007 - acc: 0.8042 - val_loss: 0.8355 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.84636 to 0.84906, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 91/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6037 - acc: 0.8154 - val_loss: 0.9272 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.84906\n",
      "Epoch 92/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5829 - acc: 0.8267 - val_loss: 0.8919 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.84906\n",
      "Epoch 93/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6124 - acc: 0.8010 - val_loss: 0.8651 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.84906\n",
      "Epoch 94/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6071 - acc: 0.8149 - val_loss: 0.9222 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.84906\n",
      "Epoch 95/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5799 - acc: 0.8308 - val_loss: 0.8773 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.84906\n",
      "Epoch 96/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5956 - acc: 0.8087 - val_loss: 0.8592 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.84906\n",
      "Epoch 97/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.6134 - acc: 0.8081 - val_loss: 0.8502 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.84906\n",
      "Epoch 98/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5870 - acc: 0.8149 - val_loss: 0.8212 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.84906\n",
      "Epoch 99/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5699 - acc: 0.8275 - val_loss: 0.8412 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.84906\n",
      "Epoch 100/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5922 - acc: 0.8116 - val_loss: 0.8228 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.84906\n",
      "Epoch 101/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5854 - acc: 0.8160 - val_loss: 0.8449 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.84906\n",
      "Epoch 102/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5408 - acc: 0.8290 - val_loss: 0.8311 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.84906 to 0.85175, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 103/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5800 - acc: 0.8146 - val_loss: 0.8426 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85175\n",
      "Epoch 104/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5441 - acc: 0.8258 - val_loss: 0.8484 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85175\n",
      "Epoch 105/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5730 - acc: 0.8149 - val_loss: 0.8670 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85175\n",
      "Epoch 106/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5628 - acc: 0.8184 - val_loss: 0.8248 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85175\n",
      "Epoch 107/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5826 - acc: 0.8175 - val_loss: 0.8329 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85175\n",
      "Epoch 108/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5658 - acc: 0.8208 - val_loss: 0.8368 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85175\n",
      "Epoch 109/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5769 - acc: 0.8160 - val_loss: 0.8702 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85175\n",
      "Epoch 110/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5305 - acc: 0.8267 - val_loss: 0.8167 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00110: val_acc improved from 0.85175 to 0.85445, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 111/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5356 - acc: 0.8358 - val_loss: 0.8319 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.85445\n",
      "Epoch 112/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5711 - acc: 0.8240 - val_loss: 0.8106 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.85445\n",
      "Epoch 113/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5614 - acc: 0.8243 - val_loss: 0.8242 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.85445\n",
      "Epoch 114/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5628 - acc: 0.8231 - val_loss: 0.8303 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.85445\n",
      "Epoch 115/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5381 - acc: 0.8296 - val_loss: 0.8368 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.85445\n",
      "Epoch 116/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5524 - acc: 0.8222 - val_loss: 0.8392 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.85445\n",
      "Epoch 117/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5478 - acc: 0.8125 - val_loss: 0.8575 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.85445\n",
      "Epoch 118/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5753 - acc: 0.8181 - val_loss: 0.8268 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.85445\n",
      "Epoch 119/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5540 - acc: 0.8308 - val_loss: 0.8243 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.85445\n",
      "Epoch 120/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5441 - acc: 0.8284 - val_loss: 0.8498 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.85445\n",
      "Epoch 121/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5264 - acc: 0.8272 - val_loss: 0.8483 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.85445\n",
      "Epoch 122/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5736 - acc: 0.8193 - val_loss: 0.8368 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.85445\n",
      "Epoch 123/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5218 - acc: 0.8387 - val_loss: 0.8210 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.85445\n",
      "Epoch 124/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5470 - acc: 0.8196 - val_loss: 0.8299 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.85445\n",
      "Epoch 125/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5598 - acc: 0.8293 - val_loss: 0.8301 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.85445\n",
      "Epoch 126/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5428 - acc: 0.8287 - val_loss: 0.8302 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.85445\n",
      "Epoch 127/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5435 - acc: 0.8149 - val_loss: 0.8135 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.85445\n",
      "Epoch 128/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5488 - acc: 0.8157 - val_loss: 0.8296 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.85445\n",
      "Epoch 129/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5319 - acc: 0.8278 - val_loss: 0.8220 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.85445\n",
      "Epoch 130/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5199 - acc: 0.8314 - val_loss: 0.8433 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.85445\n",
      "Epoch 131/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5432 - acc: 0.8166 - val_loss: 0.8130 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.85445\n",
      "Epoch 132/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5291 - acc: 0.8325 - val_loss: 0.8269 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.85445\n",
      "Epoch 133/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5428 - acc: 0.8258 - val_loss: 0.8149 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.85445\n",
      "Epoch 134/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5313 - acc: 0.8337 - val_loss: 0.8191 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.85445\n",
      "Epoch 135/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5497 - acc: 0.8246 - val_loss: 0.8211 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.85445\n",
      "Epoch 136/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5396 - acc: 0.8323 - val_loss: 0.8244 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.85445\n",
      "Epoch 137/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5347 - acc: 0.8373 - val_loss: 0.8214 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.85445\n",
      "Epoch 138/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5533 - acc: 0.8331 - val_loss: 0.8387 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.85445\n",
      "Epoch 139/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5176 - acc: 0.8405 - val_loss: 0.8283 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.85445\n",
      "Epoch 140/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5462 - acc: 0.8278 - val_loss: 0.8142 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.85445\n",
      "Epoch 141/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5518 - acc: 0.8178 - val_loss: 0.8524 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.85445\n",
      "Epoch 142/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5262 - acc: 0.8384 - val_loss: 0.8303 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.85445\n",
      "Epoch 143/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5249 - acc: 0.8373 - val_loss: 0.8151 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.85445\n",
      "Epoch 144/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5280 - acc: 0.8278 - val_loss: 0.8338 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.85445\n",
      "Epoch 145/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5264 - acc: 0.8290 - val_loss: 0.8273 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.85445\n",
      "Epoch 146/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5120 - acc: 0.8343 - val_loss: 0.8261 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.85445\n",
      "Epoch 147/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5642 - acc: 0.8210 - val_loss: 0.8222 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.85445\n",
      "Epoch 148/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5203 - acc: 0.8343 - val_loss: 0.8419 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.85445\n",
      "Epoch 149/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5164 - acc: 0.8370 - val_loss: 0.8394 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.85445\n",
      "Epoch 150/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5204 - acc: 0.8264 - val_loss: 0.8287 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.85445\n",
      "Epoch 151/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5269 - acc: 0.8323 - val_loss: 0.8464 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.85445\n",
      "Epoch 152/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5109 - acc: 0.8399 - val_loss: 0.8394 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.85445\n",
      "Epoch 153/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5272 - acc: 0.8290 - val_loss: 0.8336 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.85445\n",
      "Epoch 154/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5168 - acc: 0.8272 - val_loss: 0.8227 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.85445\n",
      "Epoch 155/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5262 - acc: 0.8272 - val_loss: 0.8353 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.85445\n",
      "Epoch 156/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5380 - acc: 0.8317 - val_loss: 0.8463 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.85445\n",
      "Epoch 157/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5186 - acc: 0.8284 - val_loss: 0.8364 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.85445\n",
      "Epoch 158/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5436 - acc: 0.8246 - val_loss: 0.8396 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.85445\n",
      "Epoch 159/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8325 - val_loss: 0.8424 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.85445\n",
      "Epoch 160/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5341 - acc: 0.8255 - val_loss: 0.8329 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.85445\n",
      "Epoch 161/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5280 - acc: 0.8317 - val_loss: 0.8337 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.85445\n",
      "Epoch 162/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5633 - acc: 0.8075 - val_loss: 0.8252 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.85445\n",
      "Epoch 163/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5163 - acc: 0.8290 - val_loss: 0.8274 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.85445\n",
      "Epoch 164/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5313 - acc: 0.8311 - val_loss: 0.8287 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.85445\n",
      "Epoch 165/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5163 - acc: 0.8311 - val_loss: 0.8331 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.85445\n",
      "Epoch 166/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5487 - acc: 0.8225 - val_loss: 0.8199 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.85445\n",
      "Epoch 167/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5183 - acc: 0.8296 - val_loss: 0.8233 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.85445\n",
      "Epoch 168/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5268 - acc: 0.8228 - val_loss: 0.8191 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.85445\n",
      "Epoch 169/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5006 - acc: 0.8355 - val_loss: 0.8322 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.85445\n",
      "Epoch 170/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5069 - acc: 0.8390 - val_loss: 0.8229 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.85445\n",
      "Epoch 171/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5272 - acc: 0.8240 - val_loss: 0.8278 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.85445\n",
      "Epoch 172/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5325 - acc: 0.8196 - val_loss: 0.8342 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.85445\n",
      "Epoch 173/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5212 - acc: 0.8343 - val_loss: 0.8248 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.85445\n",
      "Epoch 174/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5159 - acc: 0.8278 - val_loss: 0.8145 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.85445\n",
      "Epoch 175/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4982 - acc: 0.8458 - val_loss: 0.8282 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.85445\n",
      "Epoch 176/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5203 - acc: 0.8314 - val_loss: 0.8193 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.85445\n",
      "Epoch 177/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5334 - acc: 0.8287 - val_loss: 0.8296 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.85445\n",
      "Epoch 178/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5200 - acc: 0.8210 - val_loss: 0.8222 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.85445\n",
      "Epoch 179/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5203 - acc: 0.8334 - val_loss: 0.8251 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.85445\n",
      "Epoch 180/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8352 - val_loss: 0.8269 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.85445\n",
      "Epoch 181/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5229 - acc: 0.8275 - val_loss: 0.8190 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.85445\n",
      "Epoch 182/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5091 - acc: 0.8281 - val_loss: 0.8274 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.85445\n",
      "Epoch 183/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5009 - acc: 0.8390 - val_loss: 0.8222 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.85445\n",
      "Epoch 184/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8423 - val_loss: 0.8236 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.85445\n",
      "Epoch 185/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5139 - acc: 0.8343 - val_loss: 0.8287 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.85445\n",
      "Epoch 186/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5121 - acc: 0.8284 - val_loss: 0.8175 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.85445\n",
      "Epoch 187/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5117 - acc: 0.8328 - val_loss: 0.8200 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.85445\n",
      "Epoch 188/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5016 - acc: 0.8175 - val_loss: 0.8188 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.85445\n",
      "Epoch 189/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4916 - acc: 0.8446 - val_loss: 0.8248 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.85445\n",
      "Epoch 190/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5215 - acc: 0.8278 - val_loss: 0.8230 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.85445\n",
      "Epoch 191/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5040 - acc: 0.8264 - val_loss: 0.8233 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.85445\n",
      "Epoch 192/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5181 - acc: 0.8299 - val_loss: 0.8181 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.85445\n",
      "Epoch 193/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4975 - acc: 0.8349 - val_loss: 0.8207 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.85445\n",
      "Epoch 194/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5300 - acc: 0.8311 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.85445\n",
      "Epoch 195/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5327 - acc: 0.8258 - val_loss: 0.8171 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.85445\n",
      "Epoch 196/3000\n",
      "106/106 [==============================] - 11s 107ms/step - loss: 1.5139 - acc: 0.8269 - val_loss: 0.8185 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.85445\n",
      "Epoch 197/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4890 - acc: 0.8417 - val_loss: 0.8206 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.85445\n",
      "Epoch 198/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4980 - acc: 0.8281 - val_loss: 0.8185 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.85445\n",
      "Epoch 199/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5231 - acc: 0.8149 - val_loss: 0.8160 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.85445\n",
      "Epoch 200/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5110 - acc: 0.8269 - val_loss: 0.8161 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.85445\n",
      "Epoch 201/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4968 - acc: 0.8449 - val_loss: 0.8073 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.85445\n",
      "Epoch 202/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4870 - acc: 0.8346 - val_loss: 0.8104 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.85445\n",
      "Epoch 203/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4853 - acc: 0.8396 - val_loss: 0.8090 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.85445\n",
      "Epoch 204/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5271 - acc: 0.8249 - val_loss: 0.8112 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.85445\n",
      "Epoch 205/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5170 - acc: 0.8246 - val_loss: 0.8144 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.85445\n",
      "Epoch 206/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5335 - acc: 0.8237 - val_loss: 0.8177 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.85445\n",
      "Epoch 207/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5029 - acc: 0.8402 - val_loss: 0.8266 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.85445\n",
      "Epoch 208/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8237 - val_loss: 0.8264 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.85445\n",
      "Epoch 209/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4998 - acc: 0.8373 - val_loss: 0.8225 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.85445\n",
      "Epoch 210/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4929 - acc: 0.8390 - val_loss: 0.8175 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.85445\n",
      "Epoch 211/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5077 - acc: 0.8325 - val_loss: 0.8152 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.85445\n",
      "Epoch 212/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5066 - acc: 0.8396 - val_loss: 0.8169 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.85445\n",
      "Epoch 213/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5304 - acc: 0.8296 - val_loss: 0.8208 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.85445\n",
      "Epoch 214/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4722 - acc: 0.8358 - val_loss: 0.8171 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.85445\n",
      "Epoch 215/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4969 - acc: 0.8361 - val_loss: 0.8193 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.85445\n",
      "Epoch 216/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4933 - acc: 0.8305 - val_loss: 0.8157 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.85445\n",
      "Epoch 217/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5042 - acc: 0.8311 - val_loss: 0.8092 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.85445\n",
      "Epoch 218/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5227 - acc: 0.8210 - val_loss: 0.8096 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.85445\n",
      "Epoch 219/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5151 - acc: 0.8284 - val_loss: 0.8106 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.85445\n",
      "Epoch 220/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5098 - acc: 0.8237 - val_loss: 0.8144 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.85445\n",
      "Epoch 221/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4979 - acc: 0.8390 - val_loss: 0.8126 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.85445\n",
      "Epoch 222/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4913 - acc: 0.8420 - val_loss: 0.8181 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.85445\n",
      "Epoch 223/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5028 - acc: 0.8417 - val_loss: 0.8194 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.85445\n",
      "Epoch 224/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5212 - acc: 0.8334 - val_loss: 0.8179 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.85445\n",
      "Epoch 225/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5200 - acc: 0.8302 - val_loss: 0.8163 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.85445\n",
      "Epoch 226/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5129 - acc: 0.8314 - val_loss: 0.8132 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.85445\n",
      "Epoch 227/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5223 - acc: 0.8275 - val_loss: 0.8133 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.85445\n",
      "Epoch 228/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4977 - acc: 0.8364 - val_loss: 0.8146 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.85445\n",
      "Epoch 229/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5109 - acc: 0.8317 - val_loss: 0.8101 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.85445\n",
      "Epoch 230/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5073 - acc: 0.8361 - val_loss: 0.8163 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.85445\n",
      "Epoch 231/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5091 - acc: 0.8275 - val_loss: 0.8166 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.85445\n",
      "Epoch 232/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5078 - acc: 0.8376 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.85445\n",
      "Epoch 233/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5234 - acc: 0.8370 - val_loss: 0.8218 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.85445\n",
      "Epoch 234/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8343 - val_loss: 0.8177 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.85445\n",
      "Epoch 235/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4746 - acc: 0.8390 - val_loss: 0.8185 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.85445\n",
      "Epoch 236/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5325 - acc: 0.8275 - val_loss: 0.8124 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.85445\n",
      "Epoch 237/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4826 - acc: 0.8429 - val_loss: 0.8110 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.85445\n",
      "Epoch 238/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5015 - acc: 0.8390 - val_loss: 0.8133 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.85445\n",
      "Epoch 239/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8331 - val_loss: 0.8106 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.85445\n",
      "Epoch 240/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8364 - val_loss: 0.8170 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.85445\n",
      "Epoch 241/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4936 - acc: 0.8328 - val_loss: 0.8111 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.85445\n",
      "Epoch 242/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4862 - acc: 0.8323 - val_loss: 0.8072 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.85445\n",
      "Epoch 243/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4964 - acc: 0.8320 - val_loss: 0.8089 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.85445\n",
      "Epoch 244/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5084 - acc: 0.8323 - val_loss: 0.8147 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.85445\n",
      "Epoch 245/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4983 - acc: 0.8473 - val_loss: 0.8160 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.85445\n",
      "Epoch 246/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5222 - acc: 0.8258 - val_loss: 0.8090 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.85445\n",
      "Epoch 247/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5157 - acc: 0.8305 - val_loss: 0.8114 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.85445\n",
      "Epoch 248/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5015 - acc: 0.8299 - val_loss: 0.8143 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.85445\n",
      "Epoch 249/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5189 - acc: 0.8373 - val_loss: 0.8082 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00249: val_acc improved from 0.85445 to 0.85445, saving model to model/mfcc7/LGD_semi_fold6_resnet2.h5\n",
      "Epoch 250/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5040 - acc: 0.8399 - val_loss: 0.8142 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.85445\n",
      "Epoch 251/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4974 - acc: 0.8314 - val_loss: 0.8120 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.85445\n",
      "Epoch 252/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5180 - acc: 0.8275 - val_loss: 0.8078 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.85445\n",
      "Epoch 253/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5058 - acc: 0.8367 - val_loss: 0.8132 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.85445\n",
      "Epoch 254/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5164 - acc: 0.8290 - val_loss: 0.8184 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.85445\n",
      "Epoch 255/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4977 - acc: 0.8370 - val_loss: 0.8161 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.85445\n",
      "Epoch 256/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4842 - acc: 0.8358 - val_loss: 0.8191 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.85445\n",
      "Epoch 257/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5013 - acc: 0.8361 - val_loss: 0.8236 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.85445\n",
      "Epoch 258/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5058 - acc: 0.8379 - val_loss: 0.8182 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.85445\n",
      "Epoch 259/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4971 - acc: 0.8399 - val_loss: 0.8171 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.85445\n",
      "Epoch 260/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5046 - acc: 0.8370 - val_loss: 0.8168 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.85445\n",
      "Epoch 261/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5162 - acc: 0.8311 - val_loss: 0.8182 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.85445\n",
      "Epoch 262/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5065 - acc: 0.8258 - val_loss: 0.8178 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.85445\n",
      "Epoch 263/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4952 - acc: 0.8287 - val_loss: 0.8175 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.85445\n",
      "Epoch 264/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5086 - acc: 0.8302 - val_loss: 0.8108 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.85445\n",
      "Epoch 265/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4823 - acc: 0.8358 - val_loss: 0.8169 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.85445\n",
      "Epoch 266/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4907 - acc: 0.8349 - val_loss: 0.8120 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.85445\n",
      "Epoch 267/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5152 - acc: 0.8314 - val_loss: 0.8132 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.85445\n",
      "Epoch 268/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4992 - acc: 0.8346 - val_loss: 0.8127 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.85445\n",
      "Epoch 269/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4962 - acc: 0.8290 - val_loss: 0.8116 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.85445\n",
      "Epoch 270/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4906 - acc: 0.8311 - val_loss: 0.8099 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.85445\n",
      "Epoch 271/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4947 - acc: 0.8420 - val_loss: 0.8158 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.85445\n",
      "Epoch 272/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4924 - acc: 0.8411 - val_loss: 0.8142 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.85445\n",
      "Epoch 273/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5012 - acc: 0.8331 - val_loss: 0.8147 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.85445\n",
      "Epoch 274/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4950 - acc: 0.8320 - val_loss: 0.8126 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.85445\n",
      "Epoch 275/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4794 - acc: 0.8352 - val_loss: 0.8102 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.85445\n",
      "Epoch 276/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5010 - acc: 0.8328 - val_loss: 0.8113 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.85445\n",
      "Epoch 277/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5276 - acc: 0.8252 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.85445\n",
      "Epoch 278/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5161 - acc: 0.8190 - val_loss: 0.8105 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.85445\n",
      "Epoch 279/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5036 - acc: 0.8246 - val_loss: 0.8110 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.85445\n",
      "Epoch 280/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5165 - acc: 0.8228 - val_loss: 0.8097 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.85445\n",
      "Epoch 281/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8284 - val_loss: 0.8120 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.85445\n",
      "Epoch 282/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5060 - acc: 0.8275 - val_loss: 0.8101 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.85445\n",
      "Epoch 283/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5008 - acc: 0.8367 - val_loss: 0.8145 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.85445\n",
      "Epoch 284/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5039 - acc: 0.8281 - val_loss: 0.8072 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.85445\n",
      "Epoch 285/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5009 - acc: 0.8281 - val_loss: 0.8067 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.85445\n",
      "Epoch 286/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4932 - acc: 0.8361 - val_loss: 0.8083 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.85445\n",
      "Epoch 287/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5094 - acc: 0.8352 - val_loss: 0.8118 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.85445\n",
      "Epoch 288/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4743 - acc: 0.8408 - val_loss: 0.8090 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.85445\n",
      "Epoch 289/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4773 - acc: 0.8482 - val_loss: 0.8094 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.85445\n",
      "Epoch 290/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4995 - acc: 0.8317 - val_loss: 0.8095 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.85445\n",
      "Epoch 291/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5083 - acc: 0.8340 - val_loss: 0.8130 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.85445\n",
      "Epoch 292/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5038 - acc: 0.8281 - val_loss: 0.8104 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.85445\n",
      "Epoch 293/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4935 - acc: 0.8387 - val_loss: 0.8114 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.85445\n",
      "Epoch 294/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5134 - acc: 0.8302 - val_loss: 0.8151 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.85445\n",
      "Epoch 295/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4927 - acc: 0.8381 - val_loss: 0.8138 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.85445\n",
      "Epoch 296/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4838 - acc: 0.8358 - val_loss: 0.8137 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.85445\n",
      "Epoch 297/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4947 - acc: 0.8381 - val_loss: 0.8118 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.85445\n",
      "Epoch 298/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5022 - acc: 0.8323 - val_loss: 0.8118 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.85445\n",
      "Epoch 299/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5034 - acc: 0.8305 - val_loss: 0.8121 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.85445\n",
      "Epoch 300/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4832 - acc: 0.8393 - val_loss: 0.8121 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.85445\n",
      "Epoch 301/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4981 - acc: 0.8284 - val_loss: 0.8150 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.85445\n",
      "Epoch 302/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4847 - acc: 0.8317 - val_loss: 0.8119 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.85445\n",
      "Epoch 303/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5014 - acc: 0.8325 - val_loss: 0.8066 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.85445\n",
      "Epoch 304/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5332 - acc: 0.8267 - val_loss: 0.8082 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.85445\n",
      "Epoch 305/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4978 - acc: 0.8320 - val_loss: 0.8099 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.85445\n",
      "Epoch 306/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4953 - acc: 0.8396 - val_loss: 0.8202 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.85445\n",
      "Epoch 307/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5144 - acc: 0.8349 - val_loss: 0.8135 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.85445\n",
      "Epoch 308/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4962 - acc: 0.8343 - val_loss: 0.8136 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.85445\n",
      "Epoch 309/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4929 - acc: 0.8355 - val_loss: 0.8132 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.85445\n",
      "Epoch 310/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5007 - acc: 0.8414 - val_loss: 0.8105 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.85445\n",
      "Epoch 311/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5204 - acc: 0.8249 - val_loss: 0.8136 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.85445\n",
      "Epoch 312/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5171 - acc: 0.8228 - val_loss: 0.8130 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.85445\n",
      "Epoch 313/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5041 - acc: 0.8343 - val_loss: 0.8188 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.85445\n",
      "Epoch 314/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4986 - acc: 0.8290 - val_loss: 0.8156 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.85445\n",
      "Epoch 315/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5199 - acc: 0.8287 - val_loss: 0.8158 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.85445\n",
      "Epoch 316/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5029 - acc: 0.8264 - val_loss: 0.8148 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.85445\n",
      "Epoch 317/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5184 - acc: 0.8396 - val_loss: 0.8145 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.85445\n",
      "Epoch 318/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4892 - acc: 0.8396 - val_loss: 0.8151 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.85445\n",
      "Epoch 319/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4869 - acc: 0.8376 - val_loss: 0.8157 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.85445\n",
      "Epoch 320/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4926 - acc: 0.8443 - val_loss: 0.8105 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.85445\n",
      "Epoch 321/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4772 - acc: 0.8317 - val_loss: 0.8163 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.85445\n",
      "Epoch 322/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4757 - acc: 0.8340 - val_loss: 0.8138 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.85445\n",
      "Epoch 323/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5112 - acc: 0.8387 - val_loss: 0.8141 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.85445\n",
      "Epoch 324/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4883 - acc: 0.8358 - val_loss: 0.8118 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.85445\n",
      "Epoch 325/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4741 - acc: 0.8390 - val_loss: 0.8147 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.85445\n",
      "Epoch 326/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5240 - acc: 0.8367 - val_loss: 0.8180 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.85445\n",
      "Epoch 327/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4927 - acc: 0.8370 - val_loss: 0.8147 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.85445\n",
      "Epoch 328/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4863 - acc: 0.8284 - val_loss: 0.8167 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.85445\n",
      "Epoch 329/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5012 - acc: 0.8343 - val_loss: 0.8128 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.85445\n",
      "Epoch 330/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5116 - acc: 0.8393 - val_loss: 0.8127 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.85445\n",
      "Epoch 331/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4851 - acc: 0.8420 - val_loss: 0.8133 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.85445\n",
      "Epoch 332/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4812 - acc: 0.8420 - val_loss: 0.8128 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.85445\n",
      "Epoch 333/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5152 - acc: 0.8349 - val_loss: 0.8118 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.85445\n",
      "Epoch 334/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4966 - acc: 0.8408 - val_loss: 0.8116 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.85445\n",
      "Epoch 335/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4889 - acc: 0.8387 - val_loss: 0.8161 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.85445\n",
      "Epoch 336/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5180 - acc: 0.8193 - val_loss: 0.8149 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.85445\n",
      "Epoch 337/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5106 - acc: 0.8296 - val_loss: 0.8138 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.85445\n",
      "Epoch 338/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5143 - acc: 0.8352 - val_loss: 0.8162 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.85445\n",
      "Epoch 339/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5143 - acc: 0.8281 - val_loss: 0.8126 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.85445\n",
      "Epoch 340/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4981 - acc: 0.8387 - val_loss: 0.8156 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.85445\n",
      "Epoch 341/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4970 - acc: 0.8346 - val_loss: 0.8131 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.85445\n",
      "Epoch 342/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4677 - acc: 0.8373 - val_loss: 0.8145 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.85445\n",
      "Epoch 343/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5342 - acc: 0.8287 - val_loss: 0.8166 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.85445\n",
      "Epoch 344/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4547 - acc: 0.8414 - val_loss: 0.8141 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.85445\n",
      "Epoch 345/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4948 - acc: 0.8370 - val_loss: 0.8173 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.85445\n",
      "Epoch 346/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4853 - acc: 0.8352 - val_loss: 0.8153 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.85445\n",
      "Epoch 347/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4957 - acc: 0.8532 - val_loss: 0.8181 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.85445\n",
      "Epoch 348/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4950 - acc: 0.8290 - val_loss: 0.8157 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.85445\n",
      "Epoch 349/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4917 - acc: 0.8479 - val_loss: 0.8161 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.85445\n",
      "Epoch 350/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8381 - val_loss: 0.8182 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.85445\n",
      "Epoch 351/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4680 - acc: 0.8470 - val_loss: 0.8162 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.85445\n",
      "Epoch 352/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5024 - acc: 0.8252 - val_loss: 0.8195 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.85445\n",
      "Epoch 353/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4864 - acc: 0.8429 - val_loss: 0.8162 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.85445\n",
      "Epoch 354/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4821 - acc: 0.8381 - val_loss: 0.8155 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.85445\n",
      "Epoch 355/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5151 - acc: 0.8349 - val_loss: 0.8136 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.85445\n",
      "Epoch 356/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4817 - acc: 0.8287 - val_loss: 0.8149 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.85445\n",
      "Epoch 357/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4932 - acc: 0.8331 - val_loss: 0.8148 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.85445\n",
      "Epoch 358/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4661 - acc: 0.8367 - val_loss: 0.8197 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.85445\n",
      "Epoch 359/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4898 - acc: 0.8390 - val_loss: 0.8174 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.85445\n",
      "Epoch 360/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4714 - acc: 0.8461 - val_loss: 0.8199 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.85445\n",
      "Epoch 361/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5004 - acc: 0.8381 - val_loss: 0.8195 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.85445\n",
      "Epoch 362/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4915 - acc: 0.8296 - val_loss: 0.8198 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.85445\n",
      "Epoch 363/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4769 - acc: 0.8364 - val_loss: 0.8201 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.85445\n",
      "Epoch 364/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4905 - acc: 0.8361 - val_loss: 0.8194 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.85445\n",
      "Epoch 365/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5106 - acc: 0.8373 - val_loss: 0.8139 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.85445\n",
      "Epoch 366/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4840 - acc: 0.8352 - val_loss: 0.8162 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.85445\n",
      "Epoch 367/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4942 - acc: 0.8331 - val_loss: 0.8184 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.85445\n",
      "Epoch 368/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8340 - val_loss: 0.8175 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.85445\n",
      "Epoch 369/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4753 - acc: 0.8305 - val_loss: 0.8171 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.85445\n",
      "Epoch 370/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5043 - acc: 0.8343 - val_loss: 0.8192 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.85445\n",
      "Epoch 371/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5166 - acc: 0.8255 - val_loss: 0.8197 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.85445\n",
      "Epoch 372/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4973 - acc: 0.8267 - val_loss: 0.8217 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.85445\n",
      "Epoch 373/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4957 - acc: 0.8355 - val_loss: 0.8188 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.85445\n",
      "Epoch 374/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4960 - acc: 0.8393 - val_loss: 0.8150 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.85445\n",
      "Epoch 375/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4839 - acc: 0.8399 - val_loss: 0.8165 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.85445\n",
      "Epoch 376/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4871 - acc: 0.8370 - val_loss: 0.8160 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.85445\n",
      "Epoch 377/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4616 - acc: 0.8470 - val_loss: 0.8152 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.85445\n",
      "Epoch 378/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4763 - acc: 0.8432 - val_loss: 0.8178 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.85445\n",
      "Epoch 379/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4751 - acc: 0.8361 - val_loss: 0.8124 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.85445\n",
      "Epoch 380/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4769 - acc: 0.8384 - val_loss: 0.8129 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.85445\n",
      "Epoch 381/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4920 - acc: 0.8328 - val_loss: 0.8129 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.85445\n",
      "Epoch 382/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5108 - acc: 0.8305 - val_loss: 0.8112 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.85445\n",
      "Epoch 383/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4892 - acc: 0.8323 - val_loss: 0.8123 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.85445\n",
      "Epoch 384/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4869 - acc: 0.8405 - val_loss: 0.8151 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.85445\n",
      "Epoch 385/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5003 - acc: 0.8272 - val_loss: 0.8131 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.85445\n",
      "Epoch 386/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4999 - acc: 0.8417 - val_loss: 0.8174 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.85445\n",
      "Epoch 387/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4979 - acc: 0.8340 - val_loss: 0.8158 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.85445\n",
      "Epoch 388/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4748 - acc: 0.8452 - val_loss: 0.8090 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.85445\n",
      "Epoch 389/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4871 - acc: 0.8278 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.85445\n",
      "Epoch 390/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5047 - acc: 0.8334 - val_loss: 0.8143 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.85445\n",
      "Epoch 391/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4562 - acc: 0.8367 - val_loss: 0.8192 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.85445\n",
      "Epoch 392/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4790 - acc: 0.8346 - val_loss: 0.8159 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.85445\n",
      "Epoch 393/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4935 - acc: 0.8278 - val_loss: 0.8120 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.85445\n",
      "Epoch 394/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4966 - acc: 0.8414 - val_loss: 0.8121 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.85445\n",
      "Epoch 395/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4989 - acc: 0.8349 - val_loss: 0.8136 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.85445\n",
      "Epoch 396/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4792 - acc: 0.8384 - val_loss: 0.8101 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.85445\n",
      "Epoch 397/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4743 - acc: 0.8370 - val_loss: 0.8174 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.85445\n",
      "Epoch 398/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4834 - acc: 0.8296 - val_loss: 0.8178 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.85445\n",
      "Epoch 399/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4806 - acc: 0.8328 - val_loss: 0.8137 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.85445\n",
      "Epoch 400/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4954 - acc: 0.8402 - val_loss: 0.8123 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.85445\n",
      "Epoch 401/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4965 - acc: 0.8252 - val_loss: 0.8127 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.85445\n",
      "Epoch 402/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.4890 - acc: 0.8408 - val_loss: 0.8169 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.85445\n",
      "Epoch 403/3000\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 1.5033 - acc: 0.8213 - val_loss: 0.8140 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.85445\n",
      "Epoch 00403: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold7_mfcc7===\n",
      "using resnet model: 3\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 30, 130, 64)  3200        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 30, 130, 64)  256         conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 30, 130, 64)  0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 15, 65, 64)   0           activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 15, 65, 64)   256         conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 15, 65, 64)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 15, 65, 64)   36928       activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 15, 65, 64)   256         conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 15, 65, 64)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 15, 65, 256)  16640       activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 15, 65, 256)  0           conv2d_327[0][0]                 \n",
      "                                                                 conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 15, 65, 256)  1024        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 15, 65, 256)  0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 15, 65, 64)   16448       activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 15, 65, 64)   256         conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 15, 65, 64)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 15, 65, 64)   36928       activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 15, 65, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 15, 65, 64)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 15, 65, 256)  16640       activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 15, 65, 256)  0           add_114[0][0]                    \n",
      "                                                                 conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 15, 65, 256)  1024        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 15, 65, 256)  0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 15, 65, 64)   16448       activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 15, 65, 64)   256         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 15, 65, 64)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 15, 65, 64)   36928       activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 15, 65, 64)   256         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 15, 65, 64)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 15, 65, 256)  16640       activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 15, 65, 256)  0           add_115[0][0]                    \n",
      "                                                                 conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 15, 65, 256)  1024        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 15, 65, 256)  0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 8, 33, 128)   32896       activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 8, 33, 128)   512         conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 8, 33, 128)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 8, 33, 128)   147584      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 8, 33, 128)   512         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 8, 33, 128)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 8, 33, 512)   131584      add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 8, 33, 512)   66048       activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 8, 33, 512)   0           conv2d_337[0][0]                 \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 8, 33, 512)   2048        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 8, 33, 512)   0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 8, 33, 128)   65664       activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 8, 33, 128)   512         conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 8, 33, 128)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 8, 33, 128)   147584      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 8, 33, 128)   512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 8, 33, 128)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 8, 33, 512)   66048       activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 8, 33, 512)   0           add_117[0][0]                    \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 8, 33, 512)   2048        add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 8, 33, 512)   0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 8, 33, 128)   65664       activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 8, 33, 128)   512         conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 8, 33, 128)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 8, 33, 128)   147584      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 8, 33, 128)   512         conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 8, 33, 128)   0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 8, 33, 512)   66048       activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 8, 33, 512)   0           add_118[0][0]                    \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 8, 33, 512)   2048        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 8, 33, 512)   0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 8, 33, 128)   65664       activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 8, 33, 128)   512         conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 8, 33, 128)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 8, 33, 128)   147584      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 8, 33, 128)   512         conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 8, 33, 128)   0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 8, 33, 512)   66048       activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 8, 33, 512)   0           add_119[0][0]                    \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 8, 33, 512)   2048        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 8, 33, 512)   0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 17, 256)   131328      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 17, 256)   1024        conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 17, 256)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 17, 256)   590080      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 17, 256)   1024        conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 17, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 17, 1024)  525312      add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 17, 1024)  263168      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 4, 17, 1024)  0           conv2d_350[0][0]                 \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 17, 1024)  4096        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 17, 256)   262400      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 17, 256)   1024        conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 17, 256)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 17, 256)   590080      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 17, 256)   1024        conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 17, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 17, 1024)  263168      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 4, 17, 1024)  0           add_121[0][0]                    \n",
      "                                                                 conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 17, 1024)  4096        add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 17, 256)   262400      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 17, 256)   1024        conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 17, 256)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 17, 256)   590080      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 17, 256)   1024        conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 17, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 17, 1024)  263168      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 4, 17, 1024)  0           add_122[0][0]                    \n",
      "                                                                 conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 17, 1024)  4096        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 17, 256)   262400      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 17, 256)   1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 17, 256)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 17, 256)   590080      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 17, 256)   1024        conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 17, 256)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 4, 17, 1024)  263168      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_124 (Add)                   (None, 4, 17, 1024)  0           add_123[0][0]                    \n",
      "                                                                 conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 17, 1024)  4096        add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 4, 17, 256)   262400      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 17, 256)   1024        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 17, 256)   0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 4, 17, 256)   590080      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 17, 256)   1024        conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 17, 256)   0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 4, 17, 1024)  263168      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_125 (Add)                   (None, 4, 17, 1024)  0           add_124[0][0]                    \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 17, 1024)  4096        add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 4, 17, 256)   262400      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 17, 256)   1024        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 17, 256)   0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 4, 17, 256)   590080      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 17, 256)   1024        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 17, 256)   0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 4, 17, 1024)  263168      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_126 (Add)                   (None, 4, 17, 1024)  0           add_125[0][0]                    \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 17, 1024)  4096        add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 4, 17, 256)   262400      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 4, 17, 256)   1024        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 17, 256)   0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 4, 17, 256)   590080      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 4, 17, 256)   1024        conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 17, 256)   0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 4, 17, 1024)  263168      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_127 (Add)                   (None, 4, 17, 1024)  0           add_126[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 4, 17, 1024)  4096        add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 4, 17, 256)   262400      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 4, 17, 256)   1024        conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 4, 17, 256)   0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 4, 17, 256)   590080      activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 4, 17, 256)   1024        conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 4, 17, 256)   0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 4, 17, 1024)  263168      activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_128 (Add)                   (None, 4, 17, 1024)  0           add_127[0][0]                    \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 4, 17, 1024)  4096        add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 4, 17, 256)   262400      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 4, 17, 256)   1024        conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 4, 17, 256)   0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 4, 17, 256)   590080      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 4, 17, 256)   1024        conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 4, 17, 256)   0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 4, 17, 1024)  263168      activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_129 (Add)                   (None, 4, 17, 1024)  0           add_128[0][0]                    \n",
      "                                                                 conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 4, 17, 1024)  4096        add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 4, 17, 256)   262400      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 4, 17, 256)   1024        conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 4, 17, 256)   0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_376 (Conv2D)             (None, 4, 17, 256)   590080      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 4, 17, 256)   1024        conv2d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 4, 17, 256)   0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_377 (Conv2D)             (None, 4, 17, 1024)  263168      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_130 (Add)                   (None, 4, 17, 1024)  0           add_129[0][0]                    \n",
      "                                                                 conv2d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 4, 17, 1024)  4096        add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_378 (Conv2D)             (None, 4, 17, 256)   262400      activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 4, 17, 256)   1024        conv2d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 4, 17, 256)   0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_379 (Conv2D)             (None, 4, 17, 256)   590080      activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 4, 17, 256)   1024        conv2d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 4, 17, 256)   0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_380 (Conv2D)             (None, 4, 17, 1024)  263168      activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_131 (Add)                   (None, 4, 17, 1024)  0           add_130[0][0]                    \n",
      "                                                                 conv2d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 4, 17, 1024)  4096        add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_381 (Conv2D)             (None, 4, 17, 256)   262400      activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 4, 17, 256)   1024        conv2d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 4, 17, 256)   0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_382 (Conv2D)             (None, 4, 17, 256)   590080      activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 4, 17, 256)   1024        conv2d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 4, 17, 256)   0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_383 (Conv2D)             (None, 4, 17, 1024)  263168      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 4, 17, 1024)  0           add_131[0][0]                    \n",
      "                                                                 conv2d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 4, 17, 1024)  4096        add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_384 (Conv2D)             (None, 4, 17, 256)   262400      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 4, 17, 256)   1024        conv2d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 4, 17, 256)   0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_385 (Conv2D)             (None, 4, 17, 256)   590080      activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 4, 17, 256)   1024        conv2d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 4, 17, 256)   0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_386 (Conv2D)             (None, 4, 17, 1024)  263168      activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 4, 17, 1024)  0           add_132[0][0]                    \n",
      "                                                                 conv2d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 4, 17, 1024)  4096        add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_387 (Conv2D)             (None, 4, 17, 256)   262400      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 4, 17, 256)   1024        conv2d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 4, 17, 256)   0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_388 (Conv2D)             (None, 4, 17, 256)   590080      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 4, 17, 256)   1024        conv2d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 4, 17, 256)   0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 4, 17, 1024)  263168      activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 4, 17, 1024)  0           add_133[0][0]                    \n",
      "                                                                 conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 4, 17, 1024)  4096        add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 4, 17, 256)   262400      activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 4, 17, 256)   1024        conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 4, 17, 256)   0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 4, 17, 256)   590080      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 4, 17, 256)   1024        conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 4, 17, 256)   0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 4, 17, 1024)  263168      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 4, 17, 1024)  0           add_134[0][0]                    \n",
      "                                                                 conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 4, 17, 1024)  4096        add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 4, 17, 256)   262400      activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 4, 17, 256)   1024        conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 4, 17, 256)   0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 4, 17, 256)   590080      activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 4, 17, 256)   1024        conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 4, 17, 256)   0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 4, 17, 1024)  263168      activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 4, 17, 1024)  0           add_135[0][0]                    \n",
      "                                                                 conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 4, 17, 1024)  4096        add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 4, 17, 256)   262400      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 4, 17, 256)   1024        conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 4, 17, 256)   0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 4, 17, 256)   590080      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 4, 17, 256)   1024        conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 4, 17, 256)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 4, 17, 1024)  263168      activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 4, 17, 1024)  0           add_136[0][0]                    \n",
      "                                                                 conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 4, 17, 1024)  4096        add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 4, 17, 256)   262400      activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 4, 17, 256)   1024        conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 4, 17, 256)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 4, 17, 256)   590080      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 4, 17, 256)   1024        conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 4, 17, 256)   0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 4, 17, 1024)  263168      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 4, 17, 1024)  0           add_137[0][0]                    \n",
      "                                                                 conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 4, 17, 1024)  4096        add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 4, 17, 256)   262400      activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 4, 17, 256)   1024        conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 4, 17, 256)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 4, 17, 256)   590080      activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 4, 17, 256)   1024        conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 4, 17, 256)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 4, 17, 1024)  263168      activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 4, 17, 1024)  0           add_138[0][0]                    \n",
      "                                                                 conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 4, 17, 1024)  4096        add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 4, 17, 256)   262400      activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 4, 17, 256)   1024        conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 4, 17, 256)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 4, 17, 256)   590080      activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 4, 17, 256)   1024        conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 4, 17, 256)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 4, 17, 1024)  263168      activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 4, 17, 1024)  0           add_139[0][0]                    \n",
      "                                                                 conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 4, 17, 1024)  4096        add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 4, 17, 256)   262400      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 4, 17, 256)   1024        conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 4, 17, 256)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 4, 17, 256)   590080      activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 4, 17, 256)   1024        conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 4, 17, 256)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 4, 17, 1024)  263168      activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 4, 17, 1024)  0           add_140[0][0]                    \n",
      "                                                                 conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 4, 17, 1024)  4096        add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 4, 17, 256)   262400      activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 4, 17, 256)   1024        conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 4, 17, 256)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 4, 17, 256)   590080      activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 4, 17, 256)   1024        conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 4, 17, 256)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 4, 17, 1024)  263168      activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 4, 17, 1024)  0           add_141[0][0]                    \n",
      "                                                                 conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 4, 17, 1024)  4096        add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 4, 17, 256)   262400      activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 4, 17, 256)   1024        conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 4, 17, 256)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 4, 17, 256)   590080      activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 4, 17, 256)   1024        conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 4, 17, 256)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 4, 17, 1024)  263168      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 4, 17, 1024)  0           add_142[0][0]                    \n",
      "                                                                 conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 4, 17, 1024)  4096        add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 2, 9, 512)    524800      activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 2, 9, 512)    2048        conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 2, 9, 512)    0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 2, 9, 512)    2359808     activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 2, 9, 512)    2048        conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 2, 9, 512)    0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 2, 9, 2048)   2099200     add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 2, 9, 2048)   0           conv2d_420[0][0]                 \n",
      "                                                                 conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 2, 9, 2048)   8192        add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 2, 9, 512)    1049088     activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 2, 9, 512)    2048        conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 2, 9, 512)    0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 2, 9, 512)    2359808     activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 2, 9, 512)    2048        conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 2, 9, 512)    0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 2, 9, 2048)   0           add_144[0][0]                    \n",
      "                                                                 conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 2, 9, 2048)   8192        add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 2, 9, 512)    1049088     activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 2, 9, 512)    2048        conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 2, 9, 512)    0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 2, 9, 512)    2359808     activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 2, 9, 512)    2048        conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 2, 9, 512)    0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 2, 9, 2048)   0           add_145[0][0]                    \n",
      "                                                                 conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 2, 9, 2048)   8192        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1, 1, 2048)   0           activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 44)           90156       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 44)           176         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 44)           0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 41)           1845        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,728,721\n",
      "Trainable params: 42,630,969\n",
      "Non-trainable params: 97,752\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "104/104 [==============================] - 31s 298ms/step - loss: 13.1943 - acc: 0.0484 - val_loss: 11.3525 - val_acc: 0.0431\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.04313, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 2/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 9.7494 - acc: 0.0754 - val_loss: 8.6106 - val_acc: 0.0997\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.04313 to 0.09973, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 3/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 7.4657 - acc: 0.0916 - val_loss: 6.6357 - val_acc: 0.0916\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.09973\n",
      "Epoch 4/3000\n",
      "104/104 [==============================] - 19s 181ms/step - loss: 6.0615 - acc: 0.0944 - val_loss: 5.5426 - val_acc: 0.1105\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.09973 to 0.11051, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 5.2186 - acc: 0.1220 - val_loss: 5.3015 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.11051 to 0.12938, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 6/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.6955 - acc: 0.1268 - val_loss: 4.6258 - val_acc: 0.1078\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.12938\n",
      "Epoch 7/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.2841 - acc: 0.1550 - val_loss: 4.5833 - val_acc: 0.1051\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.12938\n",
      "Epoch 8/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 4.0487 - acc: 0.1617 - val_loss: 3.6932 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.12938 to 0.16442, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 9/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.8751 - acc: 0.1617 - val_loss: 3.9628 - val_acc: 0.1806\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.16442 to 0.18059, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 10/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.7542 - acc: 0.1797 - val_loss: 3.0725 - val_acc: 0.2318\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.18059 to 0.23181, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 11/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.5998 - acc: 0.1914 - val_loss: 4.1241 - val_acc: 0.1590\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.23181\n",
      "Epoch 12/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.5287 - acc: 0.1977 - val_loss: 2.9334 - val_acc: 0.3019\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.23181 to 0.30189, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 13/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.4585 - acc: 0.2073 - val_loss: 3.5923 - val_acc: 0.1644\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.30189\n",
      "Epoch 14/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.4016 - acc: 0.2163 - val_loss: 4.3131 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.30189\n",
      "Epoch 15/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.3480 - acc: 0.2344 - val_loss: 3.2426 - val_acc: 0.2210\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.30189\n",
      "Epoch 16/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.3182 - acc: 0.2200 - val_loss: 3.4447 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.30189\n",
      "Epoch 17/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.2691 - acc: 0.2434 - val_loss: 2.9332 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.30189\n",
      "Epoch 18/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.2235 - acc: 0.2584 - val_loss: 3.0088 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.30189\n",
      "Epoch 19/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 3.1939 - acc: 0.2533 - val_loss: 3.1747 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.30189\n",
      "Epoch 20/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1735 - acc: 0.2647 - val_loss: 2.8285 - val_acc: 0.3046\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.30189 to 0.30458, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 21/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1450 - acc: 0.2803 - val_loss: 2.5112 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.30458 to 0.32615, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 22/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.1205 - acc: 0.2891 - val_loss: 3.3427 - val_acc: 0.2237\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.32615\n",
      "Epoch 23/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0949 - acc: 0.2825 - val_loss: 2.4923 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.32615 to 0.32884, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 24/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0751 - acc: 0.2897 - val_loss: 3.5520 - val_acc: 0.1887\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.32884\n",
      "Epoch 25/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0454 - acc: 0.3032 - val_loss: 2.4403 - val_acc: 0.3693\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.32884 to 0.36927, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 26/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0477 - acc: 0.2930 - val_loss: 2.4589 - val_acc: 0.3477\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.36927\n",
      "Epoch 27/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0407 - acc: 0.3011 - val_loss: 2.3740 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.36927 to 0.38814, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 28/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 3.0432 - acc: 0.3032 - val_loss: 2.7198 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.38814\n",
      "Epoch 29/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9989 - acc: 0.3200 - val_loss: 2.2128 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.38814 to 0.42318, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 30/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9572 - acc: 0.3272 - val_loss: 3.0081 - val_acc: 0.2642\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.42318\n",
      "Epoch 31/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9482 - acc: 0.3459 - val_loss: 2.2337 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.42318\n",
      "Epoch 32/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9767 - acc: 0.3266 - val_loss: 2.5531 - val_acc: 0.2992\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.42318\n",
      "Epoch 33/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9355 - acc: 0.3314 - val_loss: 2.5346 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.42318\n",
      "Epoch 34/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9650 - acc: 0.3287 - val_loss: 2.5664 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.42318\n",
      "Epoch 35/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9378 - acc: 0.3438 - val_loss: 2.5534 - val_acc: 0.3531\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.42318\n",
      "Epoch 36/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8932 - acc: 0.3477 - val_loss: 2.4106 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.42318\n",
      "Epoch 37/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.9197 - acc: 0.3398 - val_loss: 2.4898 - val_acc: 0.3396\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.42318\n",
      "Epoch 38/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8974 - acc: 0.3480 - val_loss: 3.3308 - val_acc: 0.2534\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.42318\n",
      "Epoch 39/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8935 - acc: 0.3498 - val_loss: 2.6245 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.42318\n",
      "Epoch 40/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8556 - acc: 0.3666 - val_loss: 2.2348 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.42318 to 0.43396, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 41/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8504 - acc: 0.3609 - val_loss: 2.4649 - val_acc: 0.3181\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43396\n",
      "Epoch 42/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8432 - acc: 0.3750 - val_loss: 2.2757 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.43396\n",
      "Epoch 43/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.8793 - acc: 0.3579 - val_loss: 2.3172 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.43396\n",
      "Epoch 44/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8708 - acc: 0.3675 - val_loss: 2.1280 - val_acc: 0.4178\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.43396\n",
      "Epoch 45/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8275 - acc: 0.3792 - val_loss: 2.2173 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.43396\n",
      "Epoch 46/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.8328 - acc: 0.3768 - val_loss: 2.3854 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.43396\n",
      "Epoch 47/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7927 - acc: 0.3867 - val_loss: 2.2213 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.43396\n",
      "Epoch 48/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7837 - acc: 0.3888 - val_loss: 2.4305 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.43396\n",
      "Epoch 49/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.8051 - acc: 0.3909 - val_loss: 2.3590 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.43396\n",
      "Epoch 50/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7928 - acc: 0.3912 - val_loss: 2.3144 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.43396\n",
      "Epoch 51/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7991 - acc: 0.3915 - val_loss: 2.3940 - val_acc: 0.3666\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.43396\n",
      "Epoch 52/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7505 - acc: 0.4099 - val_loss: 1.9444 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.43396 to 0.50404, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 53/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7577 - acc: 0.3966 - val_loss: 2.4931 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.50404\n",
      "Epoch 54/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7224 - acc: 0.4207 - val_loss: 2.0324 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.50404 to 0.52291, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 55/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7621 - acc: 0.3975 - val_loss: 1.9603 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.52291\n",
      "Epoch 56/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7663 - acc: 0.4041 - val_loss: 2.2100 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.52291\n",
      "Epoch 57/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7498 - acc: 0.4090 - val_loss: 2.2188 - val_acc: 0.4124\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.52291\n",
      "Epoch 58/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7370 - acc: 0.4216 - val_loss: 2.2137 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.52291\n",
      "Epoch 59/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7362 - acc: 0.4306 - val_loss: 2.2040 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.52291\n",
      "Epoch 60/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7067 - acc: 0.4213 - val_loss: 5.2504 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.52291\n",
      "Epoch 61/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7194 - acc: 0.4105 - val_loss: 1.8890 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.52291\n",
      "Epoch 62/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7299 - acc: 0.4141 - val_loss: 2.5465 - val_acc: 0.3774\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.52291\n",
      "Epoch 63/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6966 - acc: 0.4273 - val_loss: 3.2679 - val_acc: 0.2399\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.52291\n",
      "Epoch 64/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6850 - acc: 0.4336 - val_loss: 1.7087 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.52291 to 0.55526, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 65/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6783 - acc: 0.4279 - val_loss: 2.4578 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.55526\n",
      "Epoch 66/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6587 - acc: 0.4375 - val_loss: 2.1841 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.55526\n",
      "Epoch 67/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.7005 - acc: 0.4264 - val_loss: 3.0916 - val_acc: 0.2507\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.55526\n",
      "Epoch 68/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.7099 - acc: 0.4300 - val_loss: 2.0903 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.55526\n",
      "Epoch 69/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6702 - acc: 0.4414 - val_loss: 2.3438 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.55526\n",
      "Epoch 70/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6685 - acc: 0.4483 - val_loss: 2.8562 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.55526\n",
      "Epoch 71/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6578 - acc: 0.4399 - val_loss: 1.9720 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.55526\n",
      "Epoch 72/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6730 - acc: 0.4429 - val_loss: 2.6241 - val_acc: 0.3774\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.55526\n",
      "Epoch 73/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6528 - acc: 0.4327 - val_loss: 2.0396 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.55526\n",
      "Epoch 74/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6283 - acc: 0.4597 - val_loss: 1.9986 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.55526\n",
      "Epoch 75/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6365 - acc: 0.4525 - val_loss: 1.8476 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.55526\n",
      "Epoch 76/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6087 - acc: 0.4567 - val_loss: 2.4647 - val_acc: 0.4097\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.55526\n",
      "Epoch 77/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6255 - acc: 0.4567 - val_loss: 1.8684 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.55526\n",
      "Epoch 78/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6284 - acc: 0.4606 - val_loss: 2.1464 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.55526\n",
      "Epoch 79/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5915 - acc: 0.4606 - val_loss: 1.9847 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.55526\n",
      "Epoch 80/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5715 - acc: 0.4724 - val_loss: 2.0777 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.55526\n",
      "Epoch 81/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6160 - acc: 0.4426 - val_loss: 1.7370 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.55526 to 0.56065, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6056 - acc: 0.4678 - val_loss: 2.1143 - val_acc: 0.4636\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.56065\n",
      "Epoch 83/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5753 - acc: 0.4663 - val_loss: 2.0739 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.56065\n",
      "Epoch 84/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.6193 - acc: 0.4627 - val_loss: 1.8494 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.56065\n",
      "Epoch 85/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5895 - acc: 0.4709 - val_loss: 1.8383 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.56065\n",
      "Epoch 86/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.6096 - acc: 0.4748 - val_loss: 1.7248 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.56065 to 0.57143, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 87/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5781 - acc: 0.4748 - val_loss: 1.7320 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.57143\n",
      "Epoch 88/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5714 - acc: 0.4856 - val_loss: 2.2558 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.57143\n",
      "Epoch 89/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5453 - acc: 0.4919 - val_loss: 2.2484 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.57143\n",
      "Epoch 90/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5316 - acc: 0.4910 - val_loss: 1.8279 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.57143\n",
      "Epoch 91/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5520 - acc: 0.4811 - val_loss: 1.9488 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.57143\n",
      "Epoch 92/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5546 - acc: 0.4775 - val_loss: 2.1518 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.57143\n",
      "Epoch 93/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5626 - acc: 0.4787 - val_loss: 2.0986 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.57143\n",
      "Epoch 94/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5196 - acc: 0.5027 - val_loss: 1.8898 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.57143\n",
      "Epoch 95/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.5557 - acc: 0.4736 - val_loss: 1.7539 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.57143\n",
      "Epoch 96/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5059 - acc: 0.4940 - val_loss: 1.6600 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.57143 to 0.59299, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 97/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5025 - acc: 0.5075 - val_loss: 1.9041 - val_acc: 0.5121\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.59299\n",
      "Epoch 98/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5244 - acc: 0.5090 - val_loss: 1.7818 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.59299\n",
      "Epoch 99/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5144 - acc: 0.4790 - val_loss: 1.9934 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.59299\n",
      "Epoch 100/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4991 - acc: 0.5036 - val_loss: 1.7473 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.59299\n",
      "Epoch 101/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4840 - acc: 0.5012 - val_loss: 2.0101 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.59299\n",
      "Epoch 102/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4865 - acc: 0.5153 - val_loss: 1.9734 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.59299\n",
      "Epoch 103/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4983 - acc: 0.4976 - val_loss: 1.7409 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.59299\n",
      "Epoch 104/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.5393 - acc: 0.4955 - val_loss: 1.7462 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.59299\n",
      "Epoch 105/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4853 - acc: 0.4958 - val_loss: 1.7381 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.59299\n",
      "Epoch 106/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4769 - acc: 0.5144 - val_loss: 2.1086 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.59299\n",
      "Epoch 107/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4683 - acc: 0.5090 - val_loss: 1.6933 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.59299\n",
      "Epoch 108/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4817 - acc: 0.5072 - val_loss: 1.9889 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.59299\n",
      "Epoch 109/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4998 - acc: 0.4946 - val_loss: 1.9330 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.59299\n",
      "Epoch 110/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4809 - acc: 0.5192 - val_loss: 1.6030 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00110: val_acc improved from 0.59299 to 0.60108, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 111/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4934 - acc: 0.5042 - val_loss: 2.1887 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.60108\n",
      "Epoch 112/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4638 - acc: 0.5084 - val_loss: 1.7815 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.60108\n",
      "Epoch 113/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4748 - acc: 0.5186 - val_loss: 1.5878 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.60108\n",
      "Epoch 114/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4155 - acc: 0.5331 - val_loss: 1.6846 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.60108\n",
      "Epoch 115/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4660 - acc: 0.5231 - val_loss: 2.2268 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.60108\n",
      "Epoch 116/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4300 - acc: 0.5204 - val_loss: 1.5104 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.60108 to 0.62534, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 117/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4558 - acc: 0.5171 - val_loss: 1.5849 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.62534\n",
      "Epoch 118/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4350 - acc: 0.5204 - val_loss: 2.2850 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.62534\n",
      "Epoch 119/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4700 - acc: 0.5171 - val_loss: 1.6191 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.62534\n",
      "Epoch 120/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4620 - acc: 0.5081 - val_loss: 1.7853 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.62534\n",
      "Epoch 121/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4300 - acc: 0.5418 - val_loss: 1.8643 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.62534\n",
      "Epoch 122/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4117 - acc: 0.5288 - val_loss: 1.3970 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00122: val_acc improved from 0.62534 to 0.66038, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 123/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4082 - acc: 0.5303 - val_loss: 1.8739 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.66038\n",
      "Epoch 124/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4262 - acc: 0.5189 - val_loss: 1.7739 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.66038\n",
      "Epoch 125/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4064 - acc: 0.5319 - val_loss: 1.7581 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.66038\n",
      "Epoch 126/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4287 - acc: 0.5346 - val_loss: 1.7310 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.66038\n",
      "Epoch 127/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4083 - acc: 0.5337 - val_loss: 1.6133 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.66038\n",
      "Epoch 128/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3811 - acc: 0.5439 - val_loss: 1.6363 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.66038\n",
      "Epoch 129/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4202 - acc: 0.5334 - val_loss: 1.8329 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.66038\n",
      "Epoch 130/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4129 - acc: 0.5319 - val_loss: 2.0886 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.66038\n",
      "Epoch 131/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.4248 - acc: 0.5343 - val_loss: 1.8967 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.66038\n",
      "Epoch 132/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4083 - acc: 0.5346 - val_loss: 1.7967 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.66038\n",
      "Epoch 133/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3871 - acc: 0.5385 - val_loss: 1.5420 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.66038\n",
      "Epoch 134/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3852 - acc: 0.5466 - val_loss: 1.4599 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.66038\n",
      "Epoch 135/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3939 - acc: 0.5457 - val_loss: 1.8613 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.66038\n",
      "Epoch 136/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3714 - acc: 0.5550 - val_loss: 1.6268 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.66038\n",
      "Epoch 137/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4009 - acc: 0.5355 - val_loss: 1.4932 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.66038\n",
      "Epoch 138/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3729 - acc: 0.5616 - val_loss: 1.4487 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.66038\n",
      "Epoch 139/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3796 - acc: 0.5607 - val_loss: 1.8176 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.66038\n",
      "Epoch 140/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3863 - acc: 0.5343 - val_loss: 1.8302 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.66038\n",
      "Epoch 141/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3615 - acc: 0.5490 - val_loss: 1.5666 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.66038\n",
      "Epoch 142/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3443 - acc: 0.5619 - val_loss: 1.4744 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.66038\n",
      "Epoch 143/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3501 - acc: 0.5640 - val_loss: 1.4659 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.66038\n",
      "Epoch 144/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3705 - acc: 0.5577 - val_loss: 1.6552 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.66038\n",
      "Epoch 145/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.4220 - acc: 0.5427 - val_loss: 1.4871 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.66038\n",
      "Epoch 146/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3787 - acc: 0.5376 - val_loss: 1.6268 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.66038\n",
      "Epoch 147/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3913 - acc: 0.5499 - val_loss: 2.0666 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.66038\n",
      "Epoch 148/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3403 - acc: 0.5646 - val_loss: 1.4627 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.66038\n",
      "Epoch 149/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3638 - acc: 0.5541 - val_loss: 1.5313 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.66038\n",
      "Epoch 150/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3268 - acc: 0.5634 - val_loss: 1.4008 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.66038\n",
      "Epoch 151/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3591 - acc: 0.5544 - val_loss: 1.7607 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.66038\n",
      "Epoch 152/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3423 - acc: 0.5739 - val_loss: 1.4763 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.66038\n",
      "Epoch 153/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3469 - acc: 0.5529 - val_loss: 1.4805 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.66038\n",
      "Epoch 154/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3717 - acc: 0.5514 - val_loss: 1.6208 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.66038\n",
      "Epoch 155/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3445 - acc: 0.5658 - val_loss: 1.5749 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.66038\n",
      "Epoch 156/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3205 - acc: 0.5667 - val_loss: 1.8703 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.66038\n",
      "Epoch 157/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3438 - acc: 0.5637 - val_loss: 1.7493 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.66038\n",
      "Epoch 158/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3480 - acc: 0.5565 - val_loss: 1.4041 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.66038\n",
      "Epoch 159/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3457 - acc: 0.5640 - val_loss: 1.4501 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.66038\n",
      "Epoch 160/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3181 - acc: 0.5895 - val_loss: 2.0663 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.66038\n",
      "Epoch 161/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3296 - acc: 0.5688 - val_loss: 1.6512 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.66038\n",
      "Epoch 162/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3306 - acc: 0.5763 - val_loss: 2.1039 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.66038\n",
      "Epoch 163/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3383 - acc: 0.5703 - val_loss: 1.7051 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.66038\n",
      "Epoch 164/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3342 - acc: 0.5808 - val_loss: 1.9652 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.66038\n",
      "Epoch 165/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3207 - acc: 0.5865 - val_loss: 1.9980 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.66038\n",
      "Epoch 166/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3088 - acc: 0.5676 - val_loss: 1.6239 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.66038\n",
      "Epoch 167/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3280 - acc: 0.5739 - val_loss: 1.4356 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.66038\n",
      "Epoch 168/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2756 - acc: 0.5871 - val_loss: 1.5352 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.66038\n",
      "Epoch 169/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2905 - acc: 0.5980 - val_loss: 1.6908 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.66038\n",
      "Epoch 170/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2982 - acc: 0.5754 - val_loss: 1.6403 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.66038\n",
      "Epoch 171/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3172 - acc: 0.5778 - val_loss: 1.6655 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.66038\n",
      "Epoch 172/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2956 - acc: 0.5871 - val_loss: 1.6640 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.66038\n",
      "Epoch 173/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2889 - acc: 0.5838 - val_loss: 1.4811 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.66038\n",
      "Epoch 174/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.3274 - acc: 0.5730 - val_loss: 1.7159 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.66038\n",
      "Epoch 175/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2742 - acc: 0.5968 - val_loss: 1.6568 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.66038\n",
      "Epoch 176/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2929 - acc: 0.5766 - val_loss: 1.4831 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.66038\n",
      "Epoch 177/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2741 - acc: 0.5814 - val_loss: 1.6636 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.66038\n",
      "Epoch 178/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2946 - acc: 0.5895 - val_loss: 1.6160 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.66038\n",
      "Epoch 179/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3148 - acc: 0.5754 - val_loss: 1.5928 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.66038\n",
      "Epoch 180/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2807 - acc: 0.5850 - val_loss: 1.5013 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.66038\n",
      "Epoch 181/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2938 - acc: 0.5841 - val_loss: 1.8515 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.66038\n",
      "Epoch 182/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2746 - acc: 0.5838 - val_loss: 1.6513 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.66038\n",
      "Epoch 183/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2887 - acc: 0.5974 - val_loss: 1.6139 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.66038\n",
      "Epoch 184/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2796 - acc: 0.5886 - val_loss: 1.4487 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.66038\n",
      "Epoch 185/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2826 - acc: 0.5793 - val_loss: 1.6489 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.66038\n",
      "Epoch 186/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2666 - acc: 0.5814 - val_loss: 1.9146 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.66038\n",
      "Epoch 187/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2733 - acc: 0.5772 - val_loss: 1.6006 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.66038\n",
      "Epoch 188/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.3081 - acc: 0.5748 - val_loss: 1.3779 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00188: val_acc improved from 0.66038 to 0.69272, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 189/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2427 - acc: 0.6001 - val_loss: 1.7476 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.69272\n",
      "Epoch 190/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2700 - acc: 0.5895 - val_loss: 1.3794 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.69272\n",
      "Epoch 191/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2885 - acc: 0.5931 - val_loss: 1.5375 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.69272\n",
      "Epoch 192/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2559 - acc: 0.5938 - val_loss: 1.9932 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.69272\n",
      "Epoch 193/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2813 - acc: 0.5925 - val_loss: 2.2800 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.69272\n",
      "Epoch 194/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2689 - acc: 0.5856 - val_loss: 1.5674 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.69272\n",
      "Epoch 195/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2293 - acc: 0.6124 - val_loss: 1.4515 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.69272\n",
      "Epoch 196/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2456 - acc: 0.5934 - val_loss: 1.4905 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.69272\n",
      "Epoch 197/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2581 - acc: 0.5983 - val_loss: 1.4136 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.69272\n",
      "Epoch 198/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2582 - acc: 0.5862 - val_loss: 1.4729 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.69272\n",
      "Epoch 199/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2444 - acc: 0.6001 - val_loss: 1.5930 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.69272\n",
      "Epoch 200/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2151 - acc: 0.6133 - val_loss: 1.4607 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.69272\n",
      "Epoch 201/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2524 - acc: 0.6028 - val_loss: 1.7673 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.69272\n",
      "Epoch 202/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2185 - acc: 0.6088 - val_loss: 1.7777 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.69272\n",
      "Epoch 203/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2624 - acc: 0.5959 - val_loss: 1.5084 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.69272\n",
      "Epoch 204/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2716 - acc: 0.5874 - val_loss: 1.6867 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.69272\n",
      "Epoch 205/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2450 - acc: 0.6040 - val_loss: 1.7222 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.69272\n",
      "Epoch 206/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2482 - acc: 0.6055 - val_loss: 2.4478 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.69272\n",
      "Epoch 207/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2531 - acc: 0.6061 - val_loss: 2.2285 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.69272\n",
      "Epoch 208/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2344 - acc: 0.6109 - val_loss: 1.4795 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.69272\n",
      "Epoch 209/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2488 - acc: 0.6040 - val_loss: 1.7277 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.69272\n",
      "Epoch 210/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2319 - acc: 0.6073 - val_loss: 1.3944 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.69272\n",
      "Epoch 211/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2177 - acc: 0.6085 - val_loss: 1.3874 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.69272\n",
      "Epoch 212/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2306 - acc: 0.6133 - val_loss: 1.6480 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.69272\n",
      "Epoch 213/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2202 - acc: 0.6040 - val_loss: 1.5637 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.69272\n",
      "Epoch 214/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2229 - acc: 0.6130 - val_loss: 1.8806 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.69272\n",
      "Epoch 215/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2370 - acc: 0.6073 - val_loss: 1.4240 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.69272\n",
      "Epoch 216/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2225 - acc: 0.6217 - val_loss: 1.5907 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.69272\n",
      "Epoch 217/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2479 - acc: 0.6037 - val_loss: 1.9466 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.69272\n",
      "Epoch 218/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2303 - acc: 0.6172 - val_loss: 1.4360 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.69272\n",
      "Epoch 219/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2380 - acc: 0.6142 - val_loss: 1.7792 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.69272\n",
      "Epoch 220/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.2356 - acc: 0.6028 - val_loss: 1.3846 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.69272\n",
      "Epoch 221/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1890 - acc: 0.6244 - val_loss: 1.4459 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.69272\n",
      "Epoch 222/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1942 - acc: 0.6154 - val_loss: 1.6171 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.69272\n",
      "Epoch 223/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1902 - acc: 0.6190 - val_loss: 1.5379 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.69272\n",
      "Epoch 224/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1716 - acc: 0.6307 - val_loss: 3.5196 - val_acc: 0.2615\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.69272\n",
      "Epoch 225/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1891 - acc: 0.6232 - val_loss: 1.5401 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.69272\n",
      "Epoch 226/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2267 - acc: 0.6187 - val_loss: 1.9193 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.69272\n",
      "Epoch 227/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1918 - acc: 0.6265 - val_loss: 1.5637 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.69272\n",
      "Epoch 228/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1940 - acc: 0.6226 - val_loss: 1.2827 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00228: val_acc improved from 0.69272 to 0.70620, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 229/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1953 - acc: 0.6217 - val_loss: 1.5901 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.70620\n",
      "Epoch 230/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2013 - acc: 0.6301 - val_loss: 1.7231 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.70620\n",
      "Epoch 231/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2142 - acc: 0.6247 - val_loss: 1.4122 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.70620\n",
      "Epoch 232/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1904 - acc: 0.6334 - val_loss: 1.4363 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.70620\n",
      "Epoch 233/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2003 - acc: 0.6358 - val_loss: 1.5008 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.70620\n",
      "Epoch 234/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1920 - acc: 0.6277 - val_loss: 1.6115 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.70620\n",
      "Epoch 235/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1700 - acc: 0.6277 - val_loss: 1.3831 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.70620\n",
      "Epoch 236/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1889 - acc: 0.6265 - val_loss: 1.8352 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.70620\n",
      "Epoch 237/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1783 - acc: 0.6280 - val_loss: 1.3895 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.70620\n",
      "Epoch 238/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1624 - acc: 0.6238 - val_loss: 1.6835 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.70620\n",
      "Epoch 239/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1509 - acc: 0.6454 - val_loss: 1.4763 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.70620\n",
      "Epoch 240/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2083 - acc: 0.6154 - val_loss: 1.3973 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.70620\n",
      "Epoch 241/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.2013 - acc: 0.6214 - val_loss: 1.8112 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.70620\n",
      "Epoch 242/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1609 - acc: 0.6349 - val_loss: 1.4818 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.70620\n",
      "Epoch 243/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1779 - acc: 0.6277 - val_loss: 1.6564 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.70620\n",
      "Epoch 244/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1617 - acc: 0.6388 - val_loss: 1.4577 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.70620\n",
      "Epoch 245/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1935 - acc: 0.6280 - val_loss: 1.4020 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.70620\n",
      "Epoch 246/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1505 - acc: 0.6400 - val_loss: 2.1682 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.70620\n",
      "Epoch 247/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1584 - acc: 0.6352 - val_loss: 1.4912 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.70620\n",
      "Epoch 248/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1601 - acc: 0.6427 - val_loss: 1.2765 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.70620\n",
      "Epoch 249/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1818 - acc: 0.6241 - val_loss: 2.1700 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.70620\n",
      "Epoch 250/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1879 - acc: 0.6304 - val_loss: 1.7386 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.70620\n",
      "Epoch 251/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1453 - acc: 0.6361 - val_loss: 1.4749 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.70620\n",
      "Epoch 252/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1769 - acc: 0.6301 - val_loss: 1.2565 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.70620\n",
      "Epoch 253/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1739 - acc: 0.6439 - val_loss: 1.2846 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.70620\n",
      "Epoch 254/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1403 - acc: 0.6463 - val_loss: 1.5311 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.70620\n",
      "Epoch 255/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1688 - acc: 0.6292 - val_loss: 1.8313 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.70620\n",
      "Epoch 256/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1594 - acc: 0.6394 - val_loss: 1.4588 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.70620\n",
      "Epoch 257/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1392 - acc: 0.6544 - val_loss: 1.6887 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.70620\n",
      "Epoch 258/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1995 - acc: 0.6172 - val_loss: 1.5550 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.70620\n",
      "Epoch 259/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1702 - acc: 0.6418 - val_loss: 1.4449 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.70620\n",
      "Epoch 260/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1514 - acc: 0.6478 - val_loss: 1.4496 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.70620\n",
      "Epoch 261/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1435 - acc: 0.6502 - val_loss: 1.7390 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.70620\n",
      "Epoch 262/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1451 - acc: 0.6469 - val_loss: 1.3273 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.70620\n",
      "Epoch 263/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1325 - acc: 0.6304 - val_loss: 1.3496 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.70620\n",
      "Epoch 264/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1498 - acc: 0.6529 - val_loss: 1.2649 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00264: val_acc improved from 0.70620 to 0.70889, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 265/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1554 - acc: 0.6316 - val_loss: 1.7085 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.70889\n",
      "Epoch 266/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1192 - acc: 0.6418 - val_loss: 1.3505 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.70889\n",
      "Epoch 267/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1516 - acc: 0.6280 - val_loss: 1.1583 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00267: val_acc improved from 0.70889 to 0.73046, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 268/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1240 - acc: 0.6559 - val_loss: 1.3674 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.73046\n",
      "Epoch 269/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1589 - acc: 0.6514 - val_loss: 1.3288 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.73046\n",
      "Epoch 270/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1436 - acc: 0.6499 - val_loss: 1.8256 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.73046\n",
      "Epoch 271/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1430 - acc: 0.6457 - val_loss: 2.0530 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.73046\n",
      "Epoch 272/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1379 - acc: 0.6487 - val_loss: 1.8271 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.73046\n",
      "Epoch 273/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1226 - acc: 0.6535 - val_loss: 1.2508 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.73046\n",
      "Epoch 274/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0993 - acc: 0.6668 - val_loss: 1.9990 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.73046\n",
      "Epoch 275/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1061 - acc: 0.6617 - val_loss: 1.6615 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.73046\n",
      "Epoch 276/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1511 - acc: 0.6502 - val_loss: 1.3801 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.73046\n",
      "Epoch 277/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1548 - acc: 0.6442 - val_loss: 2.0500 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.73046\n",
      "Epoch 278/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1378 - acc: 0.6475 - val_loss: 1.6358 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.73046\n",
      "Epoch 279/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1155 - acc: 0.6490 - val_loss: 1.7490 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.73046\n",
      "Epoch 280/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1326 - acc: 0.6493 - val_loss: 1.9097 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.73046\n",
      "Epoch 281/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1302 - acc: 0.6538 - val_loss: 1.3758 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.73046\n",
      "Epoch 282/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1438 - acc: 0.6472 - val_loss: 1.2652 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.73046\n",
      "Epoch 283/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1181 - acc: 0.6466 - val_loss: 1.4530 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.73046\n",
      "Epoch 284/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1399 - acc: 0.6451 - val_loss: 1.2450 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.73046\n",
      "Epoch 285/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1342 - acc: 0.6457 - val_loss: 1.6006 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.73046\n",
      "Epoch 286/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0976 - acc: 0.6584 - val_loss: 1.3307 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.73046\n",
      "Epoch 287/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1437 - acc: 0.6478 - val_loss: 1.3776 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.73046\n",
      "Epoch 288/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1112 - acc: 0.6532 - val_loss: 1.3052 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.73046\n",
      "Epoch 289/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1294 - acc: 0.6460 - val_loss: 1.8036 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.73046\n",
      "Epoch 290/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1170 - acc: 0.6538 - val_loss: 1.9183 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.73046\n",
      "Epoch 291/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0984 - acc: 0.6677 - val_loss: 1.3066 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.73046\n",
      "Epoch 292/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0899 - acc: 0.6695 - val_loss: 1.4468 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.73046\n",
      "Epoch 293/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1015 - acc: 0.6505 - val_loss: 1.2162 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.73046\n",
      "Epoch 294/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1128 - acc: 0.6472 - val_loss: 1.6189 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.73046\n",
      "Epoch 295/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1262 - acc: 0.6593 - val_loss: 1.4420 - val_acc: 0.6496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_acc did not improve from 0.73046\n",
      "Epoch 296/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0969 - acc: 0.6608 - val_loss: 1.4881 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.73046\n",
      "Epoch 297/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1384 - acc: 0.6505 - val_loss: 1.7873 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.73046\n",
      "Epoch 298/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0971 - acc: 0.6641 - val_loss: 1.2305 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.73046\n",
      "Epoch 299/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1311 - acc: 0.6505 - val_loss: 1.5933 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.73046\n",
      "Epoch 300/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0863 - acc: 0.6593 - val_loss: 1.9527 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.73046\n",
      "Epoch 301/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0852 - acc: 0.6701 - val_loss: 1.5428 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.73046\n",
      "Epoch 302/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0799 - acc: 0.6650 - val_loss: 1.8000 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.73046\n",
      "Epoch 303/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1111 - acc: 0.6578 - val_loss: 1.5382 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.73046\n",
      "Epoch 304/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1067 - acc: 0.6502 - val_loss: 1.2247 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.73046\n",
      "Epoch 305/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0869 - acc: 0.6695 - val_loss: 1.2794 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.73046\n",
      "Epoch 306/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1017 - acc: 0.6584 - val_loss: 1.3736 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.73046\n",
      "Epoch 307/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0970 - acc: 0.6701 - val_loss: 1.5430 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.73046\n",
      "Epoch 308/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1024 - acc: 0.6704 - val_loss: 1.2744 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.73046\n",
      "Epoch 309/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0965 - acc: 0.6737 - val_loss: 1.6512 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.73046\n",
      "Epoch 310/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0976 - acc: 0.6671 - val_loss: 1.3275 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.73046\n",
      "Epoch 311/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1008 - acc: 0.6635 - val_loss: 1.5241 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.73046\n",
      "Epoch 312/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0876 - acc: 0.6725 - val_loss: 1.5877 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.73046\n",
      "Epoch 313/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1148 - acc: 0.6575 - val_loss: 1.5968 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.73046\n",
      "Epoch 314/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1077 - acc: 0.6662 - val_loss: 1.5206 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.73046\n",
      "Epoch 315/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0836 - acc: 0.6650 - val_loss: 1.2869 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.73046\n",
      "Epoch 316/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0789 - acc: 0.6791 - val_loss: 1.9751 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.73046\n",
      "Epoch 317/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0768 - acc: 0.6629 - val_loss: 1.9617 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.73046\n",
      "Epoch 318/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0824 - acc: 0.6764 - val_loss: 1.7685 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.73046\n",
      "Epoch 319/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0949 - acc: 0.6653 - val_loss: 1.6227 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.73046\n",
      "Epoch 320/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0666 - acc: 0.6740 - val_loss: 1.3160 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.73046\n",
      "Epoch 321/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0944 - acc: 0.6605 - val_loss: 1.4872 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.73046\n",
      "Epoch 322/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0634 - acc: 0.6842 - val_loss: 1.3591 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.73046\n",
      "Epoch 323/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0950 - acc: 0.6695 - val_loss: 1.6384 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.73046\n",
      "Epoch 324/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0762 - acc: 0.6761 - val_loss: 1.5512 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.73046\n",
      "Epoch 325/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0846 - acc: 0.6734 - val_loss: 1.3904 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.73046\n",
      "Epoch 326/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.1052 - acc: 0.6644 - val_loss: 2.1830 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.73046\n",
      "Epoch 327/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0747 - acc: 0.6764 - val_loss: 1.2423 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.73046\n",
      "Epoch 328/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1100 - acc: 0.6632 - val_loss: 1.1067 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00328: val_acc improved from 0.73046 to 0.76280, saving model to model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 329/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0507 - acc: 0.6791 - val_loss: 1.5593 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.76280\n",
      "Epoch 330/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0406 - acc: 0.6845 - val_loss: 1.6521 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.76280\n",
      "Epoch 331/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0666 - acc: 0.6779 - val_loss: 1.2414 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.76280\n",
      "Epoch 332/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0566 - acc: 0.6776 - val_loss: 1.9775 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.76280\n",
      "Epoch 333/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1043 - acc: 0.6629 - val_loss: 1.3874 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.76280\n",
      "Epoch 334/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1148 - acc: 0.6596 - val_loss: 1.6611 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.76280\n",
      "Epoch 335/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.1333 - acc: 0.6529 - val_loss: 1.2480 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.76280\n",
      "Epoch 336/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0914 - acc: 0.6809 - val_loss: 1.2815 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.76280\n",
      "Epoch 337/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0578 - acc: 0.6896 - val_loss: 1.2221 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.76280\n",
      "Epoch 338/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0615 - acc: 0.6722 - val_loss: 2.0541 - val_acc: 0.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00338: val_acc did not improve from 0.76280\n",
      "Epoch 339/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0562 - acc: 0.6779 - val_loss: 1.9267 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.76280\n",
      "Epoch 340/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0765 - acc: 0.6707 - val_loss: 1.4584 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.76280\n",
      "Epoch 341/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0276 - acc: 0.6854 - val_loss: 1.2995 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.76280\n",
      "Epoch 342/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0683 - acc: 0.6653 - val_loss: 1.2638 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.76280\n",
      "Epoch 343/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0526 - acc: 0.6737 - val_loss: 1.2126 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.76280\n",
      "Epoch 344/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0472 - acc: 0.6848 - val_loss: 1.3322 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.76280\n",
      "Epoch 345/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0803 - acc: 0.6671 - val_loss: 1.7558 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.76280\n",
      "Epoch 346/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0570 - acc: 0.6866 - val_loss: 1.3614 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.76280\n",
      "Epoch 347/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0917 - acc: 0.6632 - val_loss: 1.7740 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.76280\n",
      "Epoch 348/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0666 - acc: 0.6809 - val_loss: 1.4497 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.76280\n",
      "Epoch 349/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0694 - acc: 0.6788 - val_loss: 1.4759 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.76280\n",
      "Epoch 350/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0183 - acc: 0.6887 - val_loss: 1.1892 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.76280\n",
      "Epoch 351/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0783 - acc: 0.6755 - val_loss: 1.6690 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.76280\n",
      "Epoch 352/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0547 - acc: 0.6797 - val_loss: 1.5544 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.76280\n",
      "Epoch 353/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0253 - acc: 0.6974 - val_loss: 1.3568 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.76280\n",
      "Epoch 354/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0621 - acc: 0.6719 - val_loss: 1.5162 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.76280\n",
      "Epoch 355/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0960 - acc: 0.6626 - val_loss: 1.7540 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.76280\n",
      "Epoch 356/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0229 - acc: 0.6875 - val_loss: 1.5074 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.76280\n",
      "Epoch 357/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0503 - acc: 0.6887 - val_loss: 1.5401 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.76280\n",
      "Epoch 358/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0580 - acc: 0.6761 - val_loss: 1.7477 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.76280\n",
      "Epoch 359/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0660 - acc: 0.6701 - val_loss: 1.2417 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.76280\n",
      "Epoch 360/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0258 - acc: 0.6848 - val_loss: 1.4873 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.76280\n",
      "Epoch 361/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0513 - acc: 0.6806 - val_loss: 1.3388 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.76280\n",
      "Epoch 362/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0296 - acc: 0.6836 - val_loss: 1.2766 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.76280\n",
      "Epoch 363/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0206 - acc: 0.6950 - val_loss: 1.3027 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.76280\n",
      "Epoch 364/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0672 - acc: 0.6707 - val_loss: 1.6278 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.76280\n",
      "Epoch 365/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0274 - acc: 0.6932 - val_loss: 1.3898 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.76280\n",
      "Epoch 366/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0364 - acc: 0.6959 - val_loss: 1.5615 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.76280\n",
      "Epoch 367/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0569 - acc: 0.6932 - val_loss: 1.1882 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.76280\n",
      "Epoch 368/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0681 - acc: 0.6737 - val_loss: 1.2102 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.76280\n",
      "Epoch 369/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0436 - acc: 0.6926 - val_loss: 1.4074 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.76280\n",
      "Epoch 370/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0326 - acc: 0.6911 - val_loss: 1.5461 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.76280\n",
      "Epoch 371/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0361 - acc: 0.6818 - val_loss: 2.3040 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.76280\n",
      "Epoch 372/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0239 - acc: 0.6800 - val_loss: 1.6627 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.76280\n",
      "Epoch 373/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0638 - acc: 0.6854 - val_loss: 1.4694 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.76280\n",
      "Epoch 374/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0359 - acc: 0.6926 - val_loss: 1.2959 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.76280\n",
      "Epoch 375/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0066 - acc: 0.6899 - val_loss: 1.7822 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.76280\n",
      "Epoch 376/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0310 - acc: 0.6956 - val_loss: 1.4106 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.76280\n",
      "Epoch 377/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0430 - acc: 0.6899 - val_loss: 1.5118 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.76280\n",
      "Epoch 378/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0187 - acc: 0.6971 - val_loss: 1.1478 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.76280\n",
      "Epoch 379/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0194 - acc: 0.6929 - val_loss: 1.7097 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.76280\n",
      "Epoch 380/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0005 - acc: 0.7067 - val_loss: 1.3192 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.76280\n",
      "Epoch 381/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0558 - acc: 0.6803 - val_loss: 1.1931 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.76280\n",
      "Epoch 382/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0409 - acc: 0.6905 - val_loss: 1.3713 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.76280\n",
      "Epoch 383/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0143 - acc: 0.6881 - val_loss: 1.6202 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.76280\n",
      "Epoch 384/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0667 - acc: 0.6755 - val_loss: 1.5952 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.76280\n",
      "Epoch 385/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0221 - acc: 0.6926 - val_loss: 1.5277 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.76280\n",
      "Epoch 386/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0240 - acc: 0.6872 - val_loss: 1.6782 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.76280\n",
      "Epoch 387/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0677 - acc: 0.6866 - val_loss: 1.2091 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.76280\n",
      "Epoch 388/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9832 - acc: 0.7082 - val_loss: 1.3195 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.76280\n",
      "Epoch 389/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0153 - acc: 0.6929 - val_loss: 1.4587 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.76280\n",
      "Epoch 390/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0322 - acc: 0.7061 - val_loss: 1.3641 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.76280\n",
      "Epoch 391/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0392 - acc: 0.6839 - val_loss: 1.3595 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.76280\n",
      "Epoch 392/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0028 - acc: 0.6920 - val_loss: 1.3603 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.76280\n",
      "Epoch 393/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0314 - acc: 0.6914 - val_loss: 1.2972 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.76280\n",
      "Epoch 394/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9964 - acc: 0.7004 - val_loss: 1.1727 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.76280\n",
      "Epoch 395/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0029 - acc: 0.6857 - val_loss: 1.2581 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.76280\n",
      "Epoch 396/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0074 - acc: 0.7025 - val_loss: 1.8556 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.76280\n",
      "Epoch 397/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0127 - acc: 0.6869 - val_loss: 1.2960 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.76280\n",
      "Epoch 398/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0228 - acc: 0.6989 - val_loss: 3.3260 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.76280\n",
      "Epoch 399/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0373 - acc: 0.6965 - val_loss: 1.2126 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.76280\n",
      "Epoch 400/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0253 - acc: 0.6839 - val_loss: 1.3408 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.76280\n",
      "Epoch 401/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0212 - acc: 0.6995 - val_loss: 1.3816 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.76280\n",
      "Epoch 402/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0319 - acc: 0.6863 - val_loss: 1.5352 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.76280\n",
      "Epoch 403/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0225 - acc: 0.6887 - val_loss: 1.3060 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.76280\n",
      "Epoch 404/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0405 - acc: 0.6815 - val_loss: 2.1520 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.76280\n",
      "Epoch 405/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0066 - acc: 0.6887 - val_loss: 2.0563 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.76280\n",
      "Epoch 406/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9992 - acc: 0.6959 - val_loss: 1.1097 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.76280\n",
      "Epoch 407/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0010 - acc: 0.7007 - val_loss: 1.3002 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.76280\n",
      "Epoch 408/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0208 - acc: 0.6878 - val_loss: 1.4678 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.76280\n",
      "Epoch 409/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0261 - acc: 0.6839 - val_loss: 1.2757 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.76280\n",
      "Epoch 410/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0255 - acc: 0.6917 - val_loss: 1.4862 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.76280\n",
      "Epoch 411/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 1.9870 - acc: 0.7019 - val_loss: 1.2593 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.76280\n",
      "Epoch 412/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9973 - acc: 0.7058 - val_loss: 1.8414 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.76280\n",
      "Epoch 413/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0019 - acc: 0.6992 - val_loss: 1.3700 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.76280\n",
      "Epoch 414/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0498 - acc: 0.6929 - val_loss: 2.2297 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.76280\n",
      "Epoch 415/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0027 - acc: 0.6944 - val_loss: 1.3769 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.76280\n",
      "Epoch 416/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0175 - acc: 0.7019 - val_loss: 1.3795 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.76280\n",
      "Epoch 417/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0191 - acc: 0.7001 - val_loss: 1.3292 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.76280\n",
      "Epoch 418/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0060 - acc: 0.6881 - val_loss: 1.8261 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.76280\n",
      "Epoch 419/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9899 - acc: 0.7124 - val_loss: 1.3580 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.76280\n",
      "Epoch 420/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0024 - acc: 0.7121 - val_loss: 1.2240 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.76280\n",
      "Epoch 421/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 2.0078 - acc: 0.7001 - val_loss: 1.2621 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.76280\n",
      "Epoch 422/3000\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 1.9638 - acc: 0.7142 - val_loss: 1.1693 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.76280\n",
      "Epoch 423/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9892 - acc: 0.6980 - val_loss: 1.2424 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.76280\n",
      "Epoch 424/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0104 - acc: 0.7025 - val_loss: 1.7787 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.76280\n",
      "Epoch 425/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9943 - acc: 0.7049 - val_loss: 1.2392 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.76280\n",
      "Epoch 426/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0130 - acc: 0.6875 - val_loss: 1.1697 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.76280\n",
      "Epoch 427/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 1.9907 - acc: 0.7082 - val_loss: 1.4116 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.76280\n",
      "Epoch 428/3000\n",
      "104/104 [==============================] - 19s 182ms/step - loss: 2.0145 - acc: 0.6932 - val_loss: 1.3013 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.76280\n",
      "Epoch 00428: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_7===\n",
      "semi loading: model/mfcc7/LGD_fold7_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "53/53 [==============================] - 27s 518ms/step - loss: 2.1267 - acc: 0.6197 - val_loss: 0.9797 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.77898, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 6/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 2.0533 - acc: 0.6418 - val_loss: 0.9591 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77898 to 0.79515, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 7/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 2.0226 - acc: 0.6524 - val_loss: 0.9689 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79515\n",
      "Epoch 8/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 2.0111 - acc: 0.6613 - val_loss: 0.9563 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79515\n",
      "Epoch 9/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.9717 - acc: 0.6689 - val_loss: 0.9635 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79515\n",
      "Epoch 10/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9547 - acc: 0.6798 - val_loss: 0.9561 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79515\n",
      "Epoch 11/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.9442 - acc: 0.6860 - val_loss: 0.9463 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79515\n",
      "Epoch 12/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8982 - acc: 0.7034 - val_loss: 0.9333 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79515\n",
      "Epoch 13/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9299 - acc: 0.6919 - val_loss: 0.9534 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79515\n",
      "Epoch 14/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.9074 - acc: 0.7052 - val_loss: 0.9265 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79515\n",
      "Epoch 15/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8732 - acc: 0.7140 - val_loss: 0.9284 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.79515 to 0.79784, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 16/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8879 - acc: 0.7046 - val_loss: 0.9206 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.79784 to 0.80863, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 17/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8827 - acc: 0.7084 - val_loss: 0.9205 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80863\n",
      "Epoch 18/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8787 - acc: 0.7202 - val_loss: 0.9126 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80863\n",
      "Epoch 19/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8625 - acc: 0.7155 - val_loss: 0.9320 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80863\n",
      "Epoch 20/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8696 - acc: 0.7084 - val_loss: 0.9121 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80863\n",
      "Epoch 21/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8683 - acc: 0.7102 - val_loss: 0.9506 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80863\n",
      "Epoch 22/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8543 - acc: 0.7182 - val_loss: 0.9295 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80863\n",
      "Epoch 23/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8786 - acc: 0.7008 - val_loss: 0.9328 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.80863\n",
      "Epoch 24/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8791 - acc: 0.7073 - val_loss: 0.9266 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80863\n",
      "Epoch 25/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8395 - acc: 0.7196 - val_loss: 0.9637 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80863\n",
      "Epoch 26/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8671 - acc: 0.7043 - val_loss: 0.9240 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.80863 to 0.81941, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 27/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8706 - acc: 0.7052 - val_loss: 0.9043 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.81941 to 0.82210, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 28/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8336 - acc: 0.7155 - val_loss: 0.9014 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82210\n",
      "Epoch 29/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8749 - acc: 0.7134 - val_loss: 0.9198 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82210\n",
      "Epoch 30/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8312 - acc: 0.7273 - val_loss: 0.9273 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82210\n",
      "Epoch 31/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8747 - acc: 0.7117 - val_loss: 0.9221 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82210\n",
      "Epoch 32/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8548 - acc: 0.7146 - val_loss: 0.9167 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82210\n",
      "Epoch 33/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8495 - acc: 0.7120 - val_loss: 0.9234 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82210\n",
      "Epoch 34/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8338 - acc: 0.7391 - val_loss: 0.9011 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82210\n",
      "Epoch 35/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8183 - acc: 0.7311 - val_loss: 0.8972 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82210\n",
      "Epoch 36/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8191 - acc: 0.7350 - val_loss: 0.9013 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82210\n",
      "Epoch 37/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8392 - acc: 0.7196 - val_loss: 0.9105 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82210\n",
      "Epoch 38/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8022 - acc: 0.7258 - val_loss: 0.9208 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82210\n",
      "Epoch 39/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8240 - acc: 0.7220 - val_loss: 0.8721 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.82210 to 0.82749, saving model to model/mfcc7/LGD_semi_fold7_resnet3.h5\n",
      "Epoch 40/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8247 - acc: 0.7285 - val_loss: 0.8775 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82749\n",
      "Epoch 41/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8048 - acc: 0.7409 - val_loss: 0.9070 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82749\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7992 - acc: 0.7308 - val_loss: 0.8754 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82749\n",
      "Epoch 43/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8072 - acc: 0.7220 - val_loss: 0.8747 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82749\n",
      "Epoch 44/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7823 - acc: 0.7388 - val_loss: 0.8848 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82749\n",
      "Epoch 45/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7956 - acc: 0.7397 - val_loss: 0.8953 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.82749\n",
      "Epoch 46/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7985 - acc: 0.7314 - val_loss: 0.9047 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.82749\n",
      "Epoch 47/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8168 - acc: 0.7258 - val_loss: 0.9202 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.82749\n",
      "Epoch 48/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7934 - acc: 0.7341 - val_loss: 0.8974 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.82749\n",
      "Epoch 49/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7828 - acc: 0.7317 - val_loss: 0.8953 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.82749\n",
      "Epoch 50/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8262 - acc: 0.7258 - val_loss: 0.8918 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.82749\n",
      "Epoch 51/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.8025 - acc: 0.7335 - val_loss: 0.9125 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.82749\n",
      "Epoch 52/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7802 - acc: 0.7320 - val_loss: 0.9361 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.82749\n",
      "Epoch 53/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7886 - acc: 0.7441 - val_loss: 0.9342 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.82749\n",
      "Epoch 54/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.8013 - acc: 0.7388 - val_loss: 0.9089 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.82749\n",
      "Epoch 55/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.8017 - acc: 0.7406 - val_loss: 0.8821 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.82749\n",
      "Epoch 56/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7591 - acc: 0.7450 - val_loss: 0.8948 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.82749\n",
      "Epoch 57/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7717 - acc: 0.7426 - val_loss: 0.9002 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.82749\n",
      "Epoch 58/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7772 - acc: 0.7361 - val_loss: 0.8925 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.82749\n",
      "Epoch 59/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7975 - acc: 0.7456 - val_loss: 0.8906 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.82749\n",
      "Epoch 60/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7736 - acc: 0.7426 - val_loss: 0.8994 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.82749\n",
      "Epoch 61/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7849 - acc: 0.7314 - val_loss: 0.8979 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.82749\n",
      "Epoch 62/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7652 - acc: 0.7397 - val_loss: 0.9094 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.82749\n",
      "Epoch 63/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7659 - acc: 0.7509 - val_loss: 0.8837 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.82749\n",
      "Epoch 64/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7604 - acc: 0.7527 - val_loss: 0.9070 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.82749\n",
      "Epoch 65/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7802 - acc: 0.7544 - val_loss: 0.9242 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.82749\n",
      "Epoch 66/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7830 - acc: 0.7394 - val_loss: 0.9050 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.82749\n",
      "Epoch 67/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7783 - acc: 0.7435 - val_loss: 0.9113 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.82749\n",
      "Epoch 68/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7315 - acc: 0.7577 - val_loss: 0.8870 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.82749\n",
      "Epoch 69/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7891 - acc: 0.7432 - val_loss: 0.8649 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.82749\n",
      "Epoch 70/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7532 - val_loss: 0.8730 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.82749\n",
      "Epoch 71/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7621 - acc: 0.7373 - val_loss: 0.8774 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.82749\n",
      "Epoch 72/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7605 - acc: 0.7423 - val_loss: 0.8764 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.82749\n",
      "Epoch 73/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7480 - acc: 0.7597 - val_loss: 0.8726 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.82749\n",
      "Epoch 74/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7509 - val_loss: 0.8837 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.82749\n",
      "Epoch 75/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7397 - acc: 0.7524 - val_loss: 0.8683 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.82749\n",
      "Epoch 76/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7290 - acc: 0.7515 - val_loss: 0.8799 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.82749\n",
      "Epoch 77/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7486 - acc: 0.7639 - val_loss: 0.8685 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.82749\n",
      "Epoch 78/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7425 - acc: 0.7412 - val_loss: 0.8716 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.82749\n",
      "Epoch 79/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7402 - acc: 0.7577 - val_loss: 0.8635 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.82749\n",
      "Epoch 80/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7547 - acc: 0.7482 - val_loss: 0.8707 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.82749\n",
      "Epoch 81/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7502 - acc: 0.7583 - val_loss: 0.8829 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.82749\n",
      "Epoch 82/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7666 - acc: 0.7394 - val_loss: 0.8918 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.82749\n",
      "Epoch 83/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7589 - acc: 0.7435 - val_loss: 0.8734 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.82749\n",
      "Epoch 84/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7455 - acc: 0.7553 - val_loss: 0.8799 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.82749\n",
      "Epoch 85/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7451 - acc: 0.7488 - val_loss: 0.8698 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.82749\n",
      "Epoch 86/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7731 - acc: 0.7409 - val_loss: 0.8817 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.82749\n",
      "Epoch 87/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7213 - acc: 0.7633 - val_loss: 0.8700 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.82749\n",
      "Epoch 88/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7565 - acc: 0.7538 - val_loss: 0.8613 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.82749\n",
      "Epoch 89/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7643 - acc: 0.7497 - val_loss: 0.8534 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.82749\n",
      "Epoch 90/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7603 - acc: 0.7438 - val_loss: 0.8608 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.82749\n",
      "Epoch 91/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7474 - acc: 0.7544 - val_loss: 0.8742 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.82749\n",
      "Epoch 92/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7140 - acc: 0.7550 - val_loss: 0.8692 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.82749\n",
      "Epoch 93/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7097 - acc: 0.7633 - val_loss: 0.8680 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.82749\n",
      "Epoch 94/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7477 - acc: 0.7482 - val_loss: 0.8672 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.82749\n",
      "Epoch 95/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7354 - acc: 0.7615 - val_loss: 0.8644 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.82749\n",
      "Epoch 96/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7575 - acc: 0.7512 - val_loss: 0.8610 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.82749\n",
      "Epoch 97/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7405 - acc: 0.7656 - val_loss: 0.8547 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.82749\n",
      "Epoch 98/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7186 - acc: 0.7597 - val_loss: 0.8532 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.82749\n",
      "Epoch 99/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7636 - acc: 0.7479 - val_loss: 0.8451 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.82749\n",
      "Epoch 100/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7462 - acc: 0.7400 - val_loss: 0.8537 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.82749\n",
      "Epoch 101/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7219 - acc: 0.7556 - val_loss: 0.8491 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.82749\n",
      "Epoch 102/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7317 - acc: 0.7506 - val_loss: 0.8390 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.82749\n",
      "Epoch 103/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7258 - acc: 0.7544 - val_loss: 0.8477 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82749\n",
      "Epoch 104/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7686 - acc: 0.7473 - val_loss: 0.8492 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82749\n",
      "Epoch 105/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7382 - acc: 0.7656 - val_loss: 0.8547 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82749\n",
      "Epoch 106/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7740 - acc: 0.7400 - val_loss: 0.8569 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.82749\n",
      "Epoch 107/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7508 - acc: 0.7518 - val_loss: 0.8542 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.82749\n",
      "Epoch 108/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7187 - acc: 0.7577 - val_loss: 0.8574 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.82749\n",
      "Epoch 109/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7114 - acc: 0.7503 - val_loss: 0.8631 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.82749\n",
      "Epoch 110/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7470 - acc: 0.7541 - val_loss: 0.8583 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.82749\n",
      "Epoch 111/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7191 - acc: 0.7509 - val_loss: 0.8544 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.82749\n",
      "Epoch 112/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7296 - acc: 0.7624 - val_loss: 0.8634 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.82749\n",
      "Epoch 113/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7369 - acc: 0.7562 - val_loss: 0.8670 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.82749\n",
      "Epoch 114/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7049 - acc: 0.7703 - val_loss: 0.8681 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.82749\n",
      "Epoch 115/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7250 - acc: 0.7597 - val_loss: 0.8659 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.82749\n",
      "Epoch 116/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7207 - acc: 0.7588 - val_loss: 0.8652 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.82749\n",
      "Epoch 117/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7229 - acc: 0.7568 - val_loss: 0.8616 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.82749\n",
      "Epoch 118/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7551 - acc: 0.7538 - val_loss: 0.8629 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.82749\n",
      "Epoch 119/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7361 - acc: 0.7556 - val_loss: 0.8613 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.82749\n",
      "Epoch 120/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7397 - acc: 0.7494 - val_loss: 0.8619 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.82749\n",
      "Epoch 121/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7434 - acc: 0.7485 - val_loss: 0.8605 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.82749\n",
      "Epoch 122/3000\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 1.7152 - acc: 0.7706 - val_loss: 0.8585 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.82749\n",
      "Epoch 123/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7156 - acc: 0.7509 - val_loss: 0.8571 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.82749\n",
      "Epoch 124/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7598 - acc: 0.7488 - val_loss: 0.8591 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.82749\n",
      "Epoch 125/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7264 - acc: 0.7515 - val_loss: 0.8568 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.82749\n",
      "Epoch 126/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7095 - acc: 0.7715 - val_loss: 0.8573 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.82749\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7182 - acc: 0.7583 - val_loss: 0.8605 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.82749\n",
      "Epoch 128/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7418 - acc: 0.7479 - val_loss: 0.8585 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.82749\n",
      "Epoch 129/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7143 - acc: 0.7647 - val_loss: 0.8583 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.82749\n",
      "Epoch 130/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7164 - acc: 0.7618 - val_loss: 0.8593 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.82749\n",
      "Epoch 131/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7514 - acc: 0.7506 - val_loss: 0.8583 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.82749\n",
      "Epoch 132/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7162 - acc: 0.7568 - val_loss: 0.8581 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.82749\n",
      "Epoch 133/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7220 - acc: 0.7624 - val_loss: 0.8615 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.82749\n",
      "Epoch 134/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7222 - acc: 0.7541 - val_loss: 0.8627 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.82749\n",
      "Epoch 135/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7290 - acc: 0.7562 - val_loss: 0.8580 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.82749\n",
      "Epoch 136/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7169 - acc: 0.7591 - val_loss: 0.8561 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.82749\n",
      "Epoch 137/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7146 - acc: 0.7553 - val_loss: 0.8564 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.82749\n",
      "Epoch 138/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7397 - acc: 0.7609 - val_loss: 0.8559 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.82749\n",
      "Epoch 139/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7438 - acc: 0.7571 - val_loss: 0.8579 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.82749\n",
      "Epoch 140/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7157 - acc: 0.7500 - val_loss: 0.8594 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.82749\n",
      "Epoch 141/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7360 - acc: 0.7653 - val_loss: 0.8583 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.82749\n",
      "Epoch 142/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7048 - acc: 0.7792 - val_loss: 0.8548 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.82749\n",
      "Epoch 143/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7193 - acc: 0.7597 - val_loss: 0.8547 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.82749\n",
      "Epoch 144/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7245 - acc: 0.7633 - val_loss: 0.8569 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.82749\n",
      "Epoch 145/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7385 - acc: 0.7538 - val_loss: 0.8593 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.82749\n",
      "Epoch 146/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7257 - acc: 0.7644 - val_loss: 0.8607 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.82749\n",
      "Epoch 147/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7240 - acc: 0.7689 - val_loss: 0.8562 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.82749\n",
      "Epoch 148/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7277 - acc: 0.7612 - val_loss: 0.8550 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.82749\n",
      "Epoch 149/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7341 - acc: 0.7473 - val_loss: 0.8553 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.82749\n",
      "Epoch 150/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7089 - acc: 0.7565 - val_loss: 0.8563 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.82749\n",
      "Epoch 151/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7311 - acc: 0.7594 - val_loss: 0.8572 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.82749\n",
      "Epoch 152/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7138 - acc: 0.7609 - val_loss: 0.8547 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.82749\n",
      "Epoch 153/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7235 - acc: 0.7497 - val_loss: 0.8549 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.82749\n",
      "Epoch 154/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7284 - acc: 0.7541 - val_loss: 0.8532 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.82749\n",
      "Epoch 155/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7545 - acc: 0.7450 - val_loss: 0.8541 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.82749\n",
      "Epoch 156/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7254 - acc: 0.7529 - val_loss: 0.8555 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.82749\n",
      "Epoch 157/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7137 - acc: 0.7618 - val_loss: 0.8559 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.82749\n",
      "Epoch 158/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7627 - val_loss: 0.8571 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.82749\n",
      "Epoch 159/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7133 - acc: 0.7668 - val_loss: 0.8594 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.82749\n",
      "Epoch 160/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7352 - acc: 0.7423 - val_loss: 0.8583 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.82749\n",
      "Epoch 161/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7383 - acc: 0.7577 - val_loss: 0.8568 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.82749\n",
      "Epoch 162/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7216 - acc: 0.7524 - val_loss: 0.8567 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.82749\n",
      "Epoch 163/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7222 - acc: 0.7612 - val_loss: 0.8542 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.82749\n",
      "Epoch 164/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7195 - acc: 0.7644 - val_loss: 0.8554 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.82749\n",
      "Epoch 165/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7487 - acc: 0.7503 - val_loss: 0.8550 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.82749\n",
      "Epoch 166/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7362 - acc: 0.7577 - val_loss: 0.8532 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.82749\n",
      "Epoch 167/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7072 - acc: 0.7636 - val_loss: 0.8545 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.82749\n",
      "Epoch 168/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7164 - acc: 0.7615 - val_loss: 0.8540 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.82749\n",
      "Epoch 169/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7367 - acc: 0.7544 - val_loss: 0.8554 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.82749\n",
      "Epoch 170/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7223 - acc: 0.7583 - val_loss: 0.8558 - val_acc: 0.8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00170: val_acc did not improve from 0.82749\n",
      "Epoch 171/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7306 - acc: 0.7553 - val_loss: 0.8538 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.82749\n",
      "Epoch 172/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7110 - acc: 0.7529 - val_loss: 0.8537 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.82749\n",
      "Epoch 173/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7321 - acc: 0.7541 - val_loss: 0.8532 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.82749\n",
      "Epoch 174/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7309 - acc: 0.7488 - val_loss: 0.8533 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.82749\n",
      "Epoch 175/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7115 - acc: 0.7594 - val_loss: 0.8533 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.82749\n",
      "Epoch 176/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7116 - acc: 0.7553 - val_loss: 0.8540 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.82749\n",
      "Epoch 177/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7025 - acc: 0.7627 - val_loss: 0.8541 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.82749\n",
      "Epoch 178/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.6960 - acc: 0.7706 - val_loss: 0.8542 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.82749\n",
      "Epoch 179/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7556 - val_loss: 0.8527 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.82749\n",
      "Epoch 180/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7218 - acc: 0.7553 - val_loss: 0.8537 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.82749\n",
      "Epoch 181/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7353 - acc: 0.7597 - val_loss: 0.8520 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.82749\n",
      "Epoch 182/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7105 - acc: 0.7585 - val_loss: 0.8536 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.82749\n",
      "Epoch 183/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7280 - acc: 0.7677 - val_loss: 0.8556 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.82749\n",
      "Epoch 184/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7316 - acc: 0.7580 - val_loss: 0.8555 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.82749\n",
      "Epoch 185/3000\n",
      "53/53 [==============================] - 15s 282ms/step - loss: 1.7276 - acc: 0.7571 - val_loss: 0.8585 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.82749\n",
      "Epoch 186/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7150 - acc: 0.7674 - val_loss: 0.8575 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.82749\n",
      "Epoch 187/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7142 - acc: 0.7562 - val_loss: 0.8561 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.82749\n",
      "Epoch 188/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7238 - acc: 0.7609 - val_loss: 0.8564 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.82749\n",
      "Epoch 189/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7324 - acc: 0.7644 - val_loss: 0.8585 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.82749\n",
      "Epoch 190/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7179 - acc: 0.7591 - val_loss: 0.8589 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.82749\n",
      "Epoch 191/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7318 - acc: 0.7527 - val_loss: 0.8584 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.82749\n",
      "Epoch 192/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7013 - acc: 0.7727 - val_loss: 0.8565 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.82749\n",
      "Epoch 193/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7010 - acc: 0.7709 - val_loss: 0.8545 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.82749\n",
      "Epoch 194/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7335 - acc: 0.7594 - val_loss: 0.8552 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.82749\n",
      "Epoch 195/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7068 - acc: 0.7680 - val_loss: 0.8564 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.82749\n",
      "Epoch 196/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7070 - acc: 0.7662 - val_loss: 0.8585 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.82749\n",
      "Epoch 197/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7226 - acc: 0.7686 - val_loss: 0.8588 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.82749\n",
      "Epoch 198/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7187 - acc: 0.7698 - val_loss: 0.8570 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.82749\n",
      "Epoch 199/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7319 - acc: 0.7591 - val_loss: 0.8573 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.82749\n",
      "Epoch 200/3000\n",
      "53/53 [==============================] - 15s 283ms/step - loss: 1.7109 - acc: 0.7677 - val_loss: 0.8592 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.82749\n",
      "Epoch 201/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7107 - acc: 0.7674 - val_loss: 0.8598 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.82749\n",
      "Epoch 202/3000\n",
      "53/53 [==============================] - 15s 284ms/step - loss: 1.7140 - acc: 0.7665 - val_loss: 0.8602 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.82749\n",
      "Epoch 00202: early stopping\n",
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold8_mfcc7===\n",
      "using resnet model: 3\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 30, 130, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 30, 130, 64)  256         conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 30, 130, 64)  0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 15, 65, 64)   0           activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 15, 65, 64)   256         conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 15, 65, 64)   0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 15, 65, 64)   36928       activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_511 (BatchN (None, 15, 65, 64)   256         conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 15, 65, 64)   0           batch_normalization_511[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 15, 65, 256)  16640       activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 15, 65, 256)  0           conv2d_540[0][0]                 \n",
      "                                                                 conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_512 (BatchN (None, 15, 65, 256)  1024        add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 15, 65, 256)  0           batch_normalization_512[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 15, 65, 64)   16448       activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_513 (BatchN (None, 15, 65, 64)   256         conv2d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 15, 65, 64)   0           batch_normalization_513[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 15, 65, 64)   36928       activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_514 (BatchN (None, 15, 65, 64)   256         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 15, 65, 64)   0           batch_normalization_514[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 15, 65, 256)  16640       activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 15, 65, 256)  0           add_187[0][0]                    \n",
      "                                                                 conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_515 (BatchN (None, 15, 65, 256)  1024        add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 15, 65, 256)  0           batch_normalization_515[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 15, 65, 64)   16448       activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_516 (BatchN (None, 15, 65, 64)   256         conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 15, 65, 64)   0           batch_normalization_516[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 15, 65, 64)   36928       activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_517 (BatchN (None, 15, 65, 64)   256         conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 15, 65, 64)   0           batch_normalization_517[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 15, 65, 256)  16640       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 15, 65, 256)  0           add_188[0][0]                    \n",
      "                                                                 conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_518 (BatchN (None, 15, 65, 256)  1024        add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 15, 65, 256)  0           batch_normalization_518[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 8, 33, 128)   32896       activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_519 (BatchN (None, 8, 33, 128)   512         conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 8, 33, 128)   0           batch_normalization_519[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 8, 33, 128)   147584      activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_520 (BatchN (None, 8, 33, 128)   512         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 8, 33, 128)   0           batch_normalization_520[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 8, 33, 512)   131584      add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 8, 33, 512)   66048       activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 8, 33, 512)   0           conv2d_550[0][0]                 \n",
      "                                                                 conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_521 (BatchN (None, 8, 33, 512)   2048        add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 8, 33, 512)   0           batch_normalization_521[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_551 (Conv2D)             (None, 8, 33, 128)   65664       activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_522 (BatchN (None, 8, 33, 128)   512         conv2d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 8, 33, 128)   0           batch_normalization_522[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 8, 33, 128)   147584      activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_523 (BatchN (None, 8, 33, 128)   512         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 8, 33, 128)   0           batch_normalization_523[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 8, 33, 512)   66048       activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 8, 33, 512)   0           add_190[0][0]                    \n",
      "                                                                 conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_524 (BatchN (None, 8, 33, 512)   2048        add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 8, 33, 512)   0           batch_normalization_524[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_554 (Conv2D)             (None, 8, 33, 128)   65664       activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_525 (BatchN (None, 8, 33, 128)   512         conv2d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 8, 33, 128)   0           batch_normalization_525[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 8, 33, 128)   147584      activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_526 (BatchN (None, 8, 33, 128)   512         conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 8, 33, 128)   0           batch_normalization_526[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 8, 33, 512)   66048       activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 8, 33, 512)   0           add_191[0][0]                    \n",
      "                                                                 conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_527 (BatchN (None, 8, 33, 512)   2048        add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 8, 33, 512)   0           batch_normalization_527[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 8, 33, 128)   65664       activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 8, 33, 128)   512         conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 8, 33, 128)   0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_558 (Conv2D)             (None, 8, 33, 128)   147584      activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 8, 33, 128)   512         conv2d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 8, 33, 128)   0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 8, 33, 512)   66048       activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 8, 33, 512)   0           add_192[0][0]                    \n",
      "                                                                 conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 8, 33, 512)   2048        add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 8, 33, 512)   0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 4, 17, 256)   131328      activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 4, 17, 256)   1024        conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 4, 17, 256)   0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_561 (Conv2D)             (None, 4, 17, 256)   590080      activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 4, 17, 256)   1024        conv2d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 4, 17, 256)   0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 4, 17, 1024)  525312      add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 4, 17, 1024)  263168      activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 4, 17, 1024)  0           conv2d_563[0][0]                 \n",
      "                                                                 conv2d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 4, 17, 1024)  4096        add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_564 (Conv2D)             (None, 4, 17, 256)   262400      activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 4, 17, 256)   1024        conv2d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 4, 17, 256)   0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 4, 17, 256)   590080      activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 4, 17, 256)   1024        conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 4, 17, 256)   0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 4, 17, 1024)  263168      activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 4, 17, 1024)  0           add_194[0][0]                    \n",
      "                                                                 conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 4, 17, 1024)  4096        add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_567 (Conv2D)             (None, 4, 17, 256)   262400      activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 4, 17, 256)   1024        conv2d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 4, 17, 256)   0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 4, 17, 256)   590080      activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 4, 17, 256)   1024        conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 4, 17, 256)   0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 4, 17, 1024)  263168      activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 4, 17, 1024)  0           add_195[0][0]                    \n",
      "                                                                 conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 4, 17, 1024)  4096        add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 4, 17, 256)   262400      activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 4, 17, 256)   1024        conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 4, 17, 256)   0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 4, 17, 256)   590080      activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 4, 17, 256)   1024        conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 4, 17, 256)   0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 4, 17, 1024)  263168      activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 4, 17, 1024)  0           add_196[0][0]                    \n",
      "                                                                 conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 4, 17, 1024)  4096        add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 4, 17, 256)   262400      activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 4, 17, 256)   1024        conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 4, 17, 256)   0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 4, 17, 256)   590080      activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 4, 17, 256)   1024        conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 4, 17, 256)   0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 4, 17, 1024)  263168      activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 4, 17, 1024)  0           add_197[0][0]                    \n",
      "                                                                 conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 4, 17, 1024)  4096        add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 4, 17, 256)   262400      activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 4, 17, 256)   1024        conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 4, 17, 256)   0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 4, 17, 256)   590080      activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 4, 17, 256)   1024        conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 4, 17, 256)   0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 4, 17, 1024)  263168      activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 4, 17, 1024)  0           add_198[0][0]                    \n",
      "                                                                 conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 4, 17, 1024)  4096        add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 4, 17, 256)   262400      activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 4, 17, 256)   1024        conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 4, 17, 256)   0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 4, 17, 256)   590080      activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 4, 17, 256)   1024        conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 4, 17, 256)   0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 4, 17, 1024)  263168      activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 4, 17, 1024)  0           add_199[0][0]                    \n",
      "                                                                 conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 4, 17, 1024)  4096        add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 4, 17, 256)   262400      activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 4, 17, 256)   1024        conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 4, 17, 256)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 4, 17, 256)   590080      activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 4, 17, 256)   1024        conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 4, 17, 256)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 4, 17, 1024)  263168      activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 4, 17, 1024)  0           add_200[0][0]                    \n",
      "                                                                 conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 4, 17, 1024)  4096        add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 4, 17, 256)   262400      activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 4, 17, 256)   1024        conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 4, 17, 256)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 4, 17, 256)   590080      activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 4, 17, 256)   1024        conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 4, 17, 256)   0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 4, 17, 1024)  263168      activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 4, 17, 1024)  0           add_201[0][0]                    \n",
      "                                                                 conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 4, 17, 1024)  4096        add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 4, 17, 256)   262400      activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 4, 17, 256)   1024        conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 4, 17, 256)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 4, 17, 256)   590080      activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 4, 17, 256)   1024        conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 4, 17, 256)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 4, 17, 1024)  263168      activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 4, 17, 1024)  0           add_202[0][0]                    \n",
      "                                                                 conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 4, 17, 1024)  4096        add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 4, 17, 256)   262400      activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 4, 17, 256)   1024        conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 4, 17, 256)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 4, 17, 256)   590080      activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 4, 17, 256)   1024        conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 4, 17, 256)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 4, 17, 1024)  263168      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 4, 17, 1024)  0           add_203[0][0]                    \n",
      "                                                                 conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 4, 17, 1024)  4096        add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 4, 17, 256)   262400      activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 4, 17, 256)   1024        conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 4, 17, 256)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 4, 17, 256)   590080      activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 4, 17, 256)   1024        conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 4, 17, 256)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 4, 17, 1024)  263168      activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 4, 17, 1024)  0           add_204[0][0]                    \n",
      "                                                                 conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 4, 17, 1024)  4096        add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 4, 17, 256)   262400      activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 4, 17, 256)   1024        conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 4, 17, 256)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 4, 17, 256)   590080      activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 4, 17, 256)   1024        conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 4, 17, 256)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 4, 17, 1024)  263168      activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 4, 17, 1024)  0           add_205[0][0]                    \n",
      "                                                                 conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 4, 17, 1024)  4096        add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 4, 17, 256)   262400      activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 4, 17, 256)   1024        conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 4, 17, 256)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 4, 17, 256)   590080      activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 4, 17, 256)   1024        conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 4, 17, 256)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 4, 17, 1024)  263168      activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 4, 17, 1024)  0           add_206[0][0]                    \n",
      "                                                                 conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 4, 17, 1024)  4096        add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 4, 17, 256)   262400      activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 4, 17, 256)   1024        conv2d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 4, 17, 256)   0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 4, 17, 256)   590080      activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 4, 17, 256)   1024        conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 4, 17, 256)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 4, 17, 1024)  263168      activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_208 (Add)                   (None, 4, 17, 1024)  0           add_207[0][0]                    \n",
      "                                                                 conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 4, 17, 1024)  4096        add_208[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 4, 17, 256)   262400      activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 4, 17, 256)   1024        conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 4, 17, 256)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 4, 17, 256)   590080      activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 4, 17, 256)   1024        conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 4, 17, 256)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 4, 17, 1024)  263168      activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_209 (Add)                   (None, 4, 17, 1024)  0           add_208[0][0]                    \n",
      "                                                                 conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 4, 17, 1024)  4096        add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 4, 17, 256)   262400      activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 4, 17, 256)   1024        conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 4, 17, 256)   0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 4, 17, 256)   590080      activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 4, 17, 256)   1024        conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 4, 17, 256)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 4, 17, 1024)  263168      activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_210 (Add)                   (None, 4, 17, 1024)  0           add_209[0][0]                    \n",
      "                                                                 conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 4, 17, 1024)  4096        add_210[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 4, 17, 256)   262400      activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 4, 17, 256)   1024        conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 4, 17, 256)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 4, 17, 256)   590080      activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 4, 17, 256)   1024        conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 4, 17, 256)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 4, 17, 1024)  263168      activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_211 (Add)                   (None, 4, 17, 1024)  0           add_210[0][0]                    \n",
      "                                                                 conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 4, 17, 1024)  4096        add_211[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 4, 17, 256)   262400      activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 4, 17, 256)   1024        conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 4, 17, 256)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 4, 17, 256)   590080      activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 4, 17, 256)   1024        conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 4, 17, 256)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 4, 17, 1024)  263168      activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_212 (Add)                   (None, 4, 17, 1024)  0           add_211[0][0]                    \n",
      "                                                                 conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 4, 17, 1024)  4096        add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 4, 17, 256)   262400      activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 4, 17, 256)   1024        conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 4, 17, 256)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 4, 17, 256)   590080      activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 4, 17, 256)   1024        conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 4, 17, 256)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 4, 17, 1024)  263168      activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_213 (Add)                   (None, 4, 17, 1024)  0           add_212[0][0]                    \n",
      "                                                                 conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 4, 17, 1024)  4096        add_213[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 4, 17, 256)   262400      activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 4, 17, 256)   1024        conv2d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 4, 17, 256)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 4, 17, 256)   590080      activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 4, 17, 256)   1024        conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 4, 17, 256)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 4, 17, 1024)  263168      activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_214 (Add)                   (None, 4, 17, 1024)  0           add_213[0][0]                    \n",
      "                                                                 conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 4, 17, 1024)  4096        add_214[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 4, 17, 256)   262400      activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 4, 17, 256)   1024        conv2d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 4, 17, 256)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 4, 17, 256)   590080      activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 4, 17, 256)   1024        conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 4, 17, 256)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 4, 17, 1024)  263168      activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_215 (Add)                   (None, 4, 17, 1024)  0           add_214[0][0]                    \n",
      "                                                                 conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 4, 17, 1024)  4096        add_215[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 4, 17, 256)   262400      activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 4, 17, 256)   1024        conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 4, 17, 256)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 4, 17, 256)   590080      activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 4, 17, 256)   1024        conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 4, 17, 256)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 4, 17, 1024)  263168      activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_216 (Add)                   (None, 4, 17, 1024)  0           add_215[0][0]                    \n",
      "                                                                 conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 4, 17, 1024)  4096        add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 2, 9, 512)    524800      activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 2, 9, 512)    2048        conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 2, 9, 512)    0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 2, 9, 512)    2359808     activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 2, 9, 512)    2048        conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 2, 9, 512)    0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 2, 9, 2048)   2099200     add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_217 (Add)                   (None, 2, 9, 2048)   0           conv2d_633[0][0]                 \n",
      "                                                                 conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 2, 9, 2048)   8192        add_217[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 2, 9, 512)    1049088     activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 2, 9, 512)    2048        conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 2, 9, 512)    0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 2, 9, 512)    2359808     activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 2, 9, 512)    2048        conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 2, 9, 512)    0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_218 (Add)                   (None, 2, 9, 2048)   0           add_217[0][0]                    \n",
      "                                                                 conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 2, 9, 2048)   8192        add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 2, 9, 512)    1049088     activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 2, 9, 512)    2048        conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 2, 9, 512)    0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 2, 9, 512)    2359808     activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 2, 9, 512)    2048        conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 2, 9, 512)    0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_219 (Add)                   (None, 2, 9, 2048)   0           add_218[0][0]                    \n",
      "                                                                 conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 2, 9, 2048)   8192        add_219[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 2048)   0           activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 53)           108597      flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 53)           212         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 53)           0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           2214        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,747,567\n",
      "Trainable params: 42,649,797\n",
      "Non-trainable params: 97,770\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "104/104 [==============================] - 32s 309ms/step - loss: 13.3760 - acc: 0.0484 - val_loss: 11.4375 - val_acc: 0.0970\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.09704, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 2/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 9.9622 - acc: 0.0787 - val_loss: 9.1653 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.09704\n",
      "Epoch 3/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 7.6242 - acc: 0.0802 - val_loss: 6.6265 - val_acc: 0.1078\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.09704 to 0.10782, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 4/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 6.1890 - acc: 0.0913 - val_loss: 5.6130 - val_acc: 0.1186\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.10782 to 0.11860, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 5.3541 - acc: 0.1007 - val_loss: 4.9472 - val_acc: 0.1024\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.11860\n",
      "Epoch 6/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 4.7893 - acc: 0.1142 - val_loss: 5.5962 - val_acc: 0.0566\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.11860\n",
      "Epoch 7/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 4.4489 - acc: 0.1259 - val_loss: 5.8441 - val_acc: 0.0404\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.11860\n",
      "Epoch 8/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 4.1622 - acc: 0.1478 - val_loss: 3.8388 - val_acc: 0.1375\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.11860 to 0.13747, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 9/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.9620 - acc: 0.1596 - val_loss: 4.5607 - val_acc: 0.1024\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.13747\n",
      "Epoch 10/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.8204 - acc: 0.1605 - val_loss: 3.1612 - val_acc: 0.2318\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.13747 to 0.23181, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 11/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.7270 - acc: 0.1749 - val_loss: 4.6787 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.23181\n",
      "Epoch 12/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.5893 - acc: 0.2001 - val_loss: 3.4752 - val_acc: 0.1995\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.23181\n",
      "Epoch 13/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.4940 - acc: 0.2109 - val_loss: 3.2462 - val_acc: 0.1941\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.23181\n",
      "Epoch 14/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 3.4775 - acc: 0.2004 - val_loss: 3.3004 - val_acc: 0.1833\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.23181\n",
      "Epoch 15/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.3974 - acc: 0.2218 - val_loss: 3.1354 - val_acc: 0.2399\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.23181 to 0.23989, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 16/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.3348 - acc: 0.2401 - val_loss: 3.2077 - val_acc: 0.2183\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.23989\n",
      "Epoch 17/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2869 - acc: 0.2503 - val_loss: 4.7257 - val_acc: 0.1752\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.23989\n",
      "Epoch 18/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2732 - acc: 0.2500 - val_loss: 2.9575 - val_acc: 0.2049\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.23989\n",
      "Epoch 19/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2107 - acc: 0.2587 - val_loss: 2.9994 - val_acc: 0.2695\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.23989 to 0.26954, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 20/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2043 - acc: 0.2662 - val_loss: 3.2953 - val_acc: 0.2588\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.26954\n",
      "Epoch 21/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.2032 - acc: 0.2641 - val_loss: 3.4943 - val_acc: 0.2075\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.26954\n",
      "Epoch 22/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1336 - acc: 0.2791 - val_loss: 2.9502 - val_acc: 0.2264\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.26954\n",
      "Epoch 23/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1512 - acc: 0.2674 - val_loss: 2.6885 - val_acc: 0.2803\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.26954 to 0.28032, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 24/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1189 - acc: 0.2855 - val_loss: 2.6818 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.28032\n",
      "Epoch 25/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.1224 - acc: 0.2906 - val_loss: 2.9823 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.28032\n",
      "Epoch 26/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0853 - acc: 0.2927 - val_loss: 3.2746 - val_acc: 0.2156\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.28032\n",
      "Epoch 27/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0823 - acc: 0.2909 - val_loss: 2.6944 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.28032 to 0.33423, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 28/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0586 - acc: 0.3074 - val_loss: 2.7699 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.33423\n",
      "Epoch 29/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0548 - acc: 0.2900 - val_loss: 2.6674 - val_acc: 0.2857\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.33423\n",
      "Epoch 30/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0622 - acc: 0.2990 - val_loss: 2.8039 - val_acc: 0.3100\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.33423\n",
      "Epoch 31/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0584 - acc: 0.3026 - val_loss: 2.6839 - val_acc: 0.3073\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.33423\n",
      "Epoch 32/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0092 - acc: 0.3224 - val_loss: 2.3437 - val_acc: 0.3693\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.33423 to 0.36927, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 33/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9971 - acc: 0.3149 - val_loss: 2.3258 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.36927 to 0.42049, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 34/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9981 - acc: 0.3233 - val_loss: 2.2911 - val_acc: 0.3908\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.42049\n",
      "Epoch 35/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 3.0481 - acc: 0.3059 - val_loss: 2.4513 - val_acc: 0.3585\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.42049\n",
      "Epoch 36/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9874 - acc: 0.3203 - val_loss: 2.4716 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.42049\n",
      "Epoch 37/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9505 - acc: 0.3347 - val_loss: 2.4600 - val_acc: 0.3612\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.42049\n",
      "Epoch 38/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9482 - acc: 0.3425 - val_loss: 2.5134 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.42049\n",
      "Epoch 39/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9163 - acc: 0.3456 - val_loss: 2.1414 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.42049 to 0.44744, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 40/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9393 - acc: 0.3383 - val_loss: 2.7708 - val_acc: 0.3046\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.44744\n",
      "Epoch 41/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9414 - acc: 0.3447 - val_loss: 2.2969 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.44744\n",
      "Epoch 42/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.9453 - acc: 0.3501 - val_loss: 2.1681 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.44744 to 0.45283, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 43/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8943 - acc: 0.3570 - val_loss: 2.2468 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.45283\n",
      "Epoch 44/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8965 - acc: 0.3567 - val_loss: 2.3220 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.45283\n",
      "Epoch 45/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8890 - acc: 0.3525 - val_loss: 2.3919 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.45283\n",
      "Epoch 46/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8549 - acc: 0.3645 - val_loss: 2.0684 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.45283\n",
      "Epoch 47/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8364 - acc: 0.3744 - val_loss: 2.0704 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.45283 to 0.46900, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 48/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8389 - acc: 0.3849 - val_loss: 2.7072 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.46900\n",
      "Epoch 49/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8553 - acc: 0.3753 - val_loss: 2.2492 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.46900\n",
      "Epoch 50/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8488 - acc: 0.3789 - val_loss: 2.7633 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.46900\n",
      "Epoch 51/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.8387 - acc: 0.3774 - val_loss: 2.3037 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.46900\n",
      "Epoch 52/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8298 - acc: 0.3804 - val_loss: 2.1228 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.46900\n",
      "Epoch 53/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7910 - acc: 0.3939 - val_loss: 2.1783 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.46900\n",
      "Epoch 54/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8107 - acc: 0.3852 - val_loss: 2.5295 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.46900\n",
      "Epoch 55/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7885 - acc: 0.3900 - val_loss: 2.2072 - val_acc: 0.4178\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.46900\n",
      "Epoch 56/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7815 - acc: 0.3996 - val_loss: 2.0184 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.46900\n",
      "Epoch 57/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.8000 - acc: 0.3900 - val_loss: 2.1208 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.46900 to 0.49865, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 58/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7826 - acc: 0.3978 - val_loss: 2.2737 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.49865\n",
      "Epoch 59/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7821 - acc: 0.4090 - val_loss: 1.9779 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.49865\n",
      "Epoch 60/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7570 - acc: 0.4099 - val_loss: 2.0789 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.49865 to 0.51752, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 61/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7887 - acc: 0.4123 - val_loss: 1.9556 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.51752 to 0.52022, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 62/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.7375 - acc: 0.4117 - val_loss: 2.1493 - val_acc: 0.4501\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.52022\n",
      "Epoch 63/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7536 - acc: 0.4114 - val_loss: 1.9850 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.52022\n",
      "Epoch 64/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7636 - acc: 0.4099 - val_loss: 2.2549 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.52022\n",
      "Epoch 65/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7801 - acc: 0.4072 - val_loss: 2.1358 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.52022\n",
      "Epoch 66/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7637 - acc: 0.4168 - val_loss: 2.3267 - val_acc: 0.3881\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.52022\n",
      "Epoch 67/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7005 - acc: 0.4312 - val_loss: 1.8387 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.52022 to 0.55256, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 68/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7113 - acc: 0.4207 - val_loss: 1.9080 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.55256\n",
      "Epoch 69/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.7122 - acc: 0.4198 - val_loss: 2.0377 - val_acc: 0.4825\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.55256\n",
      "Epoch 70/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6661 - acc: 0.4495 - val_loss: 2.4363 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.55256\n",
      "Epoch 71/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6911 - acc: 0.4282 - val_loss: 1.9419 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.55256\n",
      "Epoch 72/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6831 - acc: 0.4372 - val_loss: 1.7359 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.55256\n",
      "Epoch 73/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6814 - acc: 0.4471 - val_loss: 2.1874 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.55256\n",
      "Epoch 74/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6645 - acc: 0.4432 - val_loss: 1.7770 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.55256\n",
      "Epoch 75/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6698 - acc: 0.4354 - val_loss: 1.7869 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.55256\n",
      "Epoch 76/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6537 - acc: 0.4573 - val_loss: 1.7357 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.55256\n",
      "Epoch 77/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6562 - acc: 0.4534 - val_loss: 2.1362 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.55256\n",
      "Epoch 78/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6300 - acc: 0.4486 - val_loss: 2.3665 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.55256\n",
      "Epoch 79/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6350 - acc: 0.4609 - val_loss: 2.2681 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.55256\n",
      "Epoch 80/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6109 - acc: 0.4573 - val_loss: 1.8385 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.55256\n",
      "Epoch 81/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6460 - acc: 0.4516 - val_loss: 2.2295 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.55256\n",
      "Epoch 82/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6526 - acc: 0.4462 - val_loss: 2.5134 - val_acc: 0.3774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_acc did not improve from 0.55256\n",
      "Epoch 83/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6303 - acc: 0.4697 - val_loss: 1.6977 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.55256 to 0.59838, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 84/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6500 - acc: 0.4549 - val_loss: 2.3124 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.59838\n",
      "Epoch 85/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5910 - acc: 0.4681 - val_loss: 1.8620 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.59838\n",
      "Epoch 86/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6366 - acc: 0.4561 - val_loss: 1.6842 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.59838\n",
      "Epoch 87/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6128 - acc: 0.4513 - val_loss: 2.2236 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.59838\n",
      "Epoch 88/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.5987 - acc: 0.4742 - val_loss: 1.7575 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.59838\n",
      "Epoch 89/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6002 - acc: 0.4784 - val_loss: 1.8336 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.59838\n",
      "Epoch 90/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.6001 - acc: 0.4709 - val_loss: 1.7059 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.59838\n",
      "Epoch 91/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5804 - acc: 0.4811 - val_loss: 1.8844 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.59838\n",
      "Epoch 92/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5650 - acc: 0.4904 - val_loss: 1.8378 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.59838\n",
      "Epoch 93/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5761 - acc: 0.4730 - val_loss: 2.1639 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.59838\n",
      "Epoch 94/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5775 - acc: 0.4793 - val_loss: 1.7873 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.59838\n",
      "Epoch 95/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5830 - acc: 0.4910 - val_loss: 2.1218 - val_acc: 0.4744\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.59838\n",
      "Epoch 96/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5818 - acc: 0.4784 - val_loss: 1.9914 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.59838\n",
      "Epoch 97/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5866 - acc: 0.4684 - val_loss: 1.7562 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.59838\n",
      "Epoch 98/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5884 - acc: 0.4742 - val_loss: 2.2572 - val_acc: 0.4151\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.59838\n",
      "Epoch 99/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.5573 - acc: 0.4877 - val_loss: 1.6709 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.59838\n",
      "Epoch 100/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5607 - acc: 0.4754 - val_loss: 1.5671 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.59838\n",
      "Epoch 101/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5263 - acc: 0.5039 - val_loss: 1.8124 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.59838\n",
      "Epoch 102/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5728 - acc: 0.4787 - val_loss: 2.1612 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.59838\n",
      "Epoch 103/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5491 - acc: 0.4901 - val_loss: 1.6900 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.59838\n",
      "Epoch 104/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5442 - acc: 0.4901 - val_loss: 1.7118 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.59838\n",
      "Epoch 105/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5523 - acc: 0.4853 - val_loss: 1.9534 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.59838\n",
      "Epoch 106/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5138 - acc: 0.5027 - val_loss: 1.6562 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.59838\n",
      "Epoch 107/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5309 - acc: 0.4937 - val_loss: 1.8808 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.59838\n",
      "Epoch 108/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5045 - acc: 0.5051 - val_loss: 1.8519 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.59838\n",
      "Epoch 109/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5118 - acc: 0.5027 - val_loss: 1.7220 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.59838\n",
      "Epoch 110/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4993 - acc: 0.5177 - val_loss: 1.8604 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.59838\n",
      "Epoch 111/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4908 - acc: 0.5072 - val_loss: 1.9922 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.59838\n",
      "Epoch 112/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5110 - acc: 0.5021 - val_loss: 2.0670 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.59838\n",
      "Epoch 113/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5004 - acc: 0.5051 - val_loss: 2.2915 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.59838\n",
      "Epoch 114/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5066 - acc: 0.5000 - val_loss: 2.7619 - val_acc: 0.3558\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.59838\n",
      "Epoch 115/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4747 - acc: 0.5198 - val_loss: 1.6341 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00115: val_acc improved from 0.59838 to 0.61725, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 116/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.5105 - acc: 0.5063 - val_loss: 1.7404 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.61725\n",
      "Epoch 117/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4737 - acc: 0.5225 - val_loss: 1.8199 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.61725\n",
      "Epoch 118/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4736 - acc: 0.5129 - val_loss: 1.6340 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.61725\n",
      "Epoch 119/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4767 - acc: 0.5054 - val_loss: 1.8085 - val_acc: 0.5310\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.61725\n",
      "Epoch 120/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4654 - acc: 0.5243 - val_loss: 1.7456 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.61725\n",
      "Epoch 121/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.4591 - acc: 0.5270 - val_loss: 1.6399 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.61725\n",
      "Epoch 122/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4434 - acc: 0.5237 - val_loss: 1.5050 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.61725\n",
      "Epoch 123/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4724 - acc: 0.5246 - val_loss: 1.7150 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.61725\n",
      "Epoch 124/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4809 - acc: 0.5219 - val_loss: 1.6722 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.61725\n",
      "Epoch 125/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4376 - acc: 0.5309 - val_loss: 1.4753 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00125: val_acc improved from 0.61725 to 0.64420, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 126/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4627 - acc: 0.5120 - val_loss: 1.6153 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.64420\n",
      "Epoch 127/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4655 - acc: 0.5264 - val_loss: 1.7719 - val_acc: 0.5364\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.64420\n",
      "Epoch 128/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4472 - acc: 0.5279 - val_loss: 1.4644 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.64420 to 0.64420, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 129/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4598 - acc: 0.5237 - val_loss: 1.7405 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.64420\n",
      "Epoch 130/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4627 - acc: 0.5246 - val_loss: 1.6704 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.64420\n",
      "Epoch 131/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4403 - acc: 0.5319 - val_loss: 1.7909 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.64420\n",
      "Epoch 132/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4121 - acc: 0.5270 - val_loss: 1.5668 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.64420\n",
      "Epoch 133/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4541 - acc: 0.5174 - val_loss: 1.9243 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.64420\n",
      "Epoch 134/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4262 - acc: 0.5487 - val_loss: 1.8307 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.64420\n",
      "Epoch 135/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4329 - acc: 0.5276 - val_loss: 1.6366 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.64420\n",
      "Epoch 136/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4073 - acc: 0.5412 - val_loss: 1.4841 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.64420\n",
      "Epoch 137/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4255 - acc: 0.5225 - val_loss: 1.5846 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.64420\n",
      "Epoch 138/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3654 - acc: 0.5538 - val_loss: 1.6237 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.64420\n",
      "Epoch 139/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4329 - acc: 0.5379 - val_loss: 2.2759 - val_acc: 0.4205\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.64420\n",
      "Epoch 140/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4158 - acc: 0.5397 - val_loss: 1.6788 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.64420\n",
      "Epoch 141/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4370 - acc: 0.5349 - val_loss: 1.6345 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.64420\n",
      "Epoch 142/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4036 - acc: 0.5442 - val_loss: 1.5217 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.64420\n",
      "Epoch 143/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3661 - acc: 0.5475 - val_loss: 2.0611 - val_acc: 0.4798\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.64420\n",
      "Epoch 144/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4197 - acc: 0.5403 - val_loss: 1.5374 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.64420\n",
      "Epoch 145/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3897 - acc: 0.5535 - val_loss: 1.5892 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.64420\n",
      "Epoch 146/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4008 - acc: 0.5433 - val_loss: 1.4412 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.64420\n",
      "Epoch 147/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3975 - acc: 0.5388 - val_loss: 1.6185 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.64420\n",
      "Epoch 148/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3979 - acc: 0.5403 - val_loss: 1.7861 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.64420\n",
      "Epoch 149/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3726 - acc: 0.5502 - val_loss: 1.7012 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.64420\n",
      "Epoch 150/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3855 - acc: 0.5556 - val_loss: 1.6587 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.64420\n",
      "Epoch 151/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3877 - acc: 0.5520 - val_loss: 1.3957 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.64420 to 0.67385, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 152/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.4067 - acc: 0.5466 - val_loss: 1.4484 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.67385\n",
      "Epoch 153/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3883 - acc: 0.5469 - val_loss: 1.8142 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.67385\n",
      "Epoch 154/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3978 - acc: 0.5490 - val_loss: 1.8574 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.67385\n",
      "Epoch 155/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3722 - acc: 0.5562 - val_loss: 1.6603 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.67385\n",
      "Epoch 156/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3682 - acc: 0.5589 - val_loss: 1.5980 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.67385\n",
      "Epoch 157/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3456 - acc: 0.5691 - val_loss: 1.5606 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.67385\n",
      "Epoch 158/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3325 - acc: 0.5691 - val_loss: 1.6084 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.67385\n",
      "Epoch 159/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3598 - acc: 0.5610 - val_loss: 1.4255 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.67385\n",
      "Epoch 160/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3641 - acc: 0.5559 - val_loss: 1.7017 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.67385\n",
      "Epoch 161/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3785 - acc: 0.5532 - val_loss: 1.4723 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.67385\n",
      "Epoch 162/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.3546 - acc: 0.5616 - val_loss: 1.5701 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.67385\n",
      "Epoch 163/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3595 - acc: 0.5655 - val_loss: 1.4137 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.67385\n",
      "Epoch 164/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3395 - acc: 0.5649 - val_loss: 1.6430 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.67385\n",
      "Epoch 165/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3576 - acc: 0.5688 - val_loss: 1.5863 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.67385\n",
      "Epoch 166/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3761 - acc: 0.5562 - val_loss: 1.3600 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.67385\n",
      "Epoch 167/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3472 - acc: 0.5625 - val_loss: 2.0388 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.67385\n",
      "Epoch 168/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3539 - acc: 0.5565 - val_loss: 1.9881 - val_acc: 0.4879\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.67385\n",
      "Epoch 169/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3492 - acc: 0.5697 - val_loss: 1.7569 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.67385\n",
      "Epoch 170/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3426 - acc: 0.5685 - val_loss: 1.5321 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.67385\n",
      "Epoch 171/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3592 - acc: 0.5583 - val_loss: 1.5958 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.67385\n",
      "Epoch 172/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3408 - acc: 0.5793 - val_loss: 1.3412 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.67385\n",
      "Epoch 173/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3319 - acc: 0.5721 - val_loss: 1.7396 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.67385\n",
      "Epoch 174/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3504 - acc: 0.5706 - val_loss: 1.4273 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.67385\n",
      "Epoch 175/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3202 - acc: 0.5886 - val_loss: 1.9042 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.67385\n",
      "Epoch 176/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3492 - acc: 0.5775 - val_loss: 1.4455 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.67385\n",
      "Epoch 177/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2987 - acc: 0.5796 - val_loss: 1.9618 - val_acc: 0.4852\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.67385\n",
      "Epoch 178/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3157 - acc: 0.5739 - val_loss: 1.5500 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.67385\n",
      "Epoch 179/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3214 - acc: 0.5808 - val_loss: 1.7937 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.67385\n",
      "Epoch 180/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3287 - acc: 0.5730 - val_loss: 1.5885 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.67385\n",
      "Epoch 181/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3112 - acc: 0.5754 - val_loss: 1.4034 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.67385\n",
      "Epoch 182/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3229 - acc: 0.5763 - val_loss: 1.3567 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.67385\n",
      "Epoch 183/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2804 - acc: 0.5962 - val_loss: 1.4140 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.67385\n",
      "Epoch 184/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.3090 - acc: 0.5829 - val_loss: 1.3772 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.67385\n",
      "Epoch 185/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.2822 - acc: 0.5892 - val_loss: 1.5597 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.67385\n",
      "Epoch 186/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2900 - acc: 0.6001 - val_loss: 1.5213 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.67385\n",
      "Epoch 187/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2821 - acc: 0.6088 - val_loss: 1.4611 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.67385\n",
      "Epoch 188/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3372 - acc: 0.5865 - val_loss: 1.6932 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.67385\n",
      "Epoch 189/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2809 - acc: 0.6001 - val_loss: 1.7212 - val_acc: 0.5660\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.67385\n",
      "Epoch 190/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3027 - acc: 0.5953 - val_loss: 1.6482 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.67385\n",
      "Epoch 191/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3096 - acc: 0.5895 - val_loss: 1.5095 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.67385\n",
      "Epoch 192/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3133 - acc: 0.5802 - val_loss: 1.4957 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.67385\n",
      "Epoch 193/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.3010 - acc: 0.5859 - val_loss: 1.7213 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.67385\n",
      "Epoch 194/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2960 - acc: 0.5820 - val_loss: 2.0937 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.67385\n",
      "Epoch 195/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2813 - acc: 0.6025 - val_loss: 1.5650 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.67385\n",
      "Epoch 196/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2848 - acc: 0.5871 - val_loss: 1.8307 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.67385\n",
      "Epoch 197/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2899 - acc: 0.5934 - val_loss: 1.7104 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.67385\n",
      "Epoch 198/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2804 - acc: 0.5962 - val_loss: 1.4433 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.67385\n",
      "Epoch 199/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2646 - acc: 0.6022 - val_loss: 1.4124 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.67385\n",
      "Epoch 200/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2787 - acc: 0.6055 - val_loss: 1.4939 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.67385\n",
      "Epoch 201/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2714 - acc: 0.5944 - val_loss: 1.5784 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.67385\n",
      "Epoch 202/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2622 - acc: 0.5953 - val_loss: 1.7189 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.67385\n",
      "Epoch 203/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2461 - acc: 0.6070 - val_loss: 1.4696 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.67385\n",
      "Epoch 204/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2530 - acc: 0.6073 - val_loss: 1.4335 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.67385\n",
      "Epoch 205/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2618 - acc: 0.6001 - val_loss: 1.3803 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00205: val_acc improved from 0.67385 to 0.68733, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 206/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2849 - acc: 0.5962 - val_loss: 1.4073 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.68733\n",
      "Epoch 207/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2560 - acc: 0.6127 - val_loss: 1.3448 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00207: val_acc improved from 0.68733 to 0.69003, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 208/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2747 - acc: 0.5841 - val_loss: 1.4986 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.69003\n",
      "Epoch 209/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2692 - acc: 0.5995 - val_loss: 1.3139 - val_acc: 0.6954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00209: val_acc improved from 0.69003 to 0.69542, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 210/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2764 - acc: 0.6100 - val_loss: 1.4736 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.69542\n",
      "Epoch 211/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2535 - acc: 0.6016 - val_loss: 2.3240 - val_acc: 0.4636\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.69542\n",
      "Epoch 212/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2556 - acc: 0.6064 - val_loss: 1.3478 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.69542\n",
      "Epoch 213/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2493 - acc: 0.6004 - val_loss: 1.6621 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.69542\n",
      "Epoch 214/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2545 - acc: 0.6007 - val_loss: 1.6184 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.69542\n",
      "Epoch 215/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2463 - acc: 0.6085 - val_loss: 1.2520 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00215: val_acc improved from 0.69542 to 0.72776, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 216/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2635 - acc: 0.5856 - val_loss: 1.4411 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.72776\n",
      "Epoch 217/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2363 - acc: 0.6109 - val_loss: 1.2647 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.72776\n",
      "Epoch 218/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2549 - acc: 0.6061 - val_loss: 1.3616 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.72776\n",
      "Epoch 219/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2285 - acc: 0.6073 - val_loss: 1.7096 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.72776\n",
      "Epoch 220/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2337 - acc: 0.6091 - val_loss: 1.4332 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.72776\n",
      "Epoch 221/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2712 - acc: 0.5947 - val_loss: 1.5187 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.72776\n",
      "Epoch 222/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2955 - acc: 0.5868 - val_loss: 1.3979 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.72776\n",
      "Epoch 223/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2586 - acc: 0.6058 - val_loss: 1.2872 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.72776\n",
      "Epoch 224/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2392 - acc: 0.6181 - val_loss: 1.7485 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.72776\n",
      "Epoch 225/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2473 - acc: 0.6172 - val_loss: 1.6020 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.72776\n",
      "Epoch 226/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2375 - acc: 0.6292 - val_loss: 1.5538 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.72776\n",
      "Epoch 227/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2173 - acc: 0.6181 - val_loss: 1.3546 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.72776\n",
      "Epoch 228/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2321 - acc: 0.6181 - val_loss: 1.5280 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.72776\n",
      "Epoch 229/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2259 - acc: 0.6181 - val_loss: 1.4718 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.72776\n",
      "Epoch 230/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1983 - acc: 0.6262 - val_loss: 1.3121 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.72776\n",
      "Epoch 231/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2258 - acc: 0.6259 - val_loss: 1.3045 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.72776\n",
      "Epoch 232/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2226 - acc: 0.6061 - val_loss: 1.6657 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.72776\n",
      "Epoch 233/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2081 - acc: 0.6358 - val_loss: 1.4914 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.72776\n",
      "Epoch 234/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2048 - acc: 0.6214 - val_loss: 2.0802 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.72776\n",
      "Epoch 235/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2359 - acc: 0.6274 - val_loss: 1.3152 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.72776\n",
      "Epoch 236/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1950 - acc: 0.6334 - val_loss: 1.5774 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.72776\n",
      "Epoch 237/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2234 - acc: 0.6133 - val_loss: 1.3851 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.72776\n",
      "Epoch 238/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2532 - acc: 0.6130 - val_loss: 1.3604 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.72776\n",
      "Epoch 239/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1984 - acc: 0.6292 - val_loss: 1.7289 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.72776\n",
      "Epoch 240/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.2050 - acc: 0.6172 - val_loss: 1.3640 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.72776\n",
      "Epoch 241/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2111 - acc: 0.6223 - val_loss: 1.3221 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.72776\n",
      "Epoch 242/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2046 - acc: 0.6199 - val_loss: 1.5423 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.72776\n",
      "Epoch 243/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1834 - acc: 0.6349 - val_loss: 1.3481 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.72776\n",
      "Epoch 244/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2349 - acc: 0.6187 - val_loss: 1.3277 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.72776\n",
      "Epoch 245/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1966 - acc: 0.6289 - val_loss: 1.2797 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.72776\n",
      "Epoch 246/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2105 - acc: 0.6271 - val_loss: 1.3501 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.72776\n",
      "Epoch 247/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2060 - acc: 0.6358 - val_loss: 1.4767 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.72776\n",
      "Epoch 248/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2105 - acc: 0.6217 - val_loss: 1.4602 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.72776\n",
      "Epoch 249/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2025 - acc: 0.6367 - val_loss: 1.4184 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.72776\n",
      "Epoch 250/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1951 - acc: 0.6346 - val_loss: 1.5264 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.72776\n",
      "Epoch 251/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2079 - acc: 0.6277 - val_loss: 1.2014 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00251: val_acc improved from 0.72776 to 0.74663, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2047 - acc: 0.6244 - val_loss: 1.8090 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.74663\n",
      "Epoch 253/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1916 - acc: 0.6361 - val_loss: 1.4803 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.74663\n",
      "Epoch 254/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1622 - acc: 0.6535 - val_loss: 1.4311 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.74663\n",
      "Epoch 255/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2302 - acc: 0.6277 - val_loss: 1.7178 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.74663\n",
      "Epoch 256/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.2045 - acc: 0.6325 - val_loss: 1.4554 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.74663\n",
      "Epoch 257/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1558 - acc: 0.6388 - val_loss: 1.2614 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.74663\n",
      "Epoch 258/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1897 - acc: 0.6406 - val_loss: 1.6408 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.74663\n",
      "Epoch 259/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1572 - acc: 0.6502 - val_loss: 1.1478 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00259: val_acc improved from 0.74663 to 0.76011, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 260/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1553 - acc: 0.6385 - val_loss: 1.3442 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.76011\n",
      "Epoch 261/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1863 - acc: 0.6307 - val_loss: 1.2407 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.76011\n",
      "Epoch 262/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1902 - acc: 0.6421 - val_loss: 1.4609 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.76011\n",
      "Epoch 263/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1632 - acc: 0.6451 - val_loss: 1.5094 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.76011\n",
      "Epoch 264/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1886 - acc: 0.6448 - val_loss: 1.4353 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.76011\n",
      "Epoch 265/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1637 - acc: 0.6340 - val_loss: 1.2087 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.76011\n",
      "Epoch 266/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1879 - acc: 0.6358 - val_loss: 1.5911 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.76011\n",
      "Epoch 267/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1717 - acc: 0.6424 - val_loss: 1.4522 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.76011\n",
      "Epoch 268/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1717 - acc: 0.6382 - val_loss: 1.3617 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.76011\n",
      "Epoch 269/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1804 - acc: 0.6346 - val_loss: 1.4805 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.76011\n",
      "Epoch 270/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1786 - acc: 0.6361 - val_loss: 1.4846 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.76011\n",
      "Epoch 271/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1829 - acc: 0.6385 - val_loss: 1.8227 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.76011\n",
      "Epoch 272/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.1884 - acc: 0.6268 - val_loss: 1.4238 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.76011\n",
      "Epoch 273/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1675 - acc: 0.6406 - val_loss: 1.2447 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.76011\n",
      "Epoch 274/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1726 - acc: 0.6352 - val_loss: 1.3208 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.76011\n",
      "Epoch 275/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1793 - acc: 0.6355 - val_loss: 1.3380 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.76011\n",
      "Epoch 276/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1629 - acc: 0.6484 - val_loss: 1.2901 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.76011\n",
      "Epoch 277/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1930 - acc: 0.6355 - val_loss: 1.3354 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.76011\n",
      "Epoch 278/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1479 - acc: 0.6472 - val_loss: 1.3855 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.76011\n",
      "Epoch 279/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1647 - acc: 0.6400 - val_loss: 1.2017 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.76011\n",
      "Epoch 280/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1647 - acc: 0.6436 - val_loss: 1.3905 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.76011\n",
      "Epoch 281/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1579 - acc: 0.6559 - val_loss: 1.3379 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.76011\n",
      "Epoch 282/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1451 - acc: 0.6572 - val_loss: 1.5121 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.76011\n",
      "Epoch 283/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1522 - acc: 0.6520 - val_loss: 1.4511 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.76011\n",
      "Epoch 284/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1637 - acc: 0.6505 - val_loss: 1.2920 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.76011\n",
      "Epoch 285/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1820 - acc: 0.6457 - val_loss: 1.4288 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.76011\n",
      "Epoch 286/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1396 - acc: 0.6547 - val_loss: 1.7723 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.76011\n",
      "Epoch 287/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1521 - acc: 0.6490 - val_loss: 1.3949 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.76011\n",
      "Epoch 288/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1362 - acc: 0.6617 - val_loss: 1.3355 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.76011\n",
      "Epoch 289/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1579 - acc: 0.6517 - val_loss: 1.5759 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.76011\n",
      "Epoch 290/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1646 - acc: 0.6484 - val_loss: 1.5075 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.76011\n",
      "Epoch 291/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1563 - acc: 0.6448 - val_loss: 1.7078 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.76011\n",
      "Epoch 292/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1610 - acc: 0.6553 - val_loss: 1.3442 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.76011\n",
      "Epoch 293/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1462 - acc: 0.6478 - val_loss: 1.5131 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.76011\n",
      "Epoch 294/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1533 - acc: 0.6511 - val_loss: 1.4763 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.76011\n",
      "Epoch 295/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1325 - acc: 0.6538 - val_loss: 1.3034 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.76011\n",
      "Epoch 296/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1514 - acc: 0.6517 - val_loss: 1.4901 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.76011\n",
      "Epoch 297/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1486 - acc: 0.6508 - val_loss: 1.2521 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.76011\n",
      "Epoch 298/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1092 - acc: 0.6737 - val_loss: 1.2174 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.76011\n",
      "Epoch 299/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1451 - acc: 0.6575 - val_loss: 1.3106 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.76011\n",
      "Epoch 300/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1198 - acc: 0.6526 - val_loss: 1.4113 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.76011\n",
      "Epoch 301/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1467 - acc: 0.6475 - val_loss: 1.3549 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.76011\n",
      "Epoch 302/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1746 - acc: 0.6505 - val_loss: 1.5140 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.76011\n",
      "Epoch 303/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1209 - acc: 0.6650 - val_loss: 1.3698 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.76011\n",
      "Epoch 304/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1423 - acc: 0.6599 - val_loss: 1.1795 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.76011\n",
      "Epoch 305/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1125 - acc: 0.6656 - val_loss: 1.4738 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.76011\n",
      "Epoch 306/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1364 - acc: 0.6575 - val_loss: 1.4117 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.76011\n",
      "Epoch 307/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1225 - acc: 0.6746 - val_loss: 1.4611 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.76011\n",
      "Epoch 308/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1443 - acc: 0.6569 - val_loss: 1.7039 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.76011\n",
      "Epoch 309/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1499 - acc: 0.6478 - val_loss: 1.7642 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.76011\n",
      "Epoch 310/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1310 - acc: 0.6505 - val_loss: 1.3140 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.76011\n",
      "Epoch 311/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1217 - acc: 0.6749 - val_loss: 1.3427 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.76011\n",
      "Epoch 312/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1061 - acc: 0.6692 - val_loss: 1.4923 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.76011\n",
      "Epoch 313/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1230 - acc: 0.6508 - val_loss: 1.1707 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.76011\n",
      "Epoch 314/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1107 - acc: 0.6556 - val_loss: 1.5693 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.76011\n",
      "Epoch 315/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1032 - acc: 0.6638 - val_loss: 1.5349 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.76011\n",
      "Epoch 316/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1220 - acc: 0.6701 - val_loss: 1.3265 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.76011\n",
      "Epoch 317/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1363 - acc: 0.6623 - val_loss: 1.6065 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.76011\n",
      "Epoch 318/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0939 - acc: 0.6707 - val_loss: 1.4451 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.76011\n",
      "Epoch 319/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1406 - acc: 0.6674 - val_loss: 1.1805 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.76011\n",
      "Epoch 320/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1047 - acc: 0.6788 - val_loss: 1.4164 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.76011\n",
      "Epoch 321/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1041 - acc: 0.6635 - val_loss: 1.3584 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.76011\n",
      "Epoch 322/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1070 - acc: 0.6662 - val_loss: 1.4774 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.76011\n",
      "Epoch 323/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.1350 - acc: 0.6635 - val_loss: 1.1704 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.76011\n",
      "Epoch 324/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0794 - acc: 0.6881 - val_loss: 1.7919 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.76011\n",
      "Epoch 325/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1198 - acc: 0.6665 - val_loss: 1.3744 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.76011\n",
      "Epoch 326/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0942 - acc: 0.6764 - val_loss: 1.3311 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.76011\n",
      "Epoch 327/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1021 - acc: 0.6593 - val_loss: 1.3542 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.76011\n",
      "Epoch 328/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1053 - acc: 0.6668 - val_loss: 1.2933 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.76011\n",
      "Epoch 329/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0887 - acc: 0.6701 - val_loss: 1.4988 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.76011\n",
      "Epoch 330/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0779 - acc: 0.6875 - val_loss: 1.2454 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.76011\n",
      "Epoch 331/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1130 - acc: 0.6556 - val_loss: 1.3850 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.76011\n",
      "Epoch 332/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1172 - acc: 0.6569 - val_loss: 1.4160 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.76011\n",
      "Epoch 333/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1219 - acc: 0.6707 - val_loss: 1.3073 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.76011\n",
      "Epoch 334/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0760 - acc: 0.6731 - val_loss: 1.1290 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00334: val_acc improved from 0.76011 to 0.76550, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 335/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1130 - acc: 0.6680 - val_loss: 1.3220 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.76550\n",
      "Epoch 336/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0966 - acc: 0.6761 - val_loss: 1.3612 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.76550\n",
      "Epoch 337/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1091 - acc: 0.6797 - val_loss: 1.3348 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.76550\n",
      "Epoch 338/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1050 - acc: 0.6746 - val_loss: 1.3901 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.76550\n",
      "Epoch 339/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0847 - acc: 0.6677 - val_loss: 1.2867 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.76550\n",
      "Epoch 340/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0691 - acc: 0.6764 - val_loss: 1.3505 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.76550\n",
      "Epoch 341/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0988 - acc: 0.6746 - val_loss: 1.2817 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.76550\n",
      "Epoch 342/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0656 - acc: 0.6791 - val_loss: 1.4005 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.76550\n",
      "Epoch 343/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1029 - acc: 0.6755 - val_loss: 1.3200 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.76550\n",
      "Epoch 344/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0679 - acc: 0.6875 - val_loss: 1.3214 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.76550\n",
      "Epoch 345/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0998 - acc: 0.6602 - val_loss: 1.5151 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.76550\n",
      "Epoch 346/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0910 - acc: 0.6695 - val_loss: 1.1996 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.76550\n",
      "Epoch 347/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0914 - acc: 0.6677 - val_loss: 1.2809 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.76550\n",
      "Epoch 348/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0742 - acc: 0.6818 - val_loss: 1.5380 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.76550\n",
      "Epoch 349/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1074 - acc: 0.6818 - val_loss: 1.5222 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.76550\n",
      "Epoch 350/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0830 - acc: 0.6839 - val_loss: 1.5995 - val_acc: 0.6334\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.76550\n",
      "Epoch 351/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0749 - acc: 0.6815 - val_loss: 1.5206 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.76550\n",
      "Epoch 352/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0870 - acc: 0.6782 - val_loss: 1.2385 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.76550\n",
      "Epoch 353/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0790 - acc: 0.6770 - val_loss: 1.4381 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.76550\n",
      "Epoch 354/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0961 - acc: 0.6764 - val_loss: 1.8364 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.76550\n",
      "Epoch 355/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0828 - acc: 0.6797 - val_loss: 1.5120 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.76550\n",
      "Epoch 356/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.1388 - acc: 0.6614 - val_loss: 1.2597 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.76550\n",
      "Epoch 357/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0384 - acc: 0.6944 - val_loss: 1.1992 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.76550\n",
      "Epoch 358/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0714 - acc: 0.6896 - val_loss: 1.3367 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.76550\n",
      "Epoch 359/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0661 - acc: 0.6920 - val_loss: 1.3931 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.76550\n",
      "Epoch 360/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0616 - acc: 0.6923 - val_loss: 1.2357 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.76550\n",
      "Epoch 361/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0713 - acc: 0.6842 - val_loss: 1.1424 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00361: val_acc improved from 0.76550 to 0.77358, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 362/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0629 - acc: 0.7013 - val_loss: 1.6032 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.77358\n",
      "Epoch 363/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0507 - acc: 0.6872 - val_loss: 1.1277 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.77358\n",
      "Epoch 364/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0387 - acc: 0.6974 - val_loss: 1.3545 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.77358\n",
      "Epoch 365/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0872 - acc: 0.6737 - val_loss: 1.6273 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.77358\n",
      "Epoch 366/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0666 - acc: 0.6773 - val_loss: 1.3695 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.77358\n",
      "Epoch 367/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0619 - acc: 0.6791 - val_loss: 1.5065 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.77358\n",
      "Epoch 368/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0455 - acc: 0.6965 - val_loss: 1.4619 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.77358\n",
      "Epoch 369/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0917 - acc: 0.6764 - val_loss: 1.7090 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.77358\n",
      "Epoch 370/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0755 - acc: 0.6857 - val_loss: 1.1353 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.77358\n",
      "Epoch 371/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0907 - acc: 0.6800 - val_loss: 1.6262 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.77358\n",
      "Epoch 372/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0421 - acc: 0.7010 - val_loss: 1.3477 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.77358\n",
      "Epoch 373/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0454 - acc: 0.6890 - val_loss: 1.3802 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.77358\n",
      "Epoch 374/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0816 - acc: 0.6716 - val_loss: 1.6592 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.77358\n",
      "Epoch 375/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0650 - acc: 0.6890 - val_loss: 1.6290 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.77358\n",
      "Epoch 376/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0481 - acc: 0.6893 - val_loss: 1.5552 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.77358\n",
      "Epoch 377/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0421 - acc: 0.6965 - val_loss: 1.3053 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.77358\n",
      "Epoch 378/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0449 - acc: 0.6905 - val_loss: 1.4365 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.77358\n",
      "Epoch 379/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0518 - acc: 0.6743 - val_loss: 1.2428 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.77358\n",
      "Epoch 380/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0445 - acc: 0.6848 - val_loss: 1.3195 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.77358\n",
      "Epoch 381/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0470 - acc: 0.6923 - val_loss: 1.4699 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.77358\n",
      "Epoch 382/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0881 - acc: 0.6788 - val_loss: 1.5779 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.77358\n",
      "Epoch 383/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0749 - acc: 0.6971 - val_loss: 1.4672 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.77358\n",
      "Epoch 384/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0600 - acc: 0.6953 - val_loss: 1.4049 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.77358\n",
      "Epoch 385/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0634 - acc: 0.6848 - val_loss: 1.3446 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.77358\n",
      "Epoch 386/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0518 - acc: 0.6923 - val_loss: 1.3297 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.77358\n",
      "Epoch 387/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0437 - acc: 0.6866 - val_loss: 1.4644 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.77358\n",
      "Epoch 388/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0570 - acc: 0.6890 - val_loss: 1.3059 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.77358\n",
      "Epoch 389/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0447 - acc: 0.6962 - val_loss: 1.4577 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.77358\n",
      "Epoch 390/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0629 - acc: 0.6857 - val_loss: 1.1393 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.77358\n",
      "Epoch 391/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0166 - acc: 0.7028 - val_loss: 1.2743 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.77358\n",
      "Epoch 392/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0635 - acc: 0.6887 - val_loss: 1.3475 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.77358\n",
      "Epoch 393/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0494 - acc: 0.6887 - val_loss: 1.1838 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00393: val_acc improved from 0.77358 to 0.77898, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 394/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0587 - acc: 0.6965 - val_loss: 1.4799 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.77898\n",
      "Epoch 395/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0482 - acc: 0.6932 - val_loss: 1.3315 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.77898\n",
      "Epoch 396/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0640 - acc: 0.6761 - val_loss: 1.6589 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.77898\n",
      "Epoch 397/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0909 - acc: 0.6767 - val_loss: 1.3222 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.77898\n",
      "Epoch 398/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0521 - acc: 0.6935 - val_loss: 1.4973 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.77898\n",
      "Epoch 399/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0587 - acc: 0.6908 - val_loss: 1.3065 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.77898\n",
      "Epoch 400/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0572 - acc: 0.6908 - val_loss: 1.2761 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.77898\n",
      "Epoch 401/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0123 - acc: 0.7094 - val_loss: 1.1416 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.77898\n",
      "Epoch 402/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0405 - acc: 0.6923 - val_loss: 1.2749 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.77898\n",
      "Epoch 403/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0246 - acc: 0.6986 - val_loss: 1.5348 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.77898\n",
      "Epoch 404/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0862 - acc: 0.6884 - val_loss: 1.7070 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.77898\n",
      "Epoch 405/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0469 - acc: 0.6995 - val_loss: 2.0891 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.77898\n",
      "Epoch 406/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0569 - acc: 0.6998 - val_loss: 1.3283 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.77898\n",
      "Epoch 407/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0110 - acc: 0.6950 - val_loss: 1.3723 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.77898\n",
      "Epoch 408/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0426 - acc: 0.6998 - val_loss: 1.3014 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.77898\n",
      "Epoch 409/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0501 - acc: 0.6899 - val_loss: 1.2723 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.77898\n",
      "Epoch 410/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0319 - acc: 0.7166 - val_loss: 1.2477 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.77898\n",
      "Epoch 411/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0427 - acc: 0.6929 - val_loss: 1.4089 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.77898\n",
      "Epoch 412/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0387 - acc: 0.7010 - val_loss: 1.3519 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.77898\n",
      "Epoch 413/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0507 - acc: 0.6962 - val_loss: 1.4193 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.77898\n",
      "Epoch 414/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0473 - acc: 0.7007 - val_loss: 1.4665 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.77898\n",
      "Epoch 415/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0517 - acc: 0.6926 - val_loss: 1.2426 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.77898\n",
      "Epoch 416/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0563 - acc: 0.7085 - val_loss: 1.2163 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.77898\n",
      "Epoch 417/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0433 - acc: 0.6977 - val_loss: 1.3318 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.77898\n",
      "Epoch 418/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 2.0097 - acc: 0.6983 - val_loss: 1.3255 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.77898\n",
      "Epoch 419/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0446 - acc: 0.6908 - val_loss: 1.2149 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.77898\n",
      "Epoch 420/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0133 - acc: 0.7016 - val_loss: 1.4827 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.77898\n",
      "Epoch 421/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0228 - acc: 0.6959 - val_loss: 1.4929 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.77898\n",
      "Epoch 422/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0348 - acc: 0.7070 - val_loss: 1.7366 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.77898\n",
      "Epoch 423/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0305 - acc: 0.6965 - val_loss: 1.7042 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.77898\n",
      "Epoch 424/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0198 - acc: 0.7058 - val_loss: 1.2009 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.77898\n",
      "Epoch 425/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0204 - acc: 0.7148 - val_loss: 1.6524 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.77898\n",
      "Epoch 426/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0210 - acc: 0.7052 - val_loss: 1.8983 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.77898\n",
      "Epoch 427/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0576 - acc: 0.6950 - val_loss: 2.1613 - val_acc: 0.4690\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.77898\n",
      "Epoch 428/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0394 - acc: 0.6863 - val_loss: 1.3578 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.77898\n",
      "Epoch 429/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0537 - acc: 0.6926 - val_loss: 1.4100 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.77898\n",
      "Epoch 430/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0426 - acc: 0.6983 - val_loss: 1.5267 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.77898\n",
      "Epoch 431/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0409 - acc: 0.7079 - val_loss: 1.4513 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.77898\n",
      "Epoch 432/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0584 - acc: 0.6896 - val_loss: 1.3273 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.77898\n",
      "Epoch 433/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0333 - acc: 0.6959 - val_loss: 1.4475 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.77898\n",
      "Epoch 434/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0245 - acc: 0.7019 - val_loss: 1.4843 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.77898\n",
      "Epoch 435/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0437 - acc: 0.7010 - val_loss: 1.2524 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.77898\n",
      "Epoch 436/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0045 - acc: 0.7184 - val_loss: 1.2109 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.77898\n",
      "Epoch 437/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9980 - acc: 0.7094 - val_loss: 1.2228 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.77898\n",
      "Epoch 438/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0366 - acc: 0.7079 - val_loss: 1.4893 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.77898\n",
      "Epoch 439/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0442 - acc: 0.6863 - val_loss: 1.4190 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.77898\n",
      "Epoch 440/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0350 - acc: 0.6989 - val_loss: 1.2536 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.77898\n",
      "Epoch 441/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9997 - acc: 0.7197 - val_loss: 1.3241 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.77898\n",
      "Epoch 442/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0143 - acc: 0.7016 - val_loss: 1.3494 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.77898\n",
      "Epoch 443/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0187 - acc: 0.7112 - val_loss: 1.2841 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.77898\n",
      "Epoch 444/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0186 - acc: 0.7061 - val_loss: 1.5130 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.77898\n",
      "Epoch 445/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0413 - acc: 0.7085 - val_loss: 1.2981 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.77898\n",
      "Epoch 446/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0204 - acc: 0.7079 - val_loss: 1.4171 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.77898\n",
      "Epoch 447/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9694 - acc: 0.7284 - val_loss: 1.5132 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.77898\n",
      "Epoch 448/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0207 - acc: 0.6914 - val_loss: 1.6377 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.77898\n",
      "Epoch 449/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 2.0307 - acc: 0.7037 - val_loss: 1.5491 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.77898\n",
      "Epoch 450/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0213 - acc: 0.6983 - val_loss: 1.3336 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.77898\n",
      "Epoch 451/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0207 - acc: 0.7085 - val_loss: 1.3609 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.77898\n",
      "Epoch 452/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0134 - acc: 0.7184 - val_loss: 1.2364 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.77898\n",
      "Epoch 453/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7067 - val_loss: 1.4391 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.77898\n",
      "Epoch 454/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0135 - acc: 0.7118 - val_loss: 1.0787 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00454: val_acc improved from 0.77898 to 0.78706, saving model to model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 455/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0329 - acc: 0.7043 - val_loss: 1.3795 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.78706\n",
      "Epoch 456/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0115 - acc: 0.7151 - val_loss: 1.6036 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.78706\n",
      "Epoch 457/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0225 - acc: 0.7061 - val_loss: 1.2865 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.78706\n",
      "Epoch 458/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0106 - acc: 0.7130 - val_loss: 1.1863 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.78706\n",
      "Epoch 459/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0308 - acc: 0.6956 - val_loss: 1.4159 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.78706\n",
      "Epoch 460/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0297 - acc: 0.7010 - val_loss: 1.2984 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.78706\n",
      "Epoch 461/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0239 - acc: 0.7049 - val_loss: 1.3225 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.78706\n",
      "Epoch 462/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0230 - acc: 0.7109 - val_loss: 1.3122 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.78706\n",
      "Epoch 463/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7163 - val_loss: 1.2647 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.78706\n",
      "Epoch 464/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0122 - acc: 0.7139 - val_loss: 1.4570 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.78706\n",
      "Epoch 465/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9956 - acc: 0.7239 - val_loss: 1.3036 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.78706\n",
      "Epoch 466/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9952 - acc: 0.7194 - val_loss: 1.1809 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.78706\n",
      "Epoch 467/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0068 - acc: 0.7154 - val_loss: 1.6629 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.78706\n",
      "Epoch 468/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9891 - acc: 0.7197 - val_loss: 1.3335 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.78706\n",
      "Epoch 469/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0107 - acc: 0.7160 - val_loss: 1.3074 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.78706\n",
      "Epoch 470/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0044 - acc: 0.7124 - val_loss: 1.4709 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.78706\n",
      "Epoch 471/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0102 - acc: 0.6983 - val_loss: 1.7916 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.78706\n",
      "Epoch 472/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0336 - acc: 0.7022 - val_loss: 1.2016 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.78706\n",
      "Epoch 473/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0152 - acc: 0.7046 - val_loss: 1.2247 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.78706\n",
      "Epoch 474/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9775 - acc: 0.7272 - val_loss: 1.1812 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.78706\n",
      "Epoch 475/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9749 - acc: 0.7269 - val_loss: 1.3025 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.78706\n",
      "Epoch 476/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 1.9965 - acc: 0.7148 - val_loss: 1.1594 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.78706\n",
      "Epoch 477/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0188 - acc: 0.7076 - val_loss: 1.2506 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.78706\n",
      "Epoch 478/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9955 - acc: 0.7308 - val_loss: 1.2627 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.78706\n",
      "Epoch 479/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0445 - acc: 0.7025 - val_loss: 1.5154 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.78706\n",
      "Epoch 480/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0096 - acc: 0.7275 - val_loss: 1.7855 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.78706\n",
      "Epoch 481/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9720 - acc: 0.7191 - val_loss: 1.4526 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.78706\n",
      "Epoch 482/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9906 - acc: 0.7070 - val_loss: 1.3813 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.78706\n",
      "Epoch 483/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0187 - acc: 0.7163 - val_loss: 1.2448 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.78706\n",
      "Epoch 484/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0065 - acc: 0.7233 - val_loss: 1.3578 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.78706\n",
      "Epoch 485/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9752 - acc: 0.7145 - val_loss: 1.2072 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.78706\n",
      "Epoch 486/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0134 - acc: 0.7142 - val_loss: 1.3697 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.78706\n",
      "Epoch 487/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9754 - acc: 0.7281 - val_loss: 1.4385 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.78706\n",
      "Epoch 488/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0085 - acc: 0.7013 - val_loss: 1.2965 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.78706\n",
      "Epoch 489/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0041 - acc: 0.7082 - val_loss: 1.3542 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.78706\n",
      "Epoch 490/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9956 - acc: 0.7154 - val_loss: 1.2308 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.78706\n",
      "Epoch 491/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9841 - acc: 0.7061 - val_loss: 1.2199 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.78706\n",
      "Epoch 492/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0010 - acc: 0.7136 - val_loss: 1.8585 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.78706\n",
      "Epoch 493/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9809 - acc: 0.7242 - val_loss: 1.4764 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.78706\n",
      "Epoch 494/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9691 - acc: 0.7365 - val_loss: 1.5158 - val_acc: 0.6631\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.78706\n",
      "Epoch 495/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9960 - acc: 0.7145 - val_loss: 1.3454 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.78706\n",
      "Epoch 496/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0033 - acc: 0.7197 - val_loss: 2.3917 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.78706\n",
      "Epoch 497/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9877 - acc: 0.7377 - val_loss: 1.5741 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.78706\n",
      "Epoch 498/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9932 - acc: 0.7191 - val_loss: 1.3902 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.78706\n",
      "Epoch 499/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9428 - acc: 0.7323 - val_loss: 1.3465 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.78706\n",
      "Epoch 500/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0062 - acc: 0.7082 - val_loss: 1.4115 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.78706\n",
      "Epoch 501/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9808 - acc: 0.7206 - val_loss: 1.4092 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00501: val_acc did not improve from 0.78706\n",
      "Epoch 502/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0219 - acc: 0.7028 - val_loss: 1.4202 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00502: val_acc did not improve from 0.78706\n",
      "Epoch 503/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0048 - acc: 0.7100 - val_loss: 1.2632 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00503: val_acc did not improve from 0.78706\n",
      "Epoch 504/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9778 - acc: 0.7163 - val_loss: 1.4013 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00504: val_acc did not improve from 0.78706\n",
      "Epoch 505/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9734 - acc: 0.7091 - val_loss: 1.2787 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00505: val_acc did not improve from 0.78706\n",
      "Epoch 506/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9922 - acc: 0.7178 - val_loss: 1.8232 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00506: val_acc did not improve from 0.78706\n",
      "Epoch 507/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9705 - acc: 0.7236 - val_loss: 1.3333 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00507: val_acc did not improve from 0.78706\n",
      "Epoch 508/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9896 - acc: 0.7191 - val_loss: 1.5818 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00508: val_acc did not improve from 0.78706\n",
      "Epoch 509/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9837 - acc: 0.7145 - val_loss: 2.5144 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00509: val_acc did not improve from 0.78706\n",
      "Epoch 510/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9553 - acc: 0.7371 - val_loss: 1.3212 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00510: val_acc did not improve from 0.78706\n",
      "Epoch 511/3000\n",
      "104/104 [==============================] - 19s 187ms/step - loss: 1.9787 - acc: 0.7178 - val_loss: 1.2502 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00511: val_acc did not improve from 0.78706\n",
      "Epoch 512/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9786 - acc: 0.7124 - val_loss: 1.3167 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00512: val_acc did not improve from 0.78706\n",
      "Epoch 513/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9549 - acc: 0.7239 - val_loss: 1.2101 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00513: val_acc did not improve from 0.78706\n",
      "Epoch 514/3000\n",
      "104/104 [==============================] - 20s 189ms/step - loss: 1.9964 - acc: 0.7145 - val_loss: 1.7197 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00514: val_acc did not improve from 0.78706\n",
      "Epoch 515/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9621 - acc: 0.7317 - val_loss: 1.5248 - val_acc: 0.6604\n",
      "\n",
      "Epoch 00515: val_acc did not improve from 0.78706\n",
      "Epoch 516/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0188 - acc: 0.6920 - val_loss: 1.8315 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00516: val_acc did not improve from 0.78706\n",
      "Epoch 517/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9479 - acc: 0.7341 - val_loss: 1.4312 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00517: val_acc did not improve from 0.78706\n",
      "Epoch 518/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9754 - acc: 0.7293 - val_loss: 1.3772 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00518: val_acc did not improve from 0.78706\n",
      "Epoch 519/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9637 - acc: 0.7260 - val_loss: 1.3656 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00519: val_acc did not improve from 0.78706\n",
      "Epoch 520/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9762 - acc: 0.7200 - val_loss: 1.4643 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00520: val_acc did not improve from 0.78706\n",
      "Epoch 521/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9838 - acc: 0.7275 - val_loss: 1.3820 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00521: val_acc did not improve from 0.78706\n",
      "Epoch 522/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9416 - acc: 0.7338 - val_loss: 1.1738 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00522: val_acc did not improve from 0.78706\n",
      "Epoch 523/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9993 - acc: 0.7142 - val_loss: 1.4438 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00523: val_acc did not improve from 0.78706\n",
      "Epoch 524/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9852 - acc: 0.7248 - val_loss: 1.2463 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00524: val_acc did not improve from 0.78706\n",
      "Epoch 525/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9500 - acc: 0.7362 - val_loss: 1.4168 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00525: val_acc did not improve from 0.78706\n",
      "Epoch 526/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9738 - acc: 0.7233 - val_loss: 1.9376 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00526: val_acc did not improve from 0.78706\n",
      "Epoch 527/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9989 - acc: 0.7157 - val_loss: 1.1348 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00527: val_acc did not improve from 0.78706\n",
      "Epoch 528/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9792 - acc: 0.7154 - val_loss: 1.1843 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00528: val_acc did not improve from 0.78706\n",
      "Epoch 529/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9462 - acc: 0.7365 - val_loss: 1.1959 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00529: val_acc did not improve from 0.78706\n",
      "Epoch 530/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9426 - acc: 0.7275 - val_loss: 1.3018 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00530: val_acc did not improve from 0.78706\n",
      "Epoch 531/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9814 - acc: 0.7263 - val_loss: 1.5158 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00531: val_acc did not improve from 0.78706\n",
      "Epoch 532/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9631 - acc: 0.7266 - val_loss: 1.4143 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00532: val_acc did not improve from 0.78706\n",
      "Epoch 533/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9708 - acc: 0.7160 - val_loss: 1.4053 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00533: val_acc did not improve from 0.78706\n",
      "Epoch 534/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9862 - acc: 0.7320 - val_loss: 1.2874 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00534: val_acc did not improve from 0.78706\n",
      "Epoch 535/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9769 - acc: 0.7248 - val_loss: 1.3139 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00535: val_acc did not improve from 0.78706\n",
      "Epoch 536/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0051 - acc: 0.7269 - val_loss: 1.5126 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00536: val_acc did not improve from 0.78706\n",
      "Epoch 537/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 2.0072 - acc: 0.7160 - val_loss: 1.9030 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00537: val_acc did not improve from 0.78706\n",
      "Epoch 538/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9623 - acc: 0.7221 - val_loss: 1.3258 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00538: val_acc did not improve from 0.78706\n",
      "Epoch 539/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9810 - acc: 0.7215 - val_loss: 1.2655 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00539: val_acc did not improve from 0.78706\n",
      "Epoch 540/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9267 - acc: 0.7464 - val_loss: 1.3738 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00540: val_acc did not improve from 0.78706\n",
      "Epoch 541/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9427 - acc: 0.7314 - val_loss: 1.2885 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00541: val_acc did not improve from 0.78706\n",
      "Epoch 542/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9740 - acc: 0.7287 - val_loss: 1.2382 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00542: val_acc did not improve from 0.78706\n",
      "Epoch 543/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9830 - acc: 0.7200 - val_loss: 1.2856 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00543: val_acc did not improve from 0.78706\n",
      "Epoch 544/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9574 - acc: 0.7317 - val_loss: 1.1107 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00544: val_acc did not improve from 0.78706\n",
      "Epoch 545/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9869 - acc: 0.7163 - val_loss: 1.2885 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00545: val_acc did not improve from 0.78706\n",
      "Epoch 546/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9841 - acc: 0.7109 - val_loss: 1.4684 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00546: val_acc did not improve from 0.78706\n",
      "Epoch 547/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9580 - acc: 0.7320 - val_loss: 1.3891 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00547: val_acc did not improve from 0.78706\n",
      "Epoch 548/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9821 - acc: 0.7272 - val_loss: 1.4118 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00548: val_acc did not improve from 0.78706\n",
      "Epoch 549/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9510 - acc: 0.7314 - val_loss: 1.3552 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00549: val_acc did not improve from 0.78706\n",
      "Epoch 550/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9446 - acc: 0.7365 - val_loss: 1.2315 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00550: val_acc did not improve from 0.78706\n",
      "Epoch 551/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9228 - acc: 0.7362 - val_loss: 1.2380 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00551: val_acc did not improve from 0.78706\n",
      "Epoch 552/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9625 - acc: 0.7329 - val_loss: 1.1993 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00552: val_acc did not improve from 0.78706\n",
      "Epoch 553/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9804 - acc: 0.7151 - val_loss: 1.7387 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00553: val_acc did not improve from 0.78706\n",
      "Epoch 554/3000\n",
      "104/104 [==============================] - 20s 188ms/step - loss: 1.9737 - acc: 0.7290 - val_loss: 1.6991 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00554: val_acc did not improve from 0.78706\n",
      "Epoch 00554: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_8===\n",
      "semi loading: model/mfcc7/LGD_fold8_resnet3-.h5\n",
      "Epoch 5/3000\n",
      "106/106 [==============================] - 33s 314ms/step - loss: 1.6954 - acc: 0.7759 - val_loss: 1.0037 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.80593, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 6/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6621 - acc: 0.8028 - val_loss: 0.9805 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.80593 to 0.81402, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 7/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6562 - acc: 0.8045 - val_loss: 0.9747 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81402\n",
      "Epoch 8/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.6381 - acc: 0.8090 - val_loss: 0.9812 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81402\n",
      "Epoch 9/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6305 - acc: 0.8037 - val_loss: 0.9925 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81402\n",
      "Epoch 10/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6125 - acc: 0.8063 - val_loss: 0.9783 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81402\n",
      "Epoch 11/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.5906 - acc: 0.8125 - val_loss: 0.9554 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81402 to 0.81941, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 12/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5885 - acc: 0.8143 - val_loss: 0.9430 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.81941 to 0.82480, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 13/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.5754 - acc: 0.8149 - val_loss: 0.9473 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82480\n",
      "Epoch 14/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.6330 - acc: 0.8034 - val_loss: 0.9206 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.82480 to 0.83019, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 15/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5997 - acc: 0.8116 - val_loss: 0.9660 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83019\n",
      "Epoch 16/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5658 - acc: 0.8216 - val_loss: 0.9434 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83019\n",
      "Epoch 17/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5660 - acc: 0.8243 - val_loss: 0.9415 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83019\n",
      "Epoch 18/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5419 - acc: 0.8213 - val_loss: 0.9328 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83019\n",
      "Epoch 19/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5579 - acc: 0.8205 - val_loss: 0.9580 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83019\n",
      "Epoch 20/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5718 - acc: 0.8278 - val_loss: 0.9217 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83019\n",
      "Epoch 21/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5603 - acc: 0.8314 - val_loss: 0.9579 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83019\n",
      "Epoch 22/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5543 - acc: 0.8269 - val_loss: 0.9416 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83019\n",
      "Epoch 23/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5962 - acc: 0.8007 - val_loss: 0.9196 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83019\n",
      "Epoch 24/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5422 - acc: 0.8231 - val_loss: 0.9301 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83019\n",
      "Epoch 25/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5383 - acc: 0.8219 - val_loss: 0.9532 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83019\n",
      "Epoch 26/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5582 - acc: 0.8128 - val_loss: 0.9224 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83019\n",
      "Epoch 27/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5618 - acc: 0.8131 - val_loss: 0.9069 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83019\n",
      "Epoch 28/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5461 - acc: 0.8237 - val_loss: 0.9382 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83019\n",
      "Epoch 29/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5588 - acc: 0.8299 - val_loss: 0.9674 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83019\n",
      "Epoch 30/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5323 - acc: 0.8237 - val_loss: 0.9295 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83019\n",
      "Epoch 31/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5306 - acc: 0.8269 - val_loss: 0.9295 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83019\n",
      "Epoch 32/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5128 - acc: 0.8349 - val_loss: 0.9343 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83019\n",
      "Epoch 33/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5407 - acc: 0.8187 - val_loss: 0.8957 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83019\n",
      "Epoch 34/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5204 - acc: 0.8216 - val_loss: 0.9226 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83019\n",
      "Epoch 35/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5302 - acc: 0.8296 - val_loss: 0.9205 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83019\n",
      "Epoch 36/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5232 - acc: 0.8299 - val_loss: 0.9170 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83019\n",
      "Epoch 37/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5313 - acc: 0.8261 - val_loss: 0.9232 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83019\n",
      "Epoch 38/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5300 - acc: 0.8252 - val_loss: 0.9178 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83019\n",
      "Epoch 39/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5115 - acc: 0.8290 - val_loss: 0.9130 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83019\n",
      "Epoch 40/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4939 - acc: 0.8278 - val_loss: 0.9285 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83019\n",
      "Epoch 41/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5096 - acc: 0.8228 - val_loss: 0.9109 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83019\n",
      "Epoch 42/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5154 - acc: 0.8325 - val_loss: 0.8982 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83019\n",
      "Epoch 43/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5062 - acc: 0.8325 - val_loss: 0.9415 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83019\n",
      "Epoch 44/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5139 - acc: 0.8249 - val_loss: 0.8943 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83019\n",
      "Epoch 45/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5248 - acc: 0.8172 - val_loss: 0.8979 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83019\n",
      "Epoch 46/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5342 - acc: 0.8219 - val_loss: 0.9035 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83019\n",
      "Epoch 47/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5035 - acc: 0.8281 - val_loss: 0.9120 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83019\n",
      "Epoch 48/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5161 - acc: 0.8202 - val_loss: 0.9037 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83019\n",
      "Epoch 49/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5108 - acc: 0.8305 - val_loss: 0.8860 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83019\n",
      "Epoch 50/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4824 - acc: 0.8314 - val_loss: 0.9062 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83019\n",
      "Epoch 51/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5229 - acc: 0.8225 - val_loss: 0.9024 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83019\n",
      "Epoch 52/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5000 - acc: 0.8355 - val_loss: 0.8834 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83019\n",
      "Epoch 53/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4995 - acc: 0.8317 - val_loss: 0.8926 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.83019 to 0.83288, saving model to model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "Epoch 54/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5089 - acc: 0.8314 - val_loss: 0.9144 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83288\n",
      "Epoch 55/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5159 - acc: 0.8278 - val_loss: 0.9119 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83288\n",
      "Epoch 56/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5035 - acc: 0.8296 - val_loss: 0.9224 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83288\n",
      "Epoch 57/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5009 - acc: 0.8299 - val_loss: 0.8811 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83288\n",
      "Epoch 58/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5038 - acc: 0.8331 - val_loss: 0.8927 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83288\n",
      "Epoch 59/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5013 - acc: 0.8328 - val_loss: 0.9045 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.83288\n",
      "Epoch 60/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.5137 - acc: 0.8331 - val_loss: 0.8804 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.83288\n",
      "Epoch 61/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5037 - acc: 0.8340 - val_loss: 0.8838 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.83288\n",
      "Epoch 62/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4840 - acc: 0.8302 - val_loss: 0.9034 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.83288\n",
      "Epoch 63/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5089 - acc: 0.8314 - val_loss: 0.8797 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.83288\n",
      "Epoch 64/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4913 - acc: 0.8290 - val_loss: 0.8885 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.83288\n",
      "Epoch 65/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4550 - acc: 0.8349 - val_loss: 0.8680 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.83288\n",
      "Epoch 66/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.5043 - acc: 0.8302 - val_loss: 0.8957 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.83288\n",
      "Epoch 67/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4884 - acc: 0.8272 - val_loss: 0.8961 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.83288\n",
      "Epoch 68/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4814 - acc: 0.8429 - val_loss: 0.8907 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.83288\n",
      "Epoch 69/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4812 - acc: 0.8320 - val_loss: 0.8749 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.83288\n",
      "Epoch 70/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4853 - acc: 0.8255 - val_loss: 0.8906 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.83288\n",
      "Epoch 71/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4592 - acc: 0.8435 - val_loss: 0.8799 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.83288\n",
      "Epoch 72/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4905 - acc: 0.8349 - val_loss: 0.8750 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.83288\n",
      "Epoch 73/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4731 - acc: 0.8364 - val_loss: 0.8832 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.83288\n",
      "Epoch 74/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4574 - acc: 0.8494 - val_loss: 0.8950 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.83288\n",
      "Epoch 75/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4858 - acc: 0.8461 - val_loss: 0.8962 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.83288\n",
      "Epoch 76/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4648 - acc: 0.8346 - val_loss: 0.8880 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.83288\n",
      "Epoch 77/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.4868 - acc: 0.8432 - val_loss: 0.9007 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.83288\n",
      "Epoch 78/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4769 - acc: 0.8458 - val_loss: 0.8923 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.83288\n",
      "Epoch 79/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4782 - acc: 0.8408 - val_loss: 0.8858 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.83288\n",
      "Epoch 80/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4700 - acc: 0.8402 - val_loss: 0.8861 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.83288\n",
      "Epoch 81/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4886 - acc: 0.8390 - val_loss: 0.8876 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.83288\n",
      "Epoch 82/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4719 - acc: 0.8308 - val_loss: 0.8962 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.83288\n",
      "Epoch 83/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4760 - acc: 0.8387 - val_loss: 0.8872 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.83288\n",
      "Epoch 84/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4715 - acc: 0.8311 - val_loss: 0.8831 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.83288\n",
      "Epoch 85/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4518 - acc: 0.8426 - val_loss: 0.8830 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.83288\n",
      "Epoch 86/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4767 - acc: 0.8429 - val_loss: 0.8907 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.83288\n",
      "Epoch 87/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4687 - acc: 0.8346 - val_loss: 0.8949 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.83288\n",
      "Epoch 88/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4649 - acc: 0.8470 - val_loss: 0.8966 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.83288\n",
      "Epoch 89/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4777 - acc: 0.8373 - val_loss: 0.8941 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.83288\n",
      "Epoch 90/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4694 - acc: 0.8390 - val_loss: 0.8980 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.83288\n",
      "Epoch 91/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4517 - acc: 0.8429 - val_loss: 0.8956 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.83288\n",
      "Epoch 92/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4699 - acc: 0.8379 - val_loss: 0.8959 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.83288\n",
      "Epoch 93/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4606 - acc: 0.8411 - val_loss: 0.8820 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.83288\n",
      "Epoch 94/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4667 - acc: 0.8423 - val_loss: 0.8905 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.83288\n",
      "Epoch 95/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4583 - acc: 0.8390 - val_loss: 0.8932 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.83288\n",
      "Epoch 96/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4780 - acc: 0.8379 - val_loss: 0.8952 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.83288\n",
      "Epoch 97/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4739 - acc: 0.8449 - val_loss: 0.8903 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.83288\n",
      "Epoch 98/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4674 - acc: 0.8384 - val_loss: 0.8894 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.83288\n",
      "Epoch 99/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4645 - acc: 0.8499 - val_loss: 0.8869 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.83288\n",
      "Epoch 100/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4782 - acc: 0.8370 - val_loss: 0.8845 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.83288\n",
      "Epoch 101/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4646 - acc: 0.8390 - val_loss: 0.8837 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.83288\n",
      "Epoch 102/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4608 - acc: 0.8467 - val_loss: 0.8930 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.83288\n",
      "Epoch 103/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4734 - acc: 0.8558 - val_loss: 0.8862 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.83288\n",
      "Epoch 104/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4435 - acc: 0.8393 - val_loss: 0.8881 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.83288\n",
      "Epoch 105/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4858 - acc: 0.8337 - val_loss: 0.8867 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.83288\n",
      "Epoch 106/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4321 - acc: 0.8449 - val_loss: 0.8787 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.83288\n",
      "Epoch 107/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4426 - acc: 0.8464 - val_loss: 0.8856 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.83288\n",
      "Epoch 108/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4731 - acc: 0.8272 - val_loss: 0.8849 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.83288\n",
      "Epoch 109/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4917 - acc: 0.8261 - val_loss: 0.8827 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.83288\n",
      "Epoch 110/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4400 - acc: 0.8390 - val_loss: 0.8812 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.83288\n",
      "Epoch 111/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4480 - acc: 0.8387 - val_loss: 0.8815 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.83288\n",
      "Epoch 112/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4467 - acc: 0.8408 - val_loss: 0.8811 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.83288\n",
      "Epoch 113/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4384 - acc: 0.8323 - val_loss: 0.8845 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.83288\n",
      "Epoch 114/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4837 - acc: 0.8261 - val_loss: 0.8889 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.83288\n",
      "Epoch 115/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4577 - acc: 0.8411 - val_loss: 0.8844 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.83288\n",
      "Epoch 116/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4734 - acc: 0.8432 - val_loss: 0.8840 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.83288\n",
      "Epoch 117/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4709 - acc: 0.8494 - val_loss: 0.8845 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.83288\n",
      "Epoch 118/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4689 - acc: 0.8420 - val_loss: 0.8839 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.83288\n",
      "Epoch 119/3000\n",
      "106/106 [==============================] - 20s 192ms/step - loss: 1.4724 - acc: 0.8402 - val_loss: 0.8829 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.83288\n",
      "Epoch 120/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4698 - acc: 0.8361 - val_loss: 0.8815 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.83288\n",
      "Epoch 121/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4807 - acc: 0.8364 - val_loss: 0.8860 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.83288\n",
      "Epoch 122/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4547 - acc: 0.8343 - val_loss: 0.8850 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.83288\n",
      "Epoch 123/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4584 - acc: 0.8373 - val_loss: 0.8847 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.83288\n",
      "Epoch 124/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4708 - acc: 0.8361 - val_loss: 0.8804 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.83288\n",
      "Epoch 125/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4715 - acc: 0.8396 - val_loss: 0.8835 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.83288\n",
      "Epoch 126/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4746 - acc: 0.8417 - val_loss: 0.8847 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.83288\n",
      "Epoch 127/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4921 - acc: 0.8352 - val_loss: 0.8873 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.83288\n",
      "Epoch 128/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4391 - acc: 0.8317 - val_loss: 0.8842 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.83288\n",
      "Epoch 129/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4706 - acc: 0.8340 - val_loss: 0.8877 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.83288\n",
      "Epoch 130/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4698 - acc: 0.8299 - val_loss: 0.8852 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.83288\n",
      "Epoch 131/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4631 - acc: 0.8343 - val_loss: 0.8834 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.83288\n",
      "Epoch 132/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4779 - acc: 0.8381 - val_loss: 0.8881 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.83288\n",
      "Epoch 133/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4720 - acc: 0.8278 - val_loss: 0.8864 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83288\n",
      "Epoch 134/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4443 - acc: 0.8464 - val_loss: 0.8858 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83288\n",
      "Epoch 135/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4431 - acc: 0.8482 - val_loss: 0.8866 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.83288\n",
      "Epoch 136/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4572 - acc: 0.8390 - val_loss: 0.8847 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83288\n",
      "Epoch 137/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4775 - acc: 0.8320 - val_loss: 0.8833 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83288\n",
      "Epoch 138/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4581 - acc: 0.8373 - val_loss: 0.8823 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83288\n",
      "Epoch 139/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4619 - acc: 0.8414 - val_loss: 0.8830 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83288\n",
      "Epoch 140/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4470 - acc: 0.8467 - val_loss: 0.8840 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83288\n",
      "Epoch 141/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4725 - acc: 0.8361 - val_loss: 0.8888 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83288\n",
      "Epoch 142/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4460 - acc: 0.8443 - val_loss: 0.8847 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83288\n",
      "Epoch 143/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4553 - acc: 0.8393 - val_loss: 0.8826 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83288\n",
      "Epoch 144/3000\n",
      "106/106 [==============================] - 20s 190ms/step - loss: 1.4603 - acc: 0.8452 - val_loss: 0.8809 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83288\n",
      "Epoch 145/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4508 - acc: 0.8414 - val_loss: 0.8864 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83288\n",
      "Epoch 146/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4637 - acc: 0.8384 - val_loss: 0.8801 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83288\n",
      "Epoch 147/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4542 - acc: 0.8435 - val_loss: 0.8808 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83288\n",
      "Epoch 148/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4501 - acc: 0.8429 - val_loss: 0.8829 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83288\n",
      "Epoch 149/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4516 - acc: 0.8452 - val_loss: 0.8802 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83288\n",
      "Epoch 150/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4630 - acc: 0.8396 - val_loss: 0.8808 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83288\n",
      "Epoch 151/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4438 - acc: 0.8399 - val_loss: 0.8851 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.83288\n",
      "Epoch 152/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4530 - acc: 0.8461 - val_loss: 0.8885 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83288\n",
      "Epoch 153/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4821 - acc: 0.8358 - val_loss: 0.8904 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83288\n",
      "Epoch 154/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4716 - acc: 0.8408 - val_loss: 0.8869 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83288\n",
      "Epoch 155/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4509 - acc: 0.8384 - val_loss: 0.8912 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83288\n",
      "Epoch 156/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4408 - acc: 0.8485 - val_loss: 0.8874 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.83288\n",
      "Epoch 157/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4516 - acc: 0.8393 - val_loss: 0.8886 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83288\n",
      "Epoch 158/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4604 - acc: 0.8367 - val_loss: 0.8872 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83288\n",
      "Epoch 159/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4616 - acc: 0.8432 - val_loss: 0.8828 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.83288\n",
      "Epoch 160/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4643 - acc: 0.8411 - val_loss: 0.8835 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83288\n",
      "Epoch 161/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4633 - acc: 0.8370 - val_loss: 0.8834 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83288\n",
      "Epoch 162/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4677 - acc: 0.8432 - val_loss: 0.8843 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83288\n",
      "Epoch 163/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4383 - acc: 0.8420 - val_loss: 0.8833 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83288\n",
      "Epoch 164/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4438 - acc: 0.8376 - val_loss: 0.8849 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83288\n",
      "Epoch 165/3000\n",
      "106/106 [==============================] - 20s 191ms/step - loss: 1.4597 - acc: 0.8494 - val_loss: 0.8843 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83288\n",
      "Epoch 00165: early stopping\n",
      "(3339, 60, 259, 1) (3339, 41)\n",
      "===train verified_fold9_mfcc7===\n",
      "using resnet model: 1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 30, 130, 64)  3200        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 30, 130, 64)  256         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 30, 130, 64)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 15, 65, 64)   0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 15, 65, 64)   36928       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 15, 65, 64)   256         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 15, 65, 64)   0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 15, 65, 64)   36928       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 15, 65, 64)   0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 15, 65, 64)   256         add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 15, 65, 64)   0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 15, 65, 64)   36928       activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 15, 65, 64)   256         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 15, 65, 64)   0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 15, 65, 64)   36928       activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 15, 65, 64)   0           add_228[0][0]                    \n",
      "                                                                 conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 15, 65, 64)   256         add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 15, 65, 64)   0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 15, 65, 64)   36928       activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 15, 65, 64)   256         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 15, 65, 64)   0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 15, 65, 64)   36928       activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 15, 65, 64)   0           add_229[0][0]                    \n",
      "                                                                 conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 15, 65, 64)   256         add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 15, 65, 64)   0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 8, 33, 128)   73856       activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 8, 33, 128)   512         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 8, 33, 128)   0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 8, 33, 128)   8320        add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 8, 33, 128)   147584      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 8, 33, 128)   0           conv2d_669[0][0]                 \n",
      "                                                                 conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 8, 33, 128)   512         add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 8, 33, 128)   0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 8, 33, 128)   147584      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 8, 33, 128)   512         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 8, 33, 128)   0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 8, 33, 128)   147584      activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 8, 33, 128)   0           add_231[0][0]                    \n",
      "                                                                 conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 8, 33, 128)   512         add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 8, 33, 128)   0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 8, 33, 128)   147584      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 8, 33, 128)   512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 8, 33, 128)   0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 8, 33, 128)   147584      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 8, 33, 128)   0           add_232[0][0]                    \n",
      "                                                                 conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 8, 33, 128)   512         add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 8, 33, 128)   0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 8, 33, 128)   147584      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 8, 33, 128)   512         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 8, 33, 128)   0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 8, 33, 128)   147584      activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 8, 33, 128)   0           add_233[0][0]                    \n",
      "                                                                 conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 8, 33, 128)   512         add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 8, 33, 128)   0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 4, 17, 256)   295168      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 4, 17, 256)   1024        conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 4, 17, 256)   0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 4, 17, 256)   33024       add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 4, 17, 256)   590080      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 4, 17, 256)   0           conv2d_678[0][0]                 \n",
      "                                                                 conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 4, 17, 256)   1024        add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 4, 17, 256)   0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 4, 17, 256)   590080      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 4, 17, 256)   1024        conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 4, 17, 256)   0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 4, 17, 256)   590080      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 4, 17, 256)   0           add_235[0][0]                    \n",
      "                                                                 conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 4, 17, 256)   1024        add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 4, 17, 256)   0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 4, 17, 256)   590080      activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 4, 17, 256)   1024        conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 4, 17, 256)   0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 4, 17, 256)   590080      activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 4, 17, 256)   0           add_236[0][0]                    \n",
      "                                                                 conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 4, 17, 256)   1024        add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 4, 17, 256)   0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 4, 17, 256)   590080      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 4, 17, 256)   1024        conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 4, 17, 256)   0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 4, 17, 256)   590080      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 4, 17, 256)   0           add_237[0][0]                    \n",
      "                                                                 conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 4, 17, 256)   1024        add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 4, 17, 256)   0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 4, 17, 256)   590080      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 4, 17, 256)   1024        conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 4, 17, 256)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 4, 17, 256)   590080      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 4, 17, 256)   0           add_238[0][0]                    \n",
      "                                                                 conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 4, 17, 256)   1024        add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 4, 17, 256)   0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 4, 17, 256)   590080      activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 4, 17, 256)   1024        conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 4, 17, 256)   0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 4, 17, 256)   590080      activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 4, 17, 256)   0           add_239[0][0]                    \n",
      "                                                                 conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 4, 17, 256)   1024        add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 4, 17, 256)   0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 2, 9, 512)    1180160     activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 2, 9, 512)    2048        conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 2, 9, 512)    0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 2, 9, 512)    131584      add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 2, 9, 512)    2359808     activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_241 (Add)                   (None, 2, 9, 512)    0           conv2d_691[0][0]                 \n",
      "                                                                 conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 2, 9, 512)    2048        add_241[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 2, 9, 512)    0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 2, 9, 512)    2359808     activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 2, 9, 512)    2048        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 2, 9, 512)    0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 2, 9, 512)    2359808     activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_242 (Add)                   (None, 2, 9, 512)    0           add_241[0][0]                    \n",
      "                                                                 conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 2, 9, 512)    2048        add_242[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 2, 9, 512)    0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 2, 9, 512)    2359808     activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 2, 9, 512)    2048        conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 2, 9, 512)    0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 2, 9, 512)    2359808     activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_243 (Add)                   (None, 2, 9, 512)    0           add_242[0][0]                    \n",
      "                                                                 conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 2, 9, 512)    2048        add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 2, 9, 512)    0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 1, 1, 512)    0           activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 512)          0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 43)           22059       flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 43)           172         dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 43)           0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 41)           1804        dropout_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,324,387\n",
      "Trainable params: 21,309,069\n",
      "Non-trainable params: 15,318\n",
      "__________________________________________________________________________________________________\n",
      "using resnet model: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 5.4971 - acc: 0.0469 - val_loss: 6.3937 - val_acc: 0.0620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.06199, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 2/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.9433 - acc: 0.0919 - val_loss: 6.4333 - val_acc: 0.0863\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.06199 to 0.08625, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 3/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 4.6451 - acc: 0.0877 - val_loss: 5.2812 - val_acc: 0.0728\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.08625\n",
      "Epoch 4/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.3810 - acc: 0.1052 - val_loss: 4.5357 - val_acc: 0.0512\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.08625\n",
      "Epoch 5/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.1587 - acc: 0.1262 - val_loss: 4.5848 - val_acc: 0.0836\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.08625\n",
      "Epoch 6/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 4.0086 - acc: 0.1337 - val_loss: 4.0574 - val_acc: 0.1294\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.08625 to 0.12938, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 7/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.8183 - acc: 0.1653 - val_loss: 3.8645 - val_acc: 0.1509\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.12938 to 0.15094, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 8/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.7033 - acc: 0.1845 - val_loss: 3.2781 - val_acc: 0.2156\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.15094 to 0.21563, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 9/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.5858 - acc: 0.2019 - val_loss: 4.2039 - val_acc: 0.0943\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.21563\n",
      "Epoch 10/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.4870 - acc: 0.2203 - val_loss: 3.2442 - val_acc: 0.2291\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.21563 to 0.22911, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 11/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.3652 - acc: 0.2545 - val_loss: 3.9869 - val_acc: 0.0701\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.22911\n",
      "Epoch 12/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 3.3236 - acc: 0.2689 - val_loss: 3.0216 - val_acc: 0.3019\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.22911 to 0.30189, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 13/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.2276 - acc: 0.2936 - val_loss: 3.4522 - val_acc: 0.1698\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.30189\n",
      "Epoch 14/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.1802 - acc: 0.3170 - val_loss: 2.6630 - val_acc: 0.3288\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.30189 to 0.32884, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 15/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0845 - acc: 0.3398 - val_loss: 2.4967 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.32884 to 0.39892, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 16/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0849 - acc: 0.3314 - val_loss: 2.3910 - val_acc: 0.3854\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.39892\n",
      "Epoch 17/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 3.0304 - acc: 0.3516 - val_loss: 2.8012 - val_acc: 0.3342\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.39892\n",
      "Epoch 18/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.9977 - acc: 0.3738 - val_loss: 2.6922 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.39892\n",
      "Epoch 19/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.9675 - acc: 0.3660 - val_loss: 2.3541 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.39892 to 0.40162, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 20/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.9421 - acc: 0.3918 - val_loss: 2.0011 - val_acc: 0.5391\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.40162 to 0.53908, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 21/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8933 - acc: 0.4047 - val_loss: 2.2955 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.53908\n",
      "Epoch 22/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.8419 - acc: 0.4168 - val_loss: 2.5975 - val_acc: 0.3396\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.53908\n",
      "Epoch 23/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8152 - acc: 0.4387 - val_loss: 2.1626 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.53908\n",
      "Epoch 24/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8239 - acc: 0.4390 - val_loss: 1.8890 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.53908 to 0.57412, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 25/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.8086 - acc: 0.4399 - val_loss: 2.3936 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.57412\n",
      "Epoch 26/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7532 - acc: 0.4528 - val_loss: 1.9163 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.57412\n",
      "Epoch 27/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7645 - acc: 0.4528 - val_loss: 2.3758 - val_acc: 0.4016\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.57412\n",
      "Epoch 28/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7280 - acc: 0.4691 - val_loss: 2.4155 - val_acc: 0.3989\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.57412\n",
      "Epoch 29/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.7031 - acc: 0.4694 - val_loss: 2.2590 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.57412\n",
      "Epoch 30/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.6998 - acc: 0.4718 - val_loss: 2.7661 - val_acc: 0.2938\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.57412\n",
      "Epoch 31/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6789 - acc: 0.4847 - val_loss: 1.8095 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.57412\n",
      "Epoch 32/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.6710 - acc: 0.4808 - val_loss: 1.9752 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.57412\n",
      "Epoch 33/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6498 - acc: 0.4826 - val_loss: 1.9676 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.57412\n",
      "Epoch 34/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.6550 - acc: 0.4829 - val_loss: 2.1273 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.57412\n",
      "Epoch 35/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5973 - acc: 0.5192 - val_loss: 1.8021 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.57412 to 0.58760, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 36/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5881 - acc: 0.5072 - val_loss: 2.1362 - val_acc: 0.4933\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.58760\n",
      "Epoch 37/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.5520 - acc: 0.5216 - val_loss: 2.9984 - val_acc: 0.2668\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.58760\n",
      "Epoch 38/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5679 - acc: 0.5174 - val_loss: 2.4282 - val_acc: 0.4097\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.58760\n",
      "Epoch 39/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5258 - acc: 0.5385 - val_loss: 1.9172 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.58760\n",
      "Epoch 40/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4965 - acc: 0.5349 - val_loss: 1.6676 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.58760 to 0.61995, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5282 - acc: 0.5252 - val_loss: 1.8646 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.61995\n",
      "Epoch 42/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4976 - acc: 0.5529 - val_loss: 1.8756 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.61995\n",
      "Epoch 43/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.5427 - acc: 0.5276 - val_loss: 1.7960 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.61995\n",
      "Epoch 44/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4846 - acc: 0.5628 - val_loss: 1.6770 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.61995\n",
      "Epoch 45/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4563 - acc: 0.5628 - val_loss: 1.7897 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.61995\n",
      "Epoch 46/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4605 - acc: 0.5652 - val_loss: 2.0079 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.61995\n",
      "Epoch 47/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4159 - acc: 0.5811 - val_loss: 1.8004 - val_acc: 0.5768\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.61995\n",
      "Epoch 48/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4135 - acc: 0.5724 - val_loss: 1.5408 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.61995 to 0.65768, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 49/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.4051 - acc: 0.5736 - val_loss: 1.7407 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.65768\n",
      "Epoch 50/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4082 - acc: 0.5754 - val_loss: 1.5870 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.65768\n",
      "Epoch 51/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.4177 - acc: 0.5808 - val_loss: 1.7443 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.65768\n",
      "Epoch 52/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3952 - acc: 0.5862 - val_loss: 1.6028 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.65768\n",
      "Epoch 53/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3758 - acc: 0.5947 - val_loss: 1.7821 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.65768\n",
      "Epoch 54/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3624 - acc: 0.5953 - val_loss: 1.5437 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.65768 to 0.68733, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 55/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3263 - acc: 0.6073 - val_loss: 1.4203 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.68733 to 0.71159, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 56/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3303 - acc: 0.5983 - val_loss: 1.7761 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.71159\n",
      "Epoch 57/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3301 - acc: 0.6178 - val_loss: 1.4283 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.71159\n",
      "Epoch 58/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3544 - acc: 0.5880 - val_loss: 1.3528 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.71159 to 0.72507, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 59/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3426 - acc: 0.6016 - val_loss: 1.6911 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.72507\n",
      "Epoch 60/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3492 - acc: 0.6022 - val_loss: 1.3610 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.72507 to 0.72507, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 61/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.3031 - acc: 0.6145 - val_loss: 1.3833 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.72507\n",
      "Epoch 62/3000\n",
      "52/52 [==============================] - 6s 118ms/step - loss: 2.3060 - acc: 0.6148 - val_loss: 1.5282 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.72507\n",
      "Epoch 63/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.3221 - acc: 0.6145 - val_loss: 1.5392 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.72507\n",
      "Epoch 64/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2804 - acc: 0.6247 - val_loss: 1.6057 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.72507\n",
      "Epoch 65/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2476 - acc: 0.6388 - val_loss: 1.3865 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.72507\n",
      "Epoch 66/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2553 - acc: 0.6391 - val_loss: 1.4540 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.72507\n",
      "Epoch 67/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2541 - acc: 0.6226 - val_loss: 1.5567 - val_acc: 0.6496\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.72507\n",
      "Epoch 68/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2504 - acc: 0.6394 - val_loss: 1.3980 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.72507\n",
      "Epoch 69/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2472 - acc: 0.6337 - val_loss: 1.4236 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.72507\n",
      "Epoch 70/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.2393 - acc: 0.6406 - val_loss: 1.2602 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00070: val_acc improved from 0.72507 to 0.74394, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 71/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.2134 - acc: 0.6406 - val_loss: 1.2905 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.74394\n",
      "Epoch 72/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1796 - acc: 0.6644 - val_loss: 1.5347 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.74394\n",
      "Epoch 73/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1944 - acc: 0.6638 - val_loss: 1.3348 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.74394 to 0.74663, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 74/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1501 - acc: 0.6743 - val_loss: 1.4145 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.74663\n",
      "Epoch 75/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1806 - acc: 0.6593 - val_loss: 1.2945 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.74663\n",
      "Epoch 76/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1759 - acc: 0.6665 - val_loss: 1.3535 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.74663\n",
      "Epoch 77/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1777 - acc: 0.6629 - val_loss: 1.2361 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.74663\n",
      "Epoch 78/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1745 - acc: 0.6677 - val_loss: 1.6148 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.74663\n",
      "Epoch 79/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1803 - acc: 0.6562 - val_loss: 1.4534 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.74663\n",
      "Epoch 80/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1542 - acc: 0.6740 - val_loss: 1.3563 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.74663\n",
      "Epoch 81/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1299 - acc: 0.6866 - val_loss: 1.5159 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.74663\n",
      "Epoch 82/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1747 - acc: 0.6599 - val_loss: 1.2298 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.74663 to 0.75741, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1455 - acc: 0.6809 - val_loss: 1.1509 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.75741 to 0.76819, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 84/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1309 - acc: 0.6779 - val_loss: 1.3462 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76819\n",
      "Epoch 85/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1150 - acc: 0.6794 - val_loss: 1.2776 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76819\n",
      "Epoch 86/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1030 - acc: 0.6878 - val_loss: 1.1395 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76819\n",
      "Epoch 87/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.1633 - acc: 0.6653 - val_loss: 1.6134 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76819\n",
      "Epoch 88/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0858 - acc: 0.6947 - val_loss: 1.5085 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76819\n",
      "Epoch 89/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0880 - acc: 0.6863 - val_loss: 1.2815 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76819\n",
      "Epoch 90/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.1034 - acc: 0.6812 - val_loss: 1.5284 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76819\n",
      "Epoch 91/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0823 - acc: 0.6944 - val_loss: 1.5216 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76819\n",
      "Epoch 92/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0439 - acc: 0.7055 - val_loss: 1.3324 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76819\n",
      "Epoch 93/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0783 - acc: 0.6944 - val_loss: 1.6448 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76819\n",
      "Epoch 94/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0706 - acc: 0.6983 - val_loss: 1.4254 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76819\n",
      "Epoch 95/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0583 - acc: 0.6947 - val_loss: 1.1851 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.76819 to 0.77628, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 96/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0380 - acc: 0.7049 - val_loss: 1.0123 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00096: val_acc improved from 0.77628 to 0.82749, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 97/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0595 - acc: 0.7031 - val_loss: 1.5489 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.82749\n",
      "Epoch 98/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0538 - acc: 0.6986 - val_loss: 1.1504 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.82749\n",
      "Epoch 99/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0526 - acc: 0.6992 - val_loss: 1.2546 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.82749\n",
      "Epoch 100/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0291 - acc: 0.7031 - val_loss: 1.2147 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.82749\n",
      "Epoch 101/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0251 - acc: 0.7010 - val_loss: 1.3633 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.82749\n",
      "Epoch 102/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0179 - acc: 0.7112 - val_loss: 1.4068 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.82749\n",
      "Epoch 103/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0151 - acc: 0.7166 - val_loss: 1.1251 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82749\n",
      "Epoch 104/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0144 - acc: 0.7254 - val_loss: 1.2227 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82749\n",
      "Epoch 105/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 2.0109 - acc: 0.7239 - val_loss: 1.1006 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82749\n",
      "Epoch 106/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0052 - acc: 0.7178 - val_loss: 1.1956 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.82749\n",
      "Epoch 107/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9981 - acc: 0.7191 - val_loss: 1.2807 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.82749\n",
      "Epoch 108/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9916 - acc: 0.7308 - val_loss: 1.1782 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.82749\n",
      "Epoch 109/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9787 - acc: 0.7266 - val_loss: 1.3225 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.82749\n",
      "Epoch 110/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 2.0063 - acc: 0.7215 - val_loss: 1.1580 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.82749\n",
      "Epoch 111/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9825 - acc: 0.7239 - val_loss: 1.1792 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.82749\n",
      "Epoch 112/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9496 - acc: 0.7434 - val_loss: 1.3325 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.82749\n",
      "Epoch 113/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9671 - acc: 0.7257 - val_loss: 1.2432 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.82749\n",
      "Epoch 114/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9487 - acc: 0.7242 - val_loss: 1.1865 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.82749\n",
      "Epoch 115/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9505 - acc: 0.7371 - val_loss: 1.2766 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.82749\n",
      "Epoch 116/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9659 - acc: 0.7269 - val_loss: 1.2464 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.82749\n",
      "Epoch 117/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9329 - acc: 0.7437 - val_loss: 1.2101 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.82749\n",
      "Epoch 118/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9470 - acc: 0.7329 - val_loss: 1.1324 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.82749\n",
      "Epoch 119/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9313 - acc: 0.7476 - val_loss: 1.1696 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.82749\n",
      "Epoch 120/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9505 - acc: 0.7347 - val_loss: 1.1710 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.82749\n",
      "Epoch 121/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9341 - acc: 0.7461 - val_loss: 1.2192 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.82749\n",
      "Epoch 122/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9325 - acc: 0.7320 - val_loss: 1.0979 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.82749\n",
      "Epoch 123/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9155 - acc: 0.7482 - val_loss: 1.1108 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.82749\n",
      "Epoch 124/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9223 - acc: 0.7422 - val_loss: 1.0891 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.82749\n",
      "Epoch 125/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9168 - acc: 0.7440 - val_loss: 1.1973 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.82749\n",
      "Epoch 126/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9340 - acc: 0.7434 - val_loss: 1.1436 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.82749\n",
      "Epoch 127/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9201 - acc: 0.7515 - val_loss: 1.1462 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.82749\n",
      "Epoch 128/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9171 - acc: 0.7437 - val_loss: 1.1516 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.82749\n",
      "Epoch 129/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.9112 - acc: 0.7458 - val_loss: 1.1277 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.82749\n",
      "Epoch 130/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8748 - acc: 0.7563 - val_loss: 1.2279 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.82749\n",
      "Epoch 131/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8846 - acc: 0.7590 - val_loss: 1.0994 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.82749\n",
      "Epoch 132/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9077 - acc: 0.7500 - val_loss: 1.1533 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.82749\n",
      "Epoch 133/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.9212 - acc: 0.7371 - val_loss: 1.1475 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.82749\n",
      "Epoch 134/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8819 - acc: 0.7638 - val_loss: 1.1533 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.82749\n",
      "Epoch 135/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8879 - acc: 0.7497 - val_loss: 1.1151 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.82749\n",
      "Epoch 136/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8673 - acc: 0.7650 - val_loss: 1.1858 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.82749\n",
      "Epoch 137/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8781 - acc: 0.7578 - val_loss: 1.1630 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.82749\n",
      "Epoch 138/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8798 - acc: 0.7596 - val_loss: 1.1112 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.82749\n",
      "Epoch 139/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8825 - acc: 0.7518 - val_loss: 1.1195 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.82749\n",
      "Epoch 140/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8635 - acc: 0.7572 - val_loss: 1.2057 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.82749\n",
      "Epoch 141/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8633 - acc: 0.7650 - val_loss: 1.2079 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.82749\n",
      "Epoch 142/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8578 - acc: 0.7629 - val_loss: 1.2061 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.82749\n",
      "Epoch 143/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8491 - acc: 0.7644 - val_loss: 1.1376 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.82749\n",
      "Epoch 144/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8464 - acc: 0.7701 - val_loss: 1.0858 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.82749\n",
      "Epoch 145/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8581 - acc: 0.7587 - val_loss: 1.2252 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.82749\n",
      "Epoch 146/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8584 - acc: 0.7590 - val_loss: 1.3471 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.82749\n",
      "Epoch 147/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8633 - acc: 0.7659 - val_loss: 1.1197 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.82749\n",
      "Epoch 148/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8244 - acc: 0.7770 - val_loss: 1.2773 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.82749\n",
      "Epoch 149/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8566 - acc: 0.7668 - val_loss: 1.0103 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.82749\n",
      "Epoch 150/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8436 - acc: 0.7671 - val_loss: 1.1007 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.82749\n",
      "Epoch 151/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8494 - acc: 0.7713 - val_loss: 1.0075 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.82749 to 0.83558, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 152/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8396 - acc: 0.7701 - val_loss: 1.1855 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83558\n",
      "Epoch 153/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8257 - acc: 0.7788 - val_loss: 1.0930 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83558\n",
      "Epoch 154/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.8196 - acc: 0.7740 - val_loss: 1.2022 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83558\n",
      "Epoch 155/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7977 - acc: 0.7873 - val_loss: 1.0916 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83558\n",
      "Epoch 156/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8009 - acc: 0.7734 - val_loss: 1.1262 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.83558\n",
      "Epoch 157/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8105 - acc: 0.7794 - val_loss: 1.2564 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83558\n",
      "Epoch 158/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8313 - acc: 0.7740 - val_loss: 1.2038 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83558\n",
      "Epoch 159/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8178 - acc: 0.7767 - val_loss: 1.1717 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.83558\n",
      "Epoch 160/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8168 - acc: 0.7791 - val_loss: 1.0843 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83558\n",
      "Epoch 161/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8296 - acc: 0.7689 - val_loss: 1.3784 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83558\n",
      "Epoch 162/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8296 - acc: 0.7764 - val_loss: 1.1571 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83558\n",
      "Epoch 163/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7734 - acc: 0.7930 - val_loss: 1.0661 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83558\n",
      "Epoch 164/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8045 - acc: 0.7822 - val_loss: 1.0817 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83558\n",
      "Epoch 165/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7831 - acc: 0.7816 - val_loss: 1.1346 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83558\n",
      "Epoch 166/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7672 - acc: 0.8008 - val_loss: 1.1085 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.83558\n",
      "Epoch 167/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7889 - acc: 0.7773 - val_loss: 1.1265 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.83558\n",
      "Epoch 168/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8070 - acc: 0.7734 - val_loss: 1.0613 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.83558\n",
      "Epoch 169/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7776 - acc: 0.7885 - val_loss: 1.0171 - val_acc: 0.8356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00169: val_acc did not improve from 0.83558\n",
      "Epoch 170/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.8221 - acc: 0.7683 - val_loss: 1.3640 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.83558\n",
      "Epoch 171/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7859 - acc: 0.7855 - val_loss: 1.1673 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.83558\n",
      "Epoch 172/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7814 - acc: 0.7861 - val_loss: 1.0640 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.83558\n",
      "Epoch 173/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7627 - acc: 0.7945 - val_loss: 1.0827 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.83558\n",
      "Epoch 174/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7777 - acc: 0.7894 - val_loss: 1.0214 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.83558\n",
      "Epoch 175/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7862 - acc: 0.7737 - val_loss: 1.0835 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.83558\n",
      "Epoch 176/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7821 - acc: 0.7957 - val_loss: 1.2108 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.83558\n",
      "Epoch 177/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7814 - acc: 0.7837 - val_loss: 1.0378 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.83558\n",
      "Epoch 178/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7463 - acc: 0.7933 - val_loss: 1.1780 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.83558\n",
      "Epoch 179/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7956 - acc: 0.7740 - val_loss: 1.3694 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.83558\n",
      "Epoch 180/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7832 - acc: 0.7855 - val_loss: 1.2417 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.83558\n",
      "Epoch 181/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7732 - acc: 0.7788 - val_loss: 1.1892 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.83558\n",
      "Epoch 182/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7645 - acc: 0.7966 - val_loss: 1.3980 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.83558\n",
      "Epoch 183/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7594 - acc: 0.7966 - val_loss: 1.0951 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.83558\n",
      "Epoch 184/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7358 - acc: 0.7993 - val_loss: 1.1611 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.83558\n",
      "Epoch 185/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7434 - acc: 0.7900 - val_loss: 1.0008 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.83558\n",
      "Epoch 186/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7280 - acc: 0.8023 - val_loss: 1.0947 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.83558\n",
      "Epoch 187/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7381 - acc: 0.7930 - val_loss: 1.0297 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.83558\n",
      "Epoch 188/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7640 - acc: 0.7858 - val_loss: 1.1115 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.83558\n",
      "Epoch 189/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7491 - acc: 0.7900 - val_loss: 1.0493 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.83558\n",
      "Epoch 190/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7657 - acc: 0.7840 - val_loss: 1.0597 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.83558\n",
      "Epoch 191/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7654 - acc: 0.7915 - val_loss: 1.1038 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.83558\n",
      "Epoch 192/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7447 - acc: 0.7867 - val_loss: 1.2374 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.83558\n",
      "Epoch 193/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7473 - acc: 0.7900 - val_loss: 1.1973 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.83558\n",
      "Epoch 194/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7437 - acc: 0.7924 - val_loss: 1.1804 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.83558\n",
      "Epoch 195/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7532 - acc: 0.7927 - val_loss: 1.3237 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.83558\n",
      "Epoch 196/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7342 - acc: 0.7960 - val_loss: 1.0750 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.83558\n",
      "Epoch 197/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7323 - acc: 0.8035 - val_loss: 1.1456 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.83558\n",
      "Epoch 198/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7435 - acc: 0.7960 - val_loss: 1.1011 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.83558\n",
      "Epoch 199/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7688 - acc: 0.7951 - val_loss: 1.0672 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.83558\n",
      "Epoch 200/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7558 - acc: 0.7957 - val_loss: 1.3339 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.83558\n",
      "Epoch 201/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7349 - acc: 0.7951 - val_loss: 1.0644 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.83558\n",
      "Epoch 202/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7144 - acc: 0.8092 - val_loss: 1.2054 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.83558\n",
      "Epoch 203/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7128 - acc: 0.8005 - val_loss: 1.0973 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.83558\n",
      "Epoch 204/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7130 - acc: 0.8026 - val_loss: 1.0603 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.83558\n",
      "Epoch 205/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7375 - acc: 0.8068 - val_loss: 1.1104 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.83558\n",
      "Epoch 206/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7256 - acc: 0.7930 - val_loss: 1.2445 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.83558\n",
      "Epoch 207/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7335 - acc: 0.7942 - val_loss: 1.2019 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.83558\n",
      "Epoch 208/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7223 - acc: 0.7948 - val_loss: 1.0854 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.83558\n",
      "Epoch 209/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7022 - acc: 0.8089 - val_loss: 1.1002 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.83558\n",
      "Epoch 210/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7138 - acc: 0.7984 - val_loss: 1.2387 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.83558\n",
      "Epoch 211/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7080 - acc: 0.7999 - val_loss: 1.1467 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.83558\n",
      "Epoch 212/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7346 - acc: 0.7951 - val_loss: 1.2185 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.83558\n",
      "Epoch 213/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7249 - acc: 0.8062 - val_loss: 1.0127 - val_acc: 0.8086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00213: val_acc did not improve from 0.83558\n",
      "Epoch 214/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7023 - acc: 0.8062 - val_loss: 2.4547 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.83558\n",
      "Epoch 215/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6786 - acc: 0.8140 - val_loss: 1.1134 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.83558\n",
      "Epoch 216/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6886 - acc: 0.8179 - val_loss: 1.2818 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.83558\n",
      "Epoch 217/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7292 - acc: 0.7939 - val_loss: 1.0521 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.83558\n",
      "Epoch 218/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7278 - acc: 0.8050 - val_loss: 1.1343 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.83558\n",
      "Epoch 219/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7200 - acc: 0.7996 - val_loss: 1.1406 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.83558\n",
      "Epoch 220/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6923 - acc: 0.7957 - val_loss: 1.0610 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.83558\n",
      "Epoch 221/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7057 - acc: 0.8080 - val_loss: 1.1600 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.83558\n",
      "Epoch 222/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.7097 - acc: 0.7939 - val_loss: 1.2054 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.83558\n",
      "Epoch 223/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7013 - acc: 0.8110 - val_loss: 1.1078 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.83558\n",
      "Epoch 224/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6832 - acc: 0.8008 - val_loss: 1.1256 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.83558\n",
      "Epoch 225/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6650 - acc: 0.8116 - val_loss: 1.0189 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00225: val_acc improved from 0.83558 to 0.84097, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 226/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6796 - acc: 0.8062 - val_loss: 1.0791 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.84097\n",
      "Epoch 227/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7024 - acc: 0.8098 - val_loss: 1.2581 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.84097\n",
      "Epoch 228/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.7309 - acc: 0.7852 - val_loss: 1.1584 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.84097\n",
      "Epoch 229/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6792 - acc: 0.8071 - val_loss: 1.0977 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.84097\n",
      "Epoch 230/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6811 - acc: 0.8050 - val_loss: 1.0784 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.84097\n",
      "Epoch 231/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6744 - acc: 0.8095 - val_loss: 1.4286 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.84097\n",
      "Epoch 232/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6905 - acc: 0.7975 - val_loss: 1.0885 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.84097\n",
      "Epoch 233/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6854 - acc: 0.8056 - val_loss: 1.2185 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.84097\n",
      "Epoch 234/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6951 - acc: 0.8059 - val_loss: 1.0752 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.84097\n",
      "Epoch 235/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6377 - acc: 0.8281 - val_loss: 1.0598 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.84097\n",
      "Epoch 236/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6853 - acc: 0.8044 - val_loss: 1.3036 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.84097\n",
      "Epoch 237/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6462 - acc: 0.8083 - val_loss: 1.0123 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.84097\n",
      "Epoch 238/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6678 - acc: 0.8227 - val_loss: 1.0957 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.84097\n",
      "Epoch 239/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6386 - acc: 0.8185 - val_loss: 0.9974 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.84097\n",
      "Epoch 240/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6557 - acc: 0.8212 - val_loss: 1.0299 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.84097\n",
      "Epoch 241/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6687 - acc: 0.8086 - val_loss: 1.0773 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.84097\n",
      "Epoch 242/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6780 - acc: 0.8053 - val_loss: 1.2106 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.84097\n",
      "Epoch 243/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6756 - acc: 0.8092 - val_loss: 0.9951 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.84097\n",
      "Epoch 244/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6771 - acc: 0.8071 - val_loss: 1.0531 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.84097\n",
      "Epoch 245/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6698 - acc: 0.8062 - val_loss: 1.2682 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.84097\n",
      "Epoch 246/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6594 - acc: 0.8197 - val_loss: 1.0899 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.84097\n",
      "Epoch 247/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6375 - acc: 0.8146 - val_loss: 1.0863 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.84097\n",
      "Epoch 248/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6561 - acc: 0.8191 - val_loss: 1.3226 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.84097\n",
      "Epoch 249/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6864 - acc: 0.8044 - val_loss: 1.1819 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.84097\n",
      "Epoch 250/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6713 - acc: 0.8098 - val_loss: 1.1937 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.84097\n",
      "Epoch 251/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6535 - acc: 0.8098 - val_loss: 1.1912 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.84097\n",
      "Epoch 252/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6489 - acc: 0.8125 - val_loss: 1.0692 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.84097\n",
      "Epoch 253/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6860 - acc: 0.8158 - val_loss: 1.1639 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.84097\n",
      "Epoch 254/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6789 - acc: 0.8134 - val_loss: 1.1040 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.84097\n",
      "Epoch 255/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6402 - acc: 0.8167 - val_loss: 1.1204 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.84097\n",
      "Epoch 256/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6585 - acc: 0.8137 - val_loss: 1.1037 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.84097\n",
      "Epoch 257/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6196 - acc: 0.8197 - val_loss: 1.3734 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.84097\n",
      "Epoch 258/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6451 - acc: 0.8104 - val_loss: 1.1974 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.84097\n",
      "Epoch 259/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6477 - acc: 0.8239 - val_loss: 1.1408 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.84097\n",
      "Epoch 260/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6743 - acc: 0.8026 - val_loss: 1.0782 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.84097\n",
      "Epoch 261/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6434 - acc: 0.8176 - val_loss: 1.0185 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.84097\n",
      "Epoch 262/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6577 - acc: 0.8194 - val_loss: 1.0547 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.84097\n",
      "Epoch 263/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6427 - acc: 0.8101 - val_loss: 1.2481 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.84097\n",
      "Epoch 264/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6584 - acc: 0.8074 - val_loss: 1.0562 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.84097\n",
      "Epoch 265/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6498 - acc: 0.8134 - val_loss: 0.9890 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.84097\n",
      "Epoch 266/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6427 - acc: 0.8215 - val_loss: 1.2137 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.84097\n",
      "Epoch 267/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6447 - acc: 0.8203 - val_loss: 1.2881 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.84097\n",
      "Epoch 268/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6466 - acc: 0.8251 - val_loss: 1.1002 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.84097\n",
      "Epoch 269/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6626 - acc: 0.8101 - val_loss: 1.1099 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.84097\n",
      "Epoch 270/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6434 - acc: 0.8170 - val_loss: 1.4225 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.84097\n",
      "Epoch 271/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6273 - acc: 0.8197 - val_loss: 1.2017 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.84097\n",
      "Epoch 272/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6685 - acc: 0.8041 - val_loss: 1.1503 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.84097\n",
      "Epoch 273/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6151 - acc: 0.8257 - val_loss: 1.1298 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.84097\n",
      "Epoch 274/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6455 - acc: 0.8191 - val_loss: 1.1493 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.84097\n",
      "Epoch 275/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6268 - acc: 0.8134 - val_loss: 1.1245 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.84097\n",
      "Epoch 276/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6590 - acc: 0.8065 - val_loss: 1.1935 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.84097\n",
      "Epoch 277/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6164 - acc: 0.8254 - val_loss: 0.9927 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.84097\n",
      "Epoch 278/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6277 - acc: 0.8212 - val_loss: 1.0826 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.84097\n",
      "Epoch 279/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6367 - acc: 0.8101 - val_loss: 1.2276 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.84097\n",
      "Epoch 280/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6363 - acc: 0.8134 - val_loss: 1.2375 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.84097\n",
      "Epoch 281/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6329 - acc: 0.8176 - val_loss: 1.0839 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.84097\n",
      "Epoch 282/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6369 - acc: 0.8188 - val_loss: 1.0497 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.84097\n",
      "Epoch 283/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6217 - acc: 0.8176 - val_loss: 1.1372 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.84097\n",
      "Epoch 284/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6449 - acc: 0.8140 - val_loss: 1.1776 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.84097\n",
      "Epoch 285/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6572 - acc: 0.8092 - val_loss: 1.3931 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.84097\n",
      "Epoch 286/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6228 - acc: 0.8281 - val_loss: 1.1438 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.84097\n",
      "Epoch 287/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6476 - acc: 0.8080 - val_loss: 1.0357 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.84097\n",
      "Epoch 288/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6394 - acc: 0.8065 - val_loss: 1.0924 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.84097\n",
      "Epoch 289/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6639 - acc: 0.8011 - val_loss: 1.0355 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.84097\n",
      "Epoch 290/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6198 - acc: 0.8146 - val_loss: 1.1990 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.84097\n",
      "Epoch 291/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6199 - acc: 0.8176 - val_loss: 1.1462 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.84097\n",
      "Epoch 292/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6285 - acc: 0.8068 - val_loss: 1.1114 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.84097\n",
      "Epoch 293/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6202 - acc: 0.8143 - val_loss: 1.2725 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.84097\n",
      "Epoch 294/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6371 - acc: 0.8167 - val_loss: 1.1634 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.84097\n",
      "Epoch 295/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.6212 - acc: 0.8251 - val_loss: 1.1453 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.84097\n",
      "Epoch 296/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6243 - acc: 0.8260 - val_loss: 1.1441 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.84097\n",
      "Epoch 297/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6253 - acc: 0.8212 - val_loss: 1.1353 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.84097\n",
      "Epoch 298/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6333 - acc: 0.8125 - val_loss: 1.1323 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.84097\n",
      "Epoch 299/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6193 - acc: 0.8236 - val_loss: 1.1447 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.84097\n",
      "Epoch 300/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6047 - acc: 0.8257 - val_loss: 1.1909 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.84097\n",
      "Epoch 301/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6258 - acc: 0.8137 - val_loss: 1.1918 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.84097\n",
      "Epoch 302/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6409 - acc: 0.8122 - val_loss: 1.0719 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.84097\n",
      "Epoch 303/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6165 - acc: 0.8251 - val_loss: 1.1831 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.84097\n",
      "Epoch 304/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6114 - acc: 0.8182 - val_loss: 1.0449 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.84097\n",
      "Epoch 305/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6182 - acc: 0.8149 - val_loss: 1.0721 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.84097\n",
      "Epoch 306/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6077 - acc: 0.8233 - val_loss: 1.2765 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.84097\n",
      "Epoch 307/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6108 - acc: 0.8239 - val_loss: 1.0095 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.84097\n",
      "Epoch 308/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5935 - acc: 0.8314 - val_loss: 0.9898 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.84097\n",
      "Epoch 309/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6083 - acc: 0.8212 - val_loss: 0.9899 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.84097\n",
      "Epoch 310/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6201 - acc: 0.8266 - val_loss: 1.0725 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.84097\n",
      "Epoch 311/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5915 - acc: 0.8311 - val_loss: 1.0464 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.84097\n",
      "Epoch 312/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5968 - acc: 0.8272 - val_loss: 1.0435 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.84097\n",
      "Epoch 313/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6256 - acc: 0.8116 - val_loss: 1.0590 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.84097\n",
      "Epoch 314/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6066 - acc: 0.8212 - val_loss: 1.2239 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.84097\n",
      "Epoch 315/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6205 - acc: 0.8170 - val_loss: 1.1546 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.84097\n",
      "Epoch 316/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6220 - acc: 0.8137 - val_loss: 1.1748 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.84097\n",
      "Epoch 317/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6208 - acc: 0.8152 - val_loss: 1.0870 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.84097\n",
      "Epoch 318/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5932 - acc: 0.8140 - val_loss: 1.0122 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00318: val_acc improved from 0.84097 to 0.85445, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 319/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6149 - acc: 0.8167 - val_loss: 1.0794 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.85445\n",
      "Epoch 320/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5963 - acc: 0.8257 - val_loss: 1.0935 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.85445\n",
      "Epoch 321/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5929 - acc: 0.8191 - val_loss: 1.1638 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.85445\n",
      "Epoch 322/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5796 - acc: 0.8269 - val_loss: 1.2065 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.85445\n",
      "Epoch 323/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5854 - acc: 0.8230 - val_loss: 1.1454 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.85445\n",
      "Epoch 324/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6142 - acc: 0.7990 - val_loss: 1.1810 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.85445\n",
      "Epoch 325/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6114 - acc: 0.8161 - val_loss: 1.0999 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.85445\n",
      "Epoch 326/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6032 - acc: 0.8182 - val_loss: 1.0357 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.85445\n",
      "Epoch 327/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5903 - acc: 0.8179 - val_loss: 1.1549 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.85445\n",
      "Epoch 328/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6087 - acc: 0.8149 - val_loss: 0.9813 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.85445\n",
      "Epoch 329/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6033 - acc: 0.8224 - val_loss: 1.1901 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.85445\n",
      "Epoch 330/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6108 - acc: 0.8197 - val_loss: 1.2585 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.85445\n",
      "Epoch 331/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5912 - acc: 0.8206 - val_loss: 1.0759 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.85445\n",
      "Epoch 332/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5897 - acc: 0.8293 - val_loss: 1.0809 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.85445\n",
      "Epoch 333/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5936 - acc: 0.8173 - val_loss: 1.1534 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.85445\n",
      "Epoch 334/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6057 - acc: 0.8140 - val_loss: 1.0606 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.85445\n",
      "Epoch 335/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5846 - acc: 0.8317 - val_loss: 1.0405 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.85445\n",
      "Epoch 336/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5895 - acc: 0.8254 - val_loss: 1.1066 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.85445\n",
      "Epoch 337/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5961 - acc: 0.8257 - val_loss: 0.9994 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.85445\n",
      "Epoch 338/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5858 - acc: 0.8191 - val_loss: 1.1438 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.85445\n",
      "Epoch 339/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6059 - acc: 0.8212 - val_loss: 1.1891 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.85445\n",
      "Epoch 340/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5975 - acc: 0.8224 - val_loss: 1.0360 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.85445\n",
      "Epoch 341/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5947 - acc: 0.8107 - val_loss: 1.1687 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.85445\n",
      "Epoch 342/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6003 - acc: 0.8263 - val_loss: 1.0326 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.85445\n",
      "Epoch 343/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6054 - acc: 0.8131 - val_loss: 1.1275 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.85445\n",
      "Epoch 344/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5950 - acc: 0.8290 - val_loss: 0.9868 - val_acc: 0.8383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00344: val_acc did not improve from 0.85445\n",
      "Epoch 345/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5943 - acc: 0.8356 - val_loss: 1.0432 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.85445\n",
      "Epoch 346/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5767 - acc: 0.8374 - val_loss: 1.0208 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.85445\n",
      "Epoch 347/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.5753 - acc: 0.8248 - val_loss: 1.0877 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.85445\n",
      "Epoch 348/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5920 - acc: 0.8269 - val_loss: 1.1040 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.85445\n",
      "Epoch 349/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5800 - acc: 0.8290 - val_loss: 1.1097 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.85445\n",
      "Epoch 350/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5728 - acc: 0.8227 - val_loss: 1.0315 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.85445\n",
      "Epoch 351/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.6001 - acc: 0.8062 - val_loss: 1.0286 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.85445\n",
      "Epoch 352/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5881 - acc: 0.8278 - val_loss: 1.0846 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.85445\n",
      "Epoch 353/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5846 - acc: 0.8302 - val_loss: 1.3110 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.85445\n",
      "Epoch 354/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5864 - acc: 0.8137 - val_loss: 1.0697 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.85445\n",
      "Epoch 355/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5861 - acc: 0.8218 - val_loss: 1.0774 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.85445\n",
      "Epoch 356/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5821 - acc: 0.8269 - val_loss: 1.1799 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.85445\n",
      "Epoch 357/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5837 - acc: 0.8170 - val_loss: 1.1011 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.85445\n",
      "Epoch 358/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5775 - acc: 0.8263 - val_loss: 1.0122 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.85445\n",
      "Epoch 359/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5929 - acc: 0.8356 - val_loss: 1.2284 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.85445\n",
      "Epoch 360/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.6016 - acc: 0.8143 - val_loss: 1.0544 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.85445\n",
      "Epoch 361/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5780 - acc: 0.8275 - val_loss: 1.2198 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.85445\n",
      "Epoch 362/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5814 - acc: 0.8242 - val_loss: 1.0846 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.85445\n",
      "Epoch 363/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5911 - acc: 0.8236 - val_loss: 1.0406 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.85445\n",
      "Epoch 364/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5604 - acc: 0.8266 - val_loss: 1.2105 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.85445\n",
      "Epoch 365/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5938 - acc: 0.8173 - val_loss: 1.0394 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.85445\n",
      "Epoch 366/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5546 - acc: 0.8302 - val_loss: 1.2183 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.85445\n",
      "Epoch 367/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5703 - acc: 0.8329 - val_loss: 1.0978 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.85445\n",
      "Epoch 368/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5700 - acc: 0.8305 - val_loss: 1.1278 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.85445\n",
      "Epoch 369/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5737 - acc: 0.8278 - val_loss: 0.9967 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.85445\n",
      "Epoch 370/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5634 - acc: 0.8224 - val_loss: 0.9791 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.85445\n",
      "Epoch 371/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5729 - acc: 0.8245 - val_loss: 1.0451 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.85445\n",
      "Epoch 372/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5683 - acc: 0.8203 - val_loss: 1.1760 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.85445\n",
      "Epoch 373/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5925 - acc: 0.8272 - val_loss: 1.2138 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.85445\n",
      "Epoch 374/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5722 - acc: 0.8194 - val_loss: 1.0253 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.85445\n",
      "Epoch 375/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5758 - acc: 0.8179 - val_loss: 1.1015 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.85445\n",
      "Epoch 376/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5956 - acc: 0.8104 - val_loss: 1.1225 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.85445\n",
      "Epoch 377/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5730 - acc: 0.8308 - val_loss: 1.1148 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.85445\n",
      "Epoch 378/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5505 - acc: 0.8278 - val_loss: 1.0749 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.85445\n",
      "Epoch 379/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5862 - acc: 0.8182 - val_loss: 1.0942 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.85445\n",
      "Epoch 380/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5567 - acc: 0.8299 - val_loss: 1.1208 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.85445\n",
      "Epoch 381/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5741 - acc: 0.8143 - val_loss: 1.1334 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.85445\n",
      "Epoch 382/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5892 - acc: 0.8236 - val_loss: 0.9891 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.85445\n",
      "Epoch 383/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5574 - acc: 0.8287 - val_loss: 1.0910 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.85445\n",
      "Epoch 384/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5513 - acc: 0.8320 - val_loss: 1.0386 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.85445\n",
      "Epoch 385/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5659 - acc: 0.8146 - val_loss: 1.0985 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.85445\n",
      "Epoch 386/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5692 - acc: 0.8269 - val_loss: 0.9869 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.85445\n",
      "Epoch 387/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5585 - acc: 0.8275 - val_loss: 1.1452 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.85445\n",
      "Epoch 388/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5663 - acc: 0.8266 - val_loss: 1.1303 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00388: val_acc did not improve from 0.85445\n",
      "Epoch 389/3000\n",
      "52/52 [==============================] - 6s 115ms/step - loss: 1.5774 - acc: 0.8272 - val_loss: 1.2563 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.85445\n",
      "Epoch 390/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5746 - acc: 0.8263 - val_loss: 1.1668 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.85445\n",
      "Epoch 391/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5764 - acc: 0.8275 - val_loss: 1.0111 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.85445\n",
      "Epoch 392/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5751 - acc: 0.8155 - val_loss: 1.0451 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.85445\n",
      "Epoch 393/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5366 - acc: 0.8404 - val_loss: 1.1378 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.85445\n",
      "Epoch 394/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5729 - acc: 0.8260 - val_loss: 1.0081 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.85445\n",
      "Epoch 395/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5697 - acc: 0.8314 - val_loss: 1.0236 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.85445\n",
      "Epoch 396/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5555 - acc: 0.8278 - val_loss: 1.1440 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.85445\n",
      "Epoch 397/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5616 - acc: 0.8272 - val_loss: 1.0165 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.85445\n",
      "Epoch 398/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5563 - acc: 0.8245 - val_loss: 1.1619 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.85445\n",
      "Epoch 399/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5450 - acc: 0.8326 - val_loss: 1.1788 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.85445\n",
      "Epoch 400/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5648 - acc: 0.8257 - val_loss: 1.1381 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.85445\n",
      "Epoch 401/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5797 - acc: 0.8218 - val_loss: 1.1222 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.85445\n",
      "Epoch 402/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5583 - acc: 0.8257 - val_loss: 1.0346 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.85445\n",
      "Epoch 403/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5572 - acc: 0.8233 - val_loss: 1.1383 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.85445\n",
      "Epoch 404/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5767 - acc: 0.8269 - val_loss: 0.9624 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.85445\n",
      "Epoch 405/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5457 - acc: 0.8188 - val_loss: 1.0143 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.85445\n",
      "Epoch 406/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5586 - acc: 0.8287 - val_loss: 1.0701 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.85445\n",
      "Epoch 407/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5747 - acc: 0.8164 - val_loss: 1.1632 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.85445\n",
      "Epoch 408/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5418 - acc: 0.8425 - val_loss: 1.0405 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.85445\n",
      "Epoch 409/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5492 - acc: 0.8299 - val_loss: 1.1745 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.85445\n",
      "Epoch 410/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5326 - acc: 0.8383 - val_loss: 1.2188 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.85445\n",
      "Epoch 411/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5388 - acc: 0.8377 - val_loss: 1.1287 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.85445\n",
      "Epoch 412/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5549 - acc: 0.8125 - val_loss: 1.1369 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.85445\n",
      "Epoch 413/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5315 - acc: 0.8338 - val_loss: 0.9860 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.85445\n",
      "Epoch 414/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5465 - acc: 0.8260 - val_loss: 1.3130 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.85445\n",
      "Epoch 415/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5458 - acc: 0.8230 - val_loss: 1.1512 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.85445\n",
      "Epoch 416/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5483 - acc: 0.8269 - val_loss: 1.0411 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.85445\n",
      "Epoch 417/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5504 - acc: 0.8146 - val_loss: 1.0739 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.85445\n",
      "Epoch 418/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5500 - acc: 0.8299 - val_loss: 0.9059 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00418: val_acc improved from 0.85445 to 0.85445, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 419/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5692 - acc: 0.8272 - val_loss: 1.0512 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.85445\n",
      "Epoch 420/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5362 - acc: 0.8266 - val_loss: 1.0645 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.85445\n",
      "Epoch 421/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5662 - acc: 0.8191 - val_loss: 1.1039 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.85445\n",
      "Epoch 422/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5371 - acc: 0.8347 - val_loss: 1.0976 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.85445\n",
      "Epoch 423/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5519 - acc: 0.8260 - val_loss: 1.0226 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.85445\n",
      "Epoch 424/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5855 - acc: 0.8176 - val_loss: 1.1140 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.85445\n",
      "Epoch 425/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5520 - acc: 0.8314 - val_loss: 1.1679 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.85445\n",
      "Epoch 426/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5699 - acc: 0.8191 - val_loss: 1.0438 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.85445\n",
      "Epoch 427/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5623 - acc: 0.8296 - val_loss: 1.1030 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.85445\n",
      "Epoch 428/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5528 - acc: 0.8233 - val_loss: 1.0949 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.85445\n",
      "Epoch 429/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5627 - acc: 0.8230 - val_loss: 1.0016 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.85445\n",
      "Epoch 430/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5502 - acc: 0.8266 - val_loss: 1.0966 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.85445\n",
      "Epoch 431/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5632 - acc: 0.8203 - val_loss: 1.0776 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.85445\n",
      "Epoch 432/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5312 - acc: 0.8341 - val_loss: 0.9853 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.85445\n",
      "Epoch 433/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5402 - acc: 0.8335 - val_loss: 0.9896 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.85445\n",
      "Epoch 434/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5688 - acc: 0.8158 - val_loss: 1.6527 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.85445\n",
      "Epoch 435/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5384 - acc: 0.8332 - val_loss: 1.0907 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.85445\n",
      "Epoch 436/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5216 - acc: 0.8383 - val_loss: 1.1250 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.85445\n",
      "Epoch 437/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5451 - acc: 0.8200 - val_loss: 1.4021 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.85445\n",
      "Epoch 438/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5416 - acc: 0.8377 - val_loss: 1.0262 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.85445\n",
      "Epoch 439/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5574 - acc: 0.8257 - val_loss: 1.0288 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.85445\n",
      "Epoch 440/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5584 - acc: 0.8245 - val_loss: 1.0606 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.85445\n",
      "Epoch 441/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5292 - acc: 0.8350 - val_loss: 1.0502 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.85445\n",
      "Epoch 442/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5295 - acc: 0.8344 - val_loss: 1.0278 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.85445\n",
      "Epoch 443/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5469 - acc: 0.8221 - val_loss: 1.0527 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.85445\n",
      "Epoch 444/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5479 - acc: 0.8257 - val_loss: 1.2318 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.85445\n",
      "Epoch 445/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5432 - acc: 0.8296 - val_loss: 1.1268 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.85445\n",
      "Epoch 446/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5217 - acc: 0.8453 - val_loss: 1.0784 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.85445\n",
      "Epoch 447/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5383 - acc: 0.8338 - val_loss: 1.1214 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.85445\n",
      "Epoch 448/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5465 - acc: 0.8356 - val_loss: 1.0711 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.85445\n",
      "Epoch 449/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5337 - acc: 0.8281 - val_loss: 0.9248 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00449: val_acc improved from 0.85445 to 0.87332, saving model to model/mfcc7/LGD_fold9_resnet1-.h5\n",
      "Epoch 450/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5155 - acc: 0.8353 - val_loss: 1.0458 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.87332\n",
      "Epoch 451/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5489 - acc: 0.8248 - val_loss: 1.0746 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.87332\n",
      "Epoch 452/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5299 - acc: 0.8341 - val_loss: 0.9743 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.87332\n",
      "Epoch 453/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5074 - acc: 0.8335 - val_loss: 0.9802 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.87332\n",
      "Epoch 454/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5265 - acc: 0.8275 - val_loss: 1.1013 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.87332\n",
      "Epoch 455/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5322 - acc: 0.8377 - val_loss: 1.1619 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.87332\n",
      "Epoch 456/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5337 - acc: 0.8290 - val_loss: 1.1128 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.87332\n",
      "Epoch 457/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5394 - acc: 0.8245 - val_loss: 1.2607 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.87332\n",
      "Epoch 458/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5257 - acc: 0.8242 - val_loss: 1.1747 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.87332\n",
      "Epoch 459/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5162 - acc: 0.8347 - val_loss: 1.0695 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.87332\n",
      "Epoch 460/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5183 - acc: 0.8344 - val_loss: 1.0165 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.87332\n",
      "Epoch 461/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5308 - acc: 0.8293 - val_loss: 1.0346 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.87332\n",
      "Epoch 462/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5283 - acc: 0.8332 - val_loss: 1.0712 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.87332\n",
      "Epoch 463/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5384 - acc: 0.8242 - val_loss: 1.0540 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.87332\n",
      "Epoch 464/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5390 - acc: 0.8176 - val_loss: 1.1303 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.87332\n",
      "Epoch 465/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5340 - acc: 0.8293 - val_loss: 1.0787 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.87332\n",
      "Epoch 466/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5405 - acc: 0.8260 - val_loss: 0.9657 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.87332\n",
      "Epoch 467/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5016 - acc: 0.8404 - val_loss: 1.0563 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.87332\n",
      "Epoch 468/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5272 - acc: 0.8257 - val_loss: 0.9864 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.87332\n",
      "Epoch 469/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5312 - acc: 0.8389 - val_loss: 1.0462 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.87332\n",
      "Epoch 470/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5224 - acc: 0.8296 - val_loss: 1.0203 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.87332\n",
      "Epoch 471/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5285 - acc: 0.8314 - val_loss: 0.9847 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.87332\n",
      "Epoch 472/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5237 - acc: 0.8344 - val_loss: 0.9760 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.87332\n",
      "Epoch 473/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5288 - acc: 0.8269 - val_loss: 1.0796 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.87332\n",
      "Epoch 474/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4938 - acc: 0.8428 - val_loss: 1.0251 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.87332\n",
      "Epoch 475/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5083 - acc: 0.8305 - val_loss: 1.0514 - val_acc: 0.8437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00475: val_acc did not improve from 0.87332\n",
      "Epoch 476/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5243 - acc: 0.8371 - val_loss: 1.0135 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.87332\n",
      "Epoch 477/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5278 - acc: 0.8353 - val_loss: 1.1479 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.87332\n",
      "Epoch 478/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5116 - acc: 0.8438 - val_loss: 1.2744 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.87332\n",
      "Epoch 479/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5108 - acc: 0.8368 - val_loss: 1.0513 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.87332\n",
      "Epoch 480/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5124 - acc: 0.8374 - val_loss: 0.9774 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.87332\n",
      "Epoch 481/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5174 - acc: 0.8438 - val_loss: 1.0502 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.87332\n",
      "Epoch 482/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5247 - acc: 0.8335 - val_loss: 1.0889 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.87332\n",
      "Epoch 483/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5175 - acc: 0.8317 - val_loss: 1.2558 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.87332\n",
      "Epoch 484/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5251 - acc: 0.8389 - val_loss: 1.1325 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.87332\n",
      "Epoch 485/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8329 - val_loss: 1.0691 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.87332\n",
      "Epoch 486/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5210 - acc: 0.8395 - val_loss: 0.9541 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.87332\n",
      "Epoch 487/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5189 - acc: 0.8275 - val_loss: 1.0261 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.87332\n",
      "Epoch 488/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5242 - acc: 0.8380 - val_loss: 1.1889 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.87332\n",
      "Epoch 489/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5112 - acc: 0.8302 - val_loss: 0.9886 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.87332\n",
      "Epoch 490/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5363 - acc: 0.8092 - val_loss: 1.1119 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.87332\n",
      "Epoch 491/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5182 - acc: 0.8257 - val_loss: 1.1021 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.87332\n",
      "Epoch 492/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5375 - acc: 0.8281 - val_loss: 1.0447 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.87332\n",
      "Epoch 493/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5385 - acc: 0.8194 - val_loss: 1.1846 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.87332\n",
      "Epoch 494/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5136 - acc: 0.8422 - val_loss: 1.0755 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.87332\n",
      "Epoch 495/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5110 - acc: 0.8338 - val_loss: 1.0331 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.87332\n",
      "Epoch 496/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8311 - val_loss: 1.0633 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.87332\n",
      "Epoch 497/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5369 - acc: 0.8347 - val_loss: 0.9867 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.87332\n",
      "Epoch 498/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4969 - acc: 0.8392 - val_loss: 0.9712 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.87332\n",
      "Epoch 499/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4917 - acc: 0.8356 - val_loss: 0.9790 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.87332\n",
      "Epoch 500/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5142 - acc: 0.8260 - val_loss: 1.0288 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.87332\n",
      "Epoch 501/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5283 - acc: 0.8320 - val_loss: 1.0820 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00501: val_acc did not improve from 0.87332\n",
      "Epoch 502/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5406 - acc: 0.8299 - val_loss: 1.0942 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00502: val_acc did not improve from 0.87332\n",
      "Epoch 503/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5223 - acc: 0.8368 - val_loss: 1.1077 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00503: val_acc did not improve from 0.87332\n",
      "Epoch 504/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5194 - acc: 0.8374 - val_loss: 1.0882 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00504: val_acc did not improve from 0.87332\n",
      "Epoch 505/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5259 - acc: 0.8224 - val_loss: 0.9653 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00505: val_acc did not improve from 0.87332\n",
      "Epoch 506/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5135 - acc: 0.8371 - val_loss: 1.1401 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00506: val_acc did not improve from 0.87332\n",
      "Epoch 507/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.5151 - acc: 0.8308 - val_loss: 1.0324 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00507: val_acc did not improve from 0.87332\n",
      "Epoch 508/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.4993 - acc: 0.8329 - val_loss: 1.1463 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00508: val_acc did not improve from 0.87332\n",
      "Epoch 509/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5040 - acc: 0.8401 - val_loss: 1.0510 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00509: val_acc did not improve from 0.87332\n",
      "Epoch 510/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4966 - acc: 0.8404 - val_loss: 1.0626 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00510: val_acc did not improve from 0.87332\n",
      "Epoch 511/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.4792 - acc: 0.8278 - val_loss: 0.9711 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00511: val_acc did not improve from 0.87332\n",
      "Epoch 512/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5229 - acc: 0.8281 - val_loss: 1.1331 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00512: val_acc did not improve from 0.87332\n",
      "Epoch 513/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5314 - acc: 0.8398 - val_loss: 1.0362 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00513: val_acc did not improve from 0.87332\n",
      "Epoch 514/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5035 - acc: 0.8389 - val_loss: 0.9269 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00514: val_acc did not improve from 0.87332\n",
      "Epoch 515/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5077 - acc: 0.8359 - val_loss: 1.0504 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00515: val_acc did not improve from 0.87332\n",
      "Epoch 516/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5046 - acc: 0.8341 - val_loss: 0.9737 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00516: val_acc did not improve from 0.87332\n",
      "Epoch 517/3000\n",
      "52/52 [==============================] - 6s 116ms/step - loss: 1.5091 - acc: 0.8299 - val_loss: 1.0048 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00517: val_acc did not improve from 0.87332\n",
      "Epoch 518/3000\n",
      "52/52 [==============================] - 6s 117ms/step - loss: 1.4953 - acc: 0.8410 - val_loss: 1.0361 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00518: val_acc did not improve from 0.87332\n",
      "Epoch 00518: early stopping\n",
      "(3418, 60, 259, 1) (3418, 41)\n",
      "===train semi_9===\n",
      "semi loading: model/mfcc7/LGD_fold9_resnet1-.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/3000\n",
      "53/53 [==============================] - 12s 229ms/step - loss: 2.1275 - acc: 0.6176 - val_loss: 0.8698 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00005: val_acc improved from -inf to 0.88410, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 6/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.9640 - acc: 0.6775 - val_loss: 0.8594 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88410\n",
      "Epoch 7/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.8558 - acc: 0.7117 - val_loss: 0.8715 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88410\n",
      "Epoch 8/3000\n",
      "53/53 [==============================] - 6s 116ms/step - loss: 1.8232 - acc: 0.7158 - val_loss: 0.8623 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88410\n",
      "Epoch 9/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.7704 - acc: 0.7344 - val_loss: 0.8595 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88410\n",
      "Epoch 10/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.7144 - acc: 0.7497 - val_loss: 0.8554 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88410\n",
      "Epoch 11/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6810 - acc: 0.7668 - val_loss: 0.8691 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88410\n",
      "Epoch 12/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6902 - acc: 0.7574 - val_loss: 0.8457 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88410\n",
      "Epoch 13/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6438 - acc: 0.7792 - val_loss: 0.8265 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88410\n",
      "Epoch 14/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6545 - acc: 0.7730 - val_loss: 0.8475 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88410\n",
      "Epoch 15/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.6478 - acc: 0.7807 - val_loss: 0.8513 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88410\n",
      "Epoch 16/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6244 - acc: 0.7795 - val_loss: 0.8369 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88410\n",
      "Epoch 17/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5760 - acc: 0.7998 - val_loss: 0.8336 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88410\n",
      "Epoch 18/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.6021 - acc: 0.7857 - val_loss: 0.8435 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88410\n",
      "Epoch 19/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5826 - acc: 0.7922 - val_loss: 0.8236 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88410\n",
      "Epoch 20/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5762 - acc: 0.8090 - val_loss: 0.8159 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88410\n",
      "Epoch 21/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5575 - acc: 0.8028 - val_loss: 0.8023 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88410\n",
      "Epoch 22/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5580 - acc: 0.8084 - val_loss: 0.8128 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88410\n",
      "Epoch 23/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5555 - acc: 0.8075 - val_loss: 0.8175 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88410\n",
      "Epoch 24/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5170 - acc: 0.8060 - val_loss: 0.8271 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88410\n",
      "Epoch 25/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4982 - acc: 0.8296 - val_loss: 0.8081 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88410\n",
      "Epoch 26/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5266 - acc: 0.8134 - val_loss: 0.7970 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88410\n",
      "Epoch 27/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5173 - acc: 0.8078 - val_loss: 0.7929 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88410\n",
      "Epoch 28/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5162 - acc: 0.8116 - val_loss: 0.7962 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88410\n",
      "Epoch 29/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5289 - acc: 0.8075 - val_loss: 0.7933 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88410\n",
      "Epoch 30/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5115 - acc: 0.8261 - val_loss: 0.7977 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88410\n",
      "Epoch 31/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.5124 - acc: 0.8172 - val_loss: 0.7834 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88410\n",
      "Epoch 32/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4979 - acc: 0.8166 - val_loss: 0.8151 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88410\n",
      "Epoch 33/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4818 - acc: 0.8157 - val_loss: 0.8076 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88410\n",
      "Epoch 34/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4813 - acc: 0.8202 - val_loss: 0.8019 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88410\n",
      "Epoch 35/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4594 - acc: 0.8337 - val_loss: 0.7971 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88410\n",
      "Epoch 36/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4663 - acc: 0.8275 - val_loss: 0.8042 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88410\n",
      "Epoch 37/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4862 - acc: 0.8334 - val_loss: 0.8274 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88410\n",
      "Epoch 38/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4662 - acc: 0.8370 - val_loss: 0.8192 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88410\n",
      "Epoch 39/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4768 - acc: 0.8216 - val_loss: 0.8054 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88410\n",
      "Epoch 40/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4679 - acc: 0.8293 - val_loss: 0.7918 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88410\n",
      "Epoch 41/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4944 - acc: 0.8272 - val_loss: 0.7974 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88410\n",
      "Epoch 42/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4569 - acc: 0.8290 - val_loss: 0.8028 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88410\n",
      "Epoch 43/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4474 - acc: 0.8296 - val_loss: 0.8237 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88410\n",
      "Epoch 44/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4271 - acc: 0.8370 - val_loss: 0.7880 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88410\n",
      "Epoch 45/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4513 - acc: 0.8379 - val_loss: 0.8002 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88410\n",
      "Epoch 46/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4158 - acc: 0.8373 - val_loss: 0.8003 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88410\n",
      "Epoch 47/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4540 - acc: 0.8293 - val_loss: 0.7930 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88410\n",
      "Epoch 48/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4229 - acc: 0.8429 - val_loss: 0.8211 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88410\n",
      "Epoch 49/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4259 - acc: 0.8435 - val_loss: 0.8058 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88410\n",
      "Epoch 50/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4271 - acc: 0.8358 - val_loss: 0.7939 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88410\n",
      "Epoch 51/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4341 - acc: 0.8231 - val_loss: 0.7974 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88410\n",
      "Epoch 52/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4396 - acc: 0.8302 - val_loss: 0.7965 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88410\n",
      "Epoch 53/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4057 - acc: 0.8455 - val_loss: 0.8038 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88410\n",
      "Epoch 54/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4310 - acc: 0.8358 - val_loss: 0.7976 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88410\n",
      "Epoch 55/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4235 - acc: 0.8355 - val_loss: 0.7952 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88410\n",
      "Epoch 56/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3852 - acc: 0.8538 - val_loss: 0.8042 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88410\n",
      "Epoch 57/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3949 - acc: 0.8346 - val_loss: 0.7949 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88410\n",
      "Epoch 58/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4120 - acc: 0.8414 - val_loss: 0.7872 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88410\n",
      "Epoch 59/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3858 - acc: 0.8499 - val_loss: 0.7961 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88410\n",
      "Epoch 60/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3755 - acc: 0.8508 - val_loss: 0.7672 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.88410 to 0.88949, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 61/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.4319 - acc: 0.8278 - val_loss: 0.8104 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88949\n",
      "Epoch 62/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.4173 - acc: 0.8514 - val_loss: 0.7709 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88949\n",
      "Epoch 63/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3943 - acc: 0.8496 - val_loss: 0.7859 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88949\n",
      "Epoch 64/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3813 - acc: 0.8470 - val_loss: 0.7789 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88949\n",
      "Epoch 65/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3919 - acc: 0.8573 - val_loss: 0.7583 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88949\n",
      "Epoch 66/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3846 - acc: 0.8517 - val_loss: 0.7651 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88949\n",
      "Epoch 67/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3987 - acc: 0.8411 - val_loss: 0.7502 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88949\n",
      "Epoch 68/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3707 - acc: 0.8523 - val_loss: 0.7627 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88949\n",
      "Epoch 69/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3911 - acc: 0.8426 - val_loss: 0.7665 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88949\n",
      "Epoch 70/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3935 - acc: 0.8352 - val_loss: 0.7745 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88949\n",
      "Epoch 71/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3876 - acc: 0.8502 - val_loss: 0.7791 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88949\n",
      "Epoch 72/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3947 - acc: 0.8458 - val_loss: 0.7885 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88949\n",
      "Epoch 73/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3671 - acc: 0.8576 - val_loss: 0.7813 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88949\n",
      "Epoch 74/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3656 - acc: 0.8555 - val_loss: 0.7834 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88949\n",
      "Epoch 75/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3780 - acc: 0.8461 - val_loss: 0.7898 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88949\n",
      "Epoch 76/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3557 - acc: 0.8541 - val_loss: 0.7958 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88949\n",
      "Epoch 77/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3453 - acc: 0.8600 - val_loss: 0.7556 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88949\n",
      "Epoch 78/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3822 - acc: 0.8488 - val_loss: 0.7700 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88949\n",
      "Epoch 79/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3810 - acc: 0.8502 - val_loss: 0.7567 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88949\n",
      "Epoch 80/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3246 - acc: 0.8576 - val_loss: 0.7528 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.88949 to 0.89218, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 81/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3488 - acc: 0.8532 - val_loss: 0.7599 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89218\n",
      "Epoch 82/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3668 - acc: 0.8470 - val_loss: 0.7552 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89218\n",
      "Epoch 83/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3795 - acc: 0.8449 - val_loss: 0.7567 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89218\n",
      "Epoch 84/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3650 - acc: 0.8435 - val_loss: 0.7477 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89218\n",
      "Epoch 85/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3606 - acc: 0.8573 - val_loss: 0.7644 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89218\n",
      "Epoch 86/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3453 - acc: 0.8617 - val_loss: 0.7666 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89218\n",
      "Epoch 87/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3426 - acc: 0.8547 - val_loss: 0.7629 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89218\n",
      "Epoch 88/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3875 - acc: 0.8600 - val_loss: 0.7703 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89218\n",
      "Epoch 89/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3353 - acc: 0.8723 - val_loss: 0.7334 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.89218 to 0.89488, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 90/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3581 - acc: 0.8629 - val_loss: 0.7364 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.89488 to 0.89757, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 91/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3473 - acc: 0.8532 - val_loss: 0.7410 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89757\n",
      "Epoch 92/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3442 - acc: 0.8555 - val_loss: 0.7491 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89757\n",
      "Epoch 93/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3381 - acc: 0.8552 - val_loss: 0.7533 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89757\n",
      "Epoch 94/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3271 - acc: 0.8667 - val_loss: 0.7395 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89757\n",
      "Epoch 95/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3423 - acc: 0.8688 - val_loss: 0.7435 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89757\n",
      "Epoch 96/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3362 - acc: 0.8694 - val_loss: 0.7391 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89757\n",
      "Epoch 97/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3395 - acc: 0.8603 - val_loss: 0.7373 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89757\n",
      "Epoch 98/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3570 - acc: 0.8523 - val_loss: 0.7341 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89757\n",
      "Epoch 99/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3319 - acc: 0.8608 - val_loss: 0.7351 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89757\n",
      "Epoch 100/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3320 - acc: 0.8638 - val_loss: 0.7455 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89757\n",
      "Epoch 101/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3077 - acc: 0.8688 - val_loss: 0.7452 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89757\n",
      "Epoch 102/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3172 - acc: 0.8608 - val_loss: 0.7450 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89757\n",
      "Epoch 103/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3439 - acc: 0.8508 - val_loss: 0.7416 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89757\n",
      "Epoch 104/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3134 - acc: 0.8673 - val_loss: 0.7423 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89757\n",
      "Epoch 105/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3211 - acc: 0.8588 - val_loss: 0.7348 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89757\n",
      "Epoch 106/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3246 - acc: 0.8603 - val_loss: 0.7327 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89757\n",
      "Epoch 107/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3166 - acc: 0.8644 - val_loss: 0.7363 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89757\n",
      "Epoch 108/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3210 - acc: 0.8523 - val_loss: 0.7444 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89757\n",
      "Epoch 109/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3236 - acc: 0.8544 - val_loss: 0.7466 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89757\n",
      "Epoch 110/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3095 - acc: 0.8729 - val_loss: 0.7397 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89757\n",
      "Epoch 111/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3110 - acc: 0.8647 - val_loss: 0.7403 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89757\n",
      "Epoch 112/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3290 - acc: 0.8526 - val_loss: 0.7382 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89757\n",
      "Epoch 113/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3091 - acc: 0.8667 - val_loss: 0.7363 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00113: val_acc improved from 0.89757 to 0.90027, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 114/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3017 - acc: 0.8588 - val_loss: 0.7340 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90027\n",
      "Epoch 115/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3249 - acc: 0.8579 - val_loss: 0.7342 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90027\n",
      "Epoch 116/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3140 - acc: 0.8606 - val_loss: 0.7355 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.90027 to 0.90296, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 117/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3086 - acc: 0.8629 - val_loss: 0.7363 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90296\n",
      "Epoch 118/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3069 - acc: 0.8679 - val_loss: 0.7367 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90296\n",
      "Epoch 119/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3166 - acc: 0.8700 - val_loss: 0.7396 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90296\n",
      "Epoch 120/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2784 - acc: 0.8756 - val_loss: 0.7369 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90296\n",
      "Epoch 121/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3146 - acc: 0.8694 - val_loss: 0.7353 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90296\n",
      "Epoch 122/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3203 - acc: 0.8591 - val_loss: 0.7356 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90296\n",
      "Epoch 123/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3106 - acc: 0.8538 - val_loss: 0.7365 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90296\n",
      "Epoch 124/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3268 - acc: 0.8576 - val_loss: 0.7364 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90296\n",
      "Epoch 125/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3063 - acc: 0.8721 - val_loss: 0.7383 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90296\n",
      "Epoch 126/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8662 - val_loss: 0.7414 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90296\n",
      "Epoch 127/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2993 - acc: 0.8726 - val_loss: 0.7416 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90296\n",
      "Epoch 128/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8685 - val_loss: 0.7381 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.90296 to 0.90566, saving model to model/mfcc7/LGD_semi_fold9_resnet1.h5\n",
      "Epoch 129/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3287 - acc: 0.8614 - val_loss: 0.7382 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90566\n",
      "Epoch 130/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8576 - val_loss: 0.7386 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90566\n",
      "Epoch 131/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2911 - acc: 0.8623 - val_loss: 0.7391 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90566\n",
      "Epoch 132/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3036 - acc: 0.8576 - val_loss: 0.7383 - val_acc: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90566\n",
      "Epoch 133/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2915 - acc: 0.8617 - val_loss: 0.7379 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90566\n",
      "Epoch 134/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2984 - acc: 0.8735 - val_loss: 0.7374 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90566\n",
      "Epoch 135/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3162 - acc: 0.8647 - val_loss: 0.7385 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90566\n",
      "Epoch 136/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3096 - acc: 0.8597 - val_loss: 0.7406 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90566\n",
      "Epoch 137/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3134 - acc: 0.8603 - val_loss: 0.7394 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90566\n",
      "Epoch 138/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3081 - acc: 0.8691 - val_loss: 0.7406 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90566\n",
      "Epoch 139/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3024 - acc: 0.8682 - val_loss: 0.7391 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90566\n",
      "Epoch 140/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3272 - acc: 0.8617 - val_loss: 0.7392 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90566\n",
      "Epoch 141/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8632 - val_loss: 0.7387 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90566\n",
      "Epoch 142/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8685 - val_loss: 0.7363 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90566\n",
      "Epoch 143/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3148 - acc: 0.8608 - val_loss: 0.7360 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90566\n",
      "Epoch 144/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3297 - acc: 0.8608 - val_loss: 0.7370 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90566\n",
      "Epoch 145/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3079 - acc: 0.8659 - val_loss: 0.7373 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90566\n",
      "Epoch 146/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3241 - acc: 0.8647 - val_loss: 0.7364 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90566\n",
      "Epoch 147/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3092 - acc: 0.8606 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90566\n",
      "Epoch 148/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3136 - acc: 0.8502 - val_loss: 0.7358 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90566\n",
      "Epoch 149/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3223 - acc: 0.8623 - val_loss: 0.7353 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90566\n",
      "Epoch 150/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3175 - acc: 0.8641 - val_loss: 0.7359 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90566\n",
      "Epoch 151/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2974 - acc: 0.8606 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.90566\n",
      "Epoch 152/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3132 - acc: 0.8579 - val_loss: 0.7349 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.90566\n",
      "Epoch 153/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3007 - acc: 0.8768 - val_loss: 0.7347 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.90566\n",
      "Epoch 154/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3159 - acc: 0.8582 - val_loss: 0.7354 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.90566\n",
      "Epoch 155/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3173 - acc: 0.8626 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.90566\n",
      "Epoch 156/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2986 - acc: 0.8700 - val_loss: 0.7346 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.90566\n",
      "Epoch 157/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3102 - acc: 0.8567 - val_loss: 0.7378 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.90566\n",
      "Epoch 158/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2788 - acc: 0.8747 - val_loss: 0.7364 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.90566\n",
      "Epoch 159/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3069 - acc: 0.8697 - val_loss: 0.7359 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.90566\n",
      "Epoch 160/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2902 - acc: 0.8726 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.90566\n",
      "Epoch 161/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2976 - acc: 0.8685 - val_loss: 0.7365 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.90566\n",
      "Epoch 162/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2991 - acc: 0.8818 - val_loss: 0.7368 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.90566\n",
      "Epoch 163/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2871 - acc: 0.8741 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.90566\n",
      "Epoch 164/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2999 - acc: 0.8670 - val_loss: 0.7366 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.90566\n",
      "Epoch 165/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3148 - acc: 0.8659 - val_loss: 0.7375 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.90566\n",
      "Epoch 166/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3191 - acc: 0.8594 - val_loss: 0.7367 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.90566\n",
      "Epoch 167/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3145 - acc: 0.8670 - val_loss: 0.7379 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.90566\n",
      "Epoch 168/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2969 - acc: 0.8641 - val_loss: 0.7360 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.90566\n",
      "Epoch 169/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3032 - acc: 0.8620 - val_loss: 0.7375 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.90566\n",
      "Epoch 170/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3062 - acc: 0.8650 - val_loss: 0.7381 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.90566\n",
      "Epoch 171/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3077 - acc: 0.8582 - val_loss: 0.7373 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.90566\n",
      "Epoch 172/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2950 - acc: 0.8579 - val_loss: 0.7359 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.90566\n",
      "Epoch 173/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3065 - acc: 0.8679 - val_loss: 0.7368 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.90566\n",
      "Epoch 174/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3013 - acc: 0.8667 - val_loss: 0.7358 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.90566\n",
      "Epoch 175/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3034 - acc: 0.8715 - val_loss: 0.7342 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.90566\n",
      "Epoch 176/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3109 - acc: 0.8629 - val_loss: 0.7346 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.90566\n",
      "Epoch 177/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2943 - acc: 0.8588 - val_loss: 0.7350 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.90566\n",
      "Epoch 178/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3049 - acc: 0.8712 - val_loss: 0.7357 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.90566\n",
      "Epoch 179/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3020 - acc: 0.8591 - val_loss: 0.7363 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.90566\n",
      "Epoch 180/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3139 - acc: 0.8632 - val_loss: 0.7364 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.90566\n",
      "Epoch 181/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3079 - acc: 0.8682 - val_loss: 0.7350 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.90566\n",
      "Epoch 182/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3128 - acc: 0.8623 - val_loss: 0.7322 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.90566\n",
      "Epoch 183/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3006 - acc: 0.8670 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.90566\n",
      "Epoch 184/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3019 - acc: 0.8762 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.90566\n",
      "Epoch 185/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2916 - acc: 0.8608 - val_loss: 0.7342 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.90566\n",
      "Epoch 186/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2769 - acc: 0.8759 - val_loss: 0.7340 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.90566\n",
      "Epoch 187/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3184 - acc: 0.8670 - val_loss: 0.7342 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.90566\n",
      "Epoch 188/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8665 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.90566\n",
      "Epoch 189/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3059 - acc: 0.8635 - val_loss: 0.7327 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.90566\n",
      "Epoch 190/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3189 - acc: 0.8594 - val_loss: 0.7321 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.90566\n",
      "Epoch 191/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3125 - acc: 0.8712 - val_loss: 0.7319 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.90566\n",
      "Epoch 192/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2935 - acc: 0.8726 - val_loss: 0.7315 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.90566\n",
      "Epoch 193/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3041 - acc: 0.8744 - val_loss: 0.7317 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.90566\n",
      "Epoch 194/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3045 - acc: 0.8591 - val_loss: 0.7322 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.90566\n",
      "Epoch 195/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2920 - acc: 0.8688 - val_loss: 0.7328 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.90566\n",
      "Epoch 196/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3036 - acc: 0.8762 - val_loss: 0.7345 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.90566\n",
      "Epoch 197/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2991 - acc: 0.8726 - val_loss: 0.7338 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.90566\n",
      "Epoch 198/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3062 - acc: 0.8641 - val_loss: 0.7339 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.90566\n",
      "Epoch 199/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3002 - acc: 0.8620 - val_loss: 0.7343 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.90566\n",
      "Epoch 200/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3033 - acc: 0.8623 - val_loss: 0.7356 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.90566\n",
      "Epoch 201/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3129 - acc: 0.8700 - val_loss: 0.7353 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.90566\n",
      "Epoch 202/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2815 - acc: 0.8729 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.90566\n",
      "Epoch 203/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3102 - acc: 0.8632 - val_loss: 0.7320 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.90566\n",
      "Epoch 204/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2948 - acc: 0.8726 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.90566\n",
      "Epoch 205/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3185 - acc: 0.8653 - val_loss: 0.7319 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.90566\n",
      "Epoch 206/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3194 - acc: 0.8620 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.90566\n",
      "Epoch 207/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3065 - acc: 0.8653 - val_loss: 0.7326 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.90566\n",
      "Epoch 208/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3177 - acc: 0.8653 - val_loss: 0.7341 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.90566\n",
      "Epoch 209/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2984 - acc: 0.8762 - val_loss: 0.7343 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.90566\n",
      "Epoch 210/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3082 - acc: 0.8561 - val_loss: 0.7330 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.90566\n",
      "Epoch 211/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3003 - acc: 0.8665 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.90566\n",
      "Epoch 212/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2895 - acc: 0.8614 - val_loss: 0.7343 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.90566\n",
      "Epoch 213/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3004 - acc: 0.8782 - val_loss: 0.7336 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.90566\n",
      "Epoch 214/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2756 - acc: 0.8732 - val_loss: 0.7329 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.90566\n",
      "Epoch 215/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3099 - acc: 0.8650 - val_loss: 0.7324 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.90566\n",
      "Epoch 216/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8632 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.90566\n",
      "Epoch 217/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8685 - val_loss: 0.7333 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.90566\n",
      "Epoch 218/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2864 - acc: 0.8670 - val_loss: 0.7334 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.90566\n",
      "Epoch 219/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3110 - acc: 0.8732 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.90566\n",
      "Epoch 220/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2873 - acc: 0.8697 - val_loss: 0.7340 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.90566\n",
      "Epoch 221/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3053 - acc: 0.8653 - val_loss: 0.7355 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.90566\n",
      "Epoch 222/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8570 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.90566\n",
      "Epoch 223/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3140 - acc: 0.8641 - val_loss: 0.7356 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.90566\n",
      "Epoch 224/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3066 - acc: 0.8659 - val_loss: 0.7358 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.90566\n",
      "Epoch 225/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2930 - acc: 0.8726 - val_loss: 0.7358 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.90566\n",
      "Epoch 226/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2889 - acc: 0.8738 - val_loss: 0.7373 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.90566\n",
      "Epoch 227/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3028 - acc: 0.8600 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.90566\n",
      "Epoch 228/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3174 - acc: 0.8679 - val_loss: 0.7323 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.90566\n",
      "Epoch 229/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2974 - acc: 0.8635 - val_loss: 0.7330 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.90566\n",
      "Epoch 230/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2896 - acc: 0.8688 - val_loss: 0.7335 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.90566\n",
      "Epoch 231/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2980 - acc: 0.8591 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.90566\n",
      "Epoch 232/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3015 - acc: 0.8715 - val_loss: 0.7335 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.90566\n",
      "Epoch 233/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2969 - acc: 0.8676 - val_loss: 0.7351 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.90566\n",
      "Epoch 234/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2853 - acc: 0.8673 - val_loss: 0.7349 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.90566\n",
      "Epoch 235/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3108 - acc: 0.8726 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.90566\n",
      "Epoch 236/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3028 - acc: 0.8659 - val_loss: 0.7341 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.90566\n",
      "Epoch 237/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2951 - acc: 0.8694 - val_loss: 0.7340 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.90566\n",
      "Epoch 238/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3083 - acc: 0.8715 - val_loss: 0.7361 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.90566\n",
      "Epoch 239/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3281 - acc: 0.8597 - val_loss: 0.7353 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.90566\n",
      "Epoch 240/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3053 - acc: 0.8626 - val_loss: 0.7358 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.90566\n",
      "Epoch 241/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3004 - acc: 0.8659 - val_loss: 0.7341 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.90566\n",
      "Epoch 242/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2770 - acc: 0.8744 - val_loss: 0.7354 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.90566\n",
      "Epoch 243/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2881 - acc: 0.8688 - val_loss: 0.7367 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.90566\n",
      "Epoch 244/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2998 - acc: 0.8709 - val_loss: 0.7366 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.90566\n",
      "Epoch 245/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2769 - acc: 0.8676 - val_loss: 0.7371 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.90566\n",
      "Epoch 246/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2911 - acc: 0.8741 - val_loss: 0.7374 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.90566\n",
      "Epoch 247/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3090 - acc: 0.8718 - val_loss: 0.7366 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.90566\n",
      "Epoch 248/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3158 - acc: 0.8694 - val_loss: 0.7365 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.90566\n",
      "Epoch 249/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3045 - acc: 0.8756 - val_loss: 0.7354 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.90566\n",
      "Epoch 250/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3044 - acc: 0.8679 - val_loss: 0.7338 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.90566\n",
      "Epoch 251/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2888 - acc: 0.8679 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.90566\n",
      "Epoch 252/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2927 - acc: 0.8632 - val_loss: 0.7344 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.90566\n",
      "Epoch 253/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3071 - acc: 0.8647 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.90566\n",
      "Epoch 254/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2958 - acc: 0.8608 - val_loss: 0.7334 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.90566\n",
      "Epoch 255/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3184 - acc: 0.8620 - val_loss: 0.7344 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.90566\n",
      "Epoch 256/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2906 - acc: 0.8741 - val_loss: 0.7343 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.90566\n",
      "Epoch 257/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3106 - acc: 0.8641 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.90566\n",
      "Epoch 258/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3257 - acc: 0.8697 - val_loss: 0.7338 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.90566\n",
      "Epoch 259/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3000 - acc: 0.8638 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.90566\n",
      "Epoch 260/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2966 - acc: 0.8591 - val_loss: 0.7286 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.90566\n",
      "Epoch 261/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2896 - acc: 0.8667 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.90566\n",
      "Epoch 262/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3042 - acc: 0.8603 - val_loss: 0.7337 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.90566\n",
      "Epoch 263/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2920 - acc: 0.8659 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.90566\n",
      "Epoch 264/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2962 - acc: 0.8765 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.90566\n",
      "Epoch 265/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3079 - acc: 0.8662 - val_loss: 0.7305 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.90566\n",
      "Epoch 266/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2895 - acc: 0.8732 - val_loss: 0.7311 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.90566\n",
      "Epoch 267/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2971 - acc: 0.8673 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.90566\n",
      "Epoch 268/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3011 - acc: 0.8788 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.90566\n",
      "Epoch 269/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2999 - acc: 0.8726 - val_loss: 0.7321 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.90566\n",
      "Epoch 270/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3090 - acc: 0.8623 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.90566\n",
      "Epoch 271/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3135 - acc: 0.8538 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.90566\n",
      "Epoch 272/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3008 - acc: 0.8753 - val_loss: 0.7327 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.90566\n",
      "Epoch 273/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8721 - val_loss: 0.7323 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.90566\n",
      "Epoch 274/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2997 - acc: 0.8620 - val_loss: 0.7333 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.90566\n",
      "Epoch 275/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2888 - acc: 0.8715 - val_loss: 0.7324 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.90566\n",
      "Epoch 276/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3104 - acc: 0.8608 - val_loss: 0.7331 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.90566\n",
      "Epoch 277/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2952 - acc: 0.8659 - val_loss: 0.7315 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.90566\n",
      "Epoch 278/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2968 - acc: 0.8732 - val_loss: 0.7291 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.90566\n",
      "Epoch 279/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3063 - acc: 0.8679 - val_loss: 0.7297 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.90566\n",
      "Epoch 280/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2870 - acc: 0.8732 - val_loss: 0.7326 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.90566\n",
      "Epoch 281/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2830 - acc: 0.8679 - val_loss: 0.7331 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.90566\n",
      "Epoch 282/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3134 - acc: 0.8623 - val_loss: 0.7325 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.90566\n",
      "Epoch 283/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8638 - val_loss: 0.7335 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.90566\n",
      "Epoch 284/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2995 - acc: 0.8653 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.90566\n",
      "Epoch 285/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2881 - acc: 0.8682 - val_loss: 0.7349 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.90566\n",
      "Epoch 286/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2892 - acc: 0.8653 - val_loss: 0.7342 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.90566\n",
      "Epoch 287/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3173 - acc: 0.8614 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.90566\n",
      "Epoch 288/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2861 - acc: 0.8762 - val_loss: 0.7317 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.90566\n",
      "Epoch 289/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2836 - acc: 0.8741 - val_loss: 0.7308 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.90566\n",
      "Epoch 290/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2981 - acc: 0.8738 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.90566\n",
      "Epoch 291/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2873 - acc: 0.8644 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.90566\n",
      "Epoch 292/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3049 - acc: 0.8697 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.90566\n",
      "Epoch 293/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3075 - acc: 0.8726 - val_loss: 0.7312 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.90566\n",
      "Epoch 294/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2938 - acc: 0.8726 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.90566\n",
      "Epoch 295/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3004 - acc: 0.8647 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.90566\n",
      "Epoch 296/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3044 - acc: 0.8576 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.90566\n",
      "Epoch 297/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2738 - acc: 0.8735 - val_loss: 0.7326 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.90566\n",
      "Epoch 298/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3127 - acc: 0.8638 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.90566\n",
      "Epoch 299/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3196 - acc: 0.8535 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.90566\n",
      "Epoch 300/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2933 - acc: 0.8806 - val_loss: 0.7336 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.90566\n",
      "Epoch 301/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2782 - acc: 0.8729 - val_loss: 0.7336 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.90566\n",
      "Epoch 302/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2762 - acc: 0.8715 - val_loss: 0.7332 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.90566\n",
      "Epoch 303/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2831 - acc: 0.8735 - val_loss: 0.7339 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.90566\n",
      "Epoch 304/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2892 - acc: 0.8597 - val_loss: 0.7321 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.90566\n",
      "Epoch 305/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8614 - val_loss: 0.7322 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.90566\n",
      "Epoch 306/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3003 - acc: 0.8709 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.90566\n",
      "Epoch 307/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3016 - acc: 0.8712 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.90566\n",
      "Epoch 308/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3221 - acc: 0.8650 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.90566\n",
      "Epoch 309/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8806 - val_loss: 0.7327 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.90566\n",
      "Epoch 310/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2897 - acc: 0.8659 - val_loss: 0.7328 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.90566\n",
      "Epoch 311/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3001 - acc: 0.8656 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.90566\n",
      "Epoch 312/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3035 - acc: 0.8623 - val_loss: 0.7352 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.90566\n",
      "Epoch 313/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3029 - acc: 0.8603 - val_loss: 0.7348 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.90566\n",
      "Epoch 314/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2825 - acc: 0.8676 - val_loss: 0.7351 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.90566\n",
      "Epoch 315/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2862 - acc: 0.8641 - val_loss: 0.7334 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.90566\n",
      "Epoch 316/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2810 - acc: 0.8688 - val_loss: 0.7332 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.90566\n",
      "Epoch 317/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2765 - acc: 0.8818 - val_loss: 0.7307 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.90566\n",
      "Epoch 318/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2974 - acc: 0.8650 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.90566\n",
      "Epoch 319/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3060 - acc: 0.8670 - val_loss: 0.7286 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.90566\n",
      "Epoch 320/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2887 - acc: 0.8694 - val_loss: 0.7300 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.90566\n",
      "Epoch 321/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3102 - acc: 0.8626 - val_loss: 0.7296 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.90566\n",
      "Epoch 322/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2848 - acc: 0.8723 - val_loss: 0.7296 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.90566\n",
      "Epoch 323/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2803 - acc: 0.8877 - val_loss: 0.7308 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.90566\n",
      "Epoch 324/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8611 - val_loss: 0.7310 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.90566\n",
      "Epoch 325/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2724 - acc: 0.8827 - val_loss: 0.7339 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.90566\n",
      "Epoch 326/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2894 - acc: 0.8614 - val_loss: 0.7320 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.90566\n",
      "Epoch 327/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2901 - acc: 0.8665 - val_loss: 0.7315 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.90566\n",
      "Epoch 328/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2705 - acc: 0.8685 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.90566\n",
      "Epoch 329/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3101 - acc: 0.8650 - val_loss: 0.7329 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.90566\n",
      "Epoch 330/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2944 - acc: 0.8656 - val_loss: 0.7311 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.90566\n",
      "Epoch 331/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3066 - acc: 0.8632 - val_loss: 0.7331 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.90566\n",
      "Epoch 332/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3074 - acc: 0.8679 - val_loss: 0.7307 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.90566\n",
      "Epoch 333/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2807 - acc: 0.8691 - val_loss: 0.7305 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.90566\n",
      "Epoch 334/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2905 - acc: 0.8670 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.90566\n",
      "Epoch 335/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2959 - acc: 0.8611 - val_loss: 0.7306 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.90566\n",
      "Epoch 336/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3113 - acc: 0.8703 - val_loss: 0.7310 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.90566\n",
      "Epoch 337/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3007 - acc: 0.8650 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.90566\n",
      "Epoch 338/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3096 - acc: 0.8676 - val_loss: 0.7323 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.90566\n",
      "Epoch 339/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3011 - acc: 0.8659 - val_loss: 0.7330 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.90566\n",
      "Epoch 340/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2683 - acc: 0.8768 - val_loss: 0.7321 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.90566\n",
      "Epoch 341/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3017 - acc: 0.8644 - val_loss: 0.7329 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.90566\n",
      "Epoch 342/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3037 - acc: 0.8700 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.90566\n",
      "Epoch 343/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2960 - acc: 0.8718 - val_loss: 0.7327 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.90566\n",
      "Epoch 344/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3031 - acc: 0.8697 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.90566\n",
      "Epoch 345/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2936 - acc: 0.8676 - val_loss: 0.7333 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.90566\n",
      "Epoch 346/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3184 - acc: 0.8718 - val_loss: 0.7336 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.90566\n",
      "Epoch 347/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2978 - acc: 0.8673 - val_loss: 0.7328 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.90566\n",
      "Epoch 348/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2815 - acc: 0.8659 - val_loss: 0.7332 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.90566\n",
      "Epoch 349/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2838 - acc: 0.8709 - val_loss: 0.7341 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.90566\n",
      "Epoch 350/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2789 - acc: 0.8709 - val_loss: 0.7323 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.90566\n",
      "Epoch 351/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2943 - acc: 0.8579 - val_loss: 0.7307 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.90566\n",
      "Epoch 352/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2935 - acc: 0.8735 - val_loss: 0.7304 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.90566\n",
      "Epoch 353/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2987 - acc: 0.8691 - val_loss: 0.7304 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.90566\n",
      "Epoch 354/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2936 - acc: 0.8617 - val_loss: 0.7296 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.90566\n",
      "Epoch 355/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2881 - acc: 0.8620 - val_loss: 0.7292 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.90566\n",
      "Epoch 356/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2885 - acc: 0.8729 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.90566\n",
      "Epoch 357/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3047 - acc: 0.8667 - val_loss: 0.7281 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.90566\n",
      "Epoch 358/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3074 - acc: 0.8718 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.90566\n",
      "Epoch 359/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2858 - acc: 0.8694 - val_loss: 0.7287 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.90566\n",
      "Epoch 360/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2780 - acc: 0.8809 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.90566\n",
      "Epoch 361/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2927 - acc: 0.8726 - val_loss: 0.7292 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.90566\n",
      "Epoch 362/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2903 - acc: 0.8550 - val_loss: 0.7285 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.90566\n",
      "Epoch 363/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2950 - acc: 0.8694 - val_loss: 0.7272 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.90566\n",
      "Epoch 364/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3112 - acc: 0.8632 - val_loss: 0.7288 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.90566\n",
      "Epoch 365/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2823 - acc: 0.8667 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.90566\n",
      "Epoch 366/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2889 - acc: 0.8800 - val_loss: 0.7287 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.90566\n",
      "Epoch 367/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2800 - acc: 0.8718 - val_loss: 0.7302 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.90566\n",
      "Epoch 368/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2811 - acc: 0.8697 - val_loss: 0.7299 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.90566\n",
      "Epoch 369/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2848 - acc: 0.8715 - val_loss: 0.7285 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.90566\n",
      "Epoch 370/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3105 - acc: 0.8694 - val_loss: 0.7277 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.90566\n",
      "Epoch 371/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2787 - acc: 0.8706 - val_loss: 0.7284 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.90566\n",
      "Epoch 372/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2796 - acc: 0.8611 - val_loss: 0.7282 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.90566\n",
      "Epoch 373/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2981 - acc: 0.8768 - val_loss: 0.7290 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.90566\n",
      "Epoch 374/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2831 - acc: 0.8662 - val_loss: 0.7298 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.90566\n",
      "Epoch 375/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3051 - acc: 0.8667 - val_loss: 0.7275 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.90566\n",
      "Epoch 376/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3078 - acc: 0.8626 - val_loss: 0.7267 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.90566\n",
      "Epoch 377/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3095 - acc: 0.8709 - val_loss: 0.7264 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.90566\n",
      "Epoch 378/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2841 - acc: 0.8620 - val_loss: 0.7275 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.90566\n",
      "Epoch 379/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2876 - acc: 0.8718 - val_loss: 0.7268 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.90566\n",
      "Epoch 380/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2879 - acc: 0.8706 - val_loss: 0.7268 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.90566\n",
      "Epoch 381/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2789 - acc: 0.8576 - val_loss: 0.7270 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.90566\n",
      "Epoch 382/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3002 - acc: 0.8591 - val_loss: 0.7262 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.90566\n",
      "Epoch 383/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8641 - val_loss: 0.7250 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.90566\n",
      "Epoch 384/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2964 - acc: 0.8670 - val_loss: 0.7244 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.90566\n",
      "Epoch 385/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2965 - acc: 0.8697 - val_loss: 0.7259 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.90566\n",
      "Epoch 386/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3037 - acc: 0.8614 - val_loss: 0.7281 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.90566\n",
      "Epoch 387/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2822 - acc: 0.8715 - val_loss: 0.7285 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.90566\n",
      "Epoch 388/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2908 - acc: 0.8653 - val_loss: 0.7284 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.90566\n",
      "Epoch 389/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8635 - val_loss: 0.7286 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.90566\n",
      "Epoch 390/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2907 - acc: 0.8656 - val_loss: 0.7291 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.90566\n",
      "Epoch 391/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2798 - acc: 0.8759 - val_loss: 0.7293 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.90566\n",
      "Epoch 392/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2641 - acc: 0.8632 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.90566\n",
      "Epoch 393/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3075 - acc: 0.8632 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.90566\n",
      "Epoch 394/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2863 - acc: 0.8650 - val_loss: 0.7299 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.90566\n",
      "Epoch 395/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8656 - val_loss: 0.7312 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.90566\n",
      "Epoch 396/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2876 - acc: 0.8665 - val_loss: 0.7308 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.90566\n",
      "Epoch 397/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2779 - acc: 0.8700 - val_loss: 0.7289 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.90566\n",
      "Epoch 398/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2639 - acc: 0.8747 - val_loss: 0.7298 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.90566\n",
      "Epoch 399/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2938 - acc: 0.8756 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.90566\n",
      "Epoch 400/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2798 - acc: 0.8721 - val_loss: 0.7308 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.90566\n",
      "Epoch 401/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2719 - acc: 0.8729 - val_loss: 0.7313 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.90566\n",
      "Epoch 402/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2951 - acc: 0.8608 - val_loss: 0.7306 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.90566\n",
      "Epoch 403/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3031 - acc: 0.8653 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.90566\n",
      "Epoch 404/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2746 - acc: 0.8620 - val_loss: 0.7314 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.90566\n",
      "Epoch 405/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2877 - acc: 0.8629 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.90566\n",
      "Epoch 406/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3069 - acc: 0.8620 - val_loss: 0.7293 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.90566\n",
      "Epoch 407/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3104 - acc: 0.8626 - val_loss: 0.7301 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.90566\n",
      "Epoch 408/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3225 - acc: 0.8611 - val_loss: 0.7294 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.90566\n",
      "Epoch 409/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2780 - acc: 0.8679 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.90566\n",
      "Epoch 410/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2934 - acc: 0.8803 - val_loss: 0.7319 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.90566\n",
      "Epoch 411/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2905 - acc: 0.8712 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.90566\n",
      "Epoch 412/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2886 - acc: 0.8676 - val_loss: 0.7301 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.90566\n",
      "Epoch 413/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2868 - acc: 0.8676 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.90566\n",
      "Epoch 414/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3083 - acc: 0.8665 - val_loss: 0.7299 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.90566\n",
      "Epoch 415/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2949 - acc: 0.8641 - val_loss: 0.7323 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.90566\n",
      "Epoch 416/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2853 - acc: 0.8765 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.90566\n",
      "Epoch 417/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2591 - acc: 0.8844 - val_loss: 0.7306 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.90566\n",
      "Epoch 418/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2802 - acc: 0.8641 - val_loss: 0.7304 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.90566\n",
      "Epoch 419/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.3014 - acc: 0.8667 - val_loss: 0.7308 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.90566\n",
      "Epoch 420/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3177 - acc: 0.8697 - val_loss: 0.7309 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.90566\n",
      "Epoch 421/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2792 - acc: 0.8809 - val_loss: 0.7323 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.90566\n",
      "Epoch 422/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2999 - acc: 0.8670 - val_loss: 0.7349 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.90566\n",
      "Epoch 423/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2990 - acc: 0.8659 - val_loss: 0.7351 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.90566\n",
      "Epoch 424/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2685 - acc: 0.8679 - val_loss: 0.7337 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.90566\n",
      "Epoch 425/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2906 - acc: 0.8638 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.90566\n",
      "Epoch 426/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2942 - acc: 0.8673 - val_loss: 0.7361 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.90566\n",
      "Epoch 427/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2679 - acc: 0.8756 - val_loss: 0.7338 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.90566\n",
      "Epoch 428/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3160 - acc: 0.8611 - val_loss: 0.7334 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.90566\n",
      "Epoch 429/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2784 - acc: 0.8676 - val_loss: 0.7335 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.90566\n",
      "Epoch 430/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2663 - acc: 0.8703 - val_loss: 0.7307 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.90566\n",
      "Epoch 431/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2778 - acc: 0.8712 - val_loss: 0.7317 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.90566\n",
      "Epoch 432/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2731 - acc: 0.8738 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.90566\n",
      "Epoch 433/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2710 - acc: 0.8800 - val_loss: 0.7349 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.90566\n",
      "Epoch 434/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2743 - acc: 0.8738 - val_loss: 0.7328 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.90566\n",
      "Epoch 435/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2856 - acc: 0.8697 - val_loss: 0.7310 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.90566\n",
      "Epoch 436/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2784 - acc: 0.8750 - val_loss: 0.7298 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.90566\n",
      "Epoch 437/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2678 - acc: 0.8762 - val_loss: 0.7284 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.90566\n",
      "Epoch 438/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2908 - acc: 0.8665 - val_loss: 0.7297 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.90566\n",
      "Epoch 439/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2822 - acc: 0.8694 - val_loss: 0.7305 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.90566\n",
      "Epoch 440/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2708 - acc: 0.8768 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.90566\n",
      "Epoch 441/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2832 - acc: 0.8726 - val_loss: 0.7316 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.90566\n",
      "Epoch 442/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2966 - acc: 0.8715 - val_loss: 0.7305 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.90566\n",
      "Epoch 443/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2741 - acc: 0.8726 - val_loss: 0.7311 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.90566\n",
      "Epoch 444/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2872 - acc: 0.8785 - val_loss: 0.7321 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.90566\n",
      "Epoch 445/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2809 - acc: 0.8750 - val_loss: 0.7306 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.90566\n",
      "Epoch 446/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2773 - acc: 0.8735 - val_loss: 0.7315 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.90566\n",
      "Epoch 447/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2816 - acc: 0.8718 - val_loss: 0.7311 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.90566\n",
      "Epoch 448/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2710 - acc: 0.8785 - val_loss: 0.7301 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.90566\n",
      "Epoch 449/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2754 - acc: 0.8771 - val_loss: 0.7302 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.90566\n",
      "Epoch 450/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2802 - acc: 0.8774 - val_loss: 0.7306 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.90566\n",
      "Epoch 451/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2724 - acc: 0.8673 - val_loss: 0.7311 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.90566\n",
      "Epoch 452/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2867 - acc: 0.8691 - val_loss: 0.7309 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.90566\n",
      "Epoch 453/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2950 - acc: 0.8673 - val_loss: 0.7314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.90566\n",
      "Epoch 454/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2832 - acc: 0.8688 - val_loss: 0.7293 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.90566\n",
      "Epoch 455/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2666 - acc: 0.8777 - val_loss: 0.7294 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.90566\n",
      "Epoch 456/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3088 - acc: 0.8706 - val_loss: 0.7291 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.90566\n",
      "Epoch 457/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2860 - acc: 0.8620 - val_loss: 0.7288 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.90566\n",
      "Epoch 458/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2736 - acc: 0.8729 - val_loss: 0.7307 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.90566\n",
      "Epoch 459/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3017 - acc: 0.8644 - val_loss: 0.7309 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.90566\n",
      "Epoch 460/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2890 - acc: 0.8591 - val_loss: 0.7319 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.90566\n",
      "Epoch 461/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2948 - acc: 0.8629 - val_loss: 0.7311 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.90566\n",
      "Epoch 462/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2774 - acc: 0.8744 - val_loss: 0.7312 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.90566\n",
      "Epoch 463/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2805 - acc: 0.8718 - val_loss: 0.7325 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.90566\n",
      "Epoch 464/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2912 - acc: 0.8632 - val_loss: 0.7333 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.90566\n",
      "Epoch 465/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2777 - acc: 0.8700 - val_loss: 0.7322 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.90566\n",
      "Epoch 466/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2838 - acc: 0.8723 - val_loss: 0.7297 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.90566\n",
      "Epoch 467/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2924 - acc: 0.8659 - val_loss: 0.7292 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.90566\n",
      "Epoch 468/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2965 - acc: 0.8667 - val_loss: 0.7312 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.90566\n",
      "Epoch 469/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2736 - acc: 0.8753 - val_loss: 0.7317 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.90566\n",
      "Epoch 470/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3012 - acc: 0.8659 - val_loss: 0.7307 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.90566\n",
      "Epoch 471/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2668 - acc: 0.8735 - val_loss: 0.7293 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.90566\n",
      "Epoch 472/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2830 - acc: 0.8694 - val_loss: 0.7290 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.90566\n",
      "Epoch 473/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.3091 - acc: 0.8567 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.90566\n",
      "Epoch 474/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2877 - acc: 0.8700 - val_loss: 0.7326 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.90566\n",
      "Epoch 475/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2764 - acc: 0.8785 - val_loss: 0.7327 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.90566\n",
      "Epoch 476/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2799 - acc: 0.8768 - val_loss: 0.7326 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.90566\n",
      "Epoch 477/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2657 - acc: 0.8753 - val_loss: 0.7325 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.90566\n",
      "Epoch 478/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2677 - acc: 0.8841 - val_loss: 0.7329 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.90566\n",
      "Epoch 479/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2864 - acc: 0.8697 - val_loss: 0.7305 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.90566\n",
      "Epoch 480/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2931 - acc: 0.8662 - val_loss: 0.7292 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.90566\n",
      "Epoch 481/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2865 - acc: 0.8718 - val_loss: 0.7294 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.90566\n",
      "Epoch 482/3000\n",
      "53/53 [==============================] - 6s 118ms/step - loss: 1.2758 - acc: 0.8656 - val_loss: 0.7300 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.90566\n",
      "Epoch 483/3000\n",
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2763 - acc: 0.8665 - val_loss: 0.7298 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.90566\n",
      "Epoch 484/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 6s 117ms/step - loss: 1.2837 - acc: 0.8638 - val_loss: 0.7295 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.90566\n",
      "Epoch 00484: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fold=0\n",
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "    print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "    model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(model,X_semi,Y_semi,fold,model_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC7\n",
    "#0=>0.84367(resnet3_semi)/0.83288(not semi)\n",
    "#1=>0.84367(resnet3_semi)/0.81132(not semi)\n",
    "#2=>0.84097(resnet2_semi)/0.81671(not semi)\n",
    "#3=>0.88410(resnet1_semi)/0.84367\n",
    "#4=>0.82210(resnet4_semi)/0.74663(not semi)\n",
    "#5 => 0.82749(resnet4_semi)/0.77628\n",
    "#6 => 0.85445(resnet2_semi)/0.78437\n",
    "#7 => 0.82749(resnet3_semi)/0.76280\n",
    "#8 => 0.83288(resnet3_semi)/0.78706\n",
    "# 9=> 0.90566(resnet1_semi)/0.87332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC6\n",
    "# 0=> 0.81941 (not semi)\n",
    "# 1=>0.83019 (semi)\n",
    "# 2=>0.81941 (semi)\n",
    "# 3=>0.85984 (resnet1_not semi)、0.78437\n",
    "# 4=>0.84367 (renet1_not semi)、0.81132\n",
    "# 5=> 0.85175(resnet3_semi)、0.82749(not semi)\n",
    "#6 => 0.85904(resnet4_semi)、0.79784(not semi)\n",
    "# 7=>0.88949(resnet2_semi)、0.83558(not semi)\n",
    "# 8=>0.83558(resnet1 not semi)\n",
    "# 9=>0.86792(resnet2_semi)、0.8112(not semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00044347.wav', '002d256b.wav', '003b91e8.wav', ...,\n",
       "       'fff37590.wav', 'fff44ac6.wav', 'fff6a13d.wav'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.load('feature/mfcc6/fname_unverified.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "# Co-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def normalize(X_train, X_val):\n",
    "    X_train = (X_train - mean)/(std)\n",
    "#     X_train = (X_train - min_)/range_\n",
    "    X_val = (X_val - mean)/(std)\n",
    "#     X_val = (X_val - min_)/range_\n",
    "\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet.ResnetBuilder.build_resnet_50((1, 64, 431), 41)\n",
    "# model.summary()\n",
    "def train_unverified(X_semi,Y_semi,fold):\n",
    "    name = glob.glob('model/'+feature_type+'/'+'**_fold'+str(fold)+'_res**')[0]\n",
    "    if fold in [0,1,2,3,4,5]: # MFCC6\n",
    "        name = glob.glob('model/'+feature_type+'/'+'**_fold'+str(fold)+'_co_**')[0]\n",
    "    print('semi loading: '+ name)\n",
    "    model = load_model(name)\n",
    "    model.summary()\n",
    "    if 'resnet4'in name:\n",
    "        batchSize=[32]\n",
    "    elif 'resnet2' in name or 'resnet3' in name:\n",
    "        batchSize=[32,64]#,128,256]\n",
    "    elif 'resnet1' in name:\n",
    "        batchSize=[32,64,128]\n",
    "    else:\n",
    "        batchSize=[32,64]\n",
    "#     batchSize=[32,64,128,256] ##ERR?\n",
    "    batchSize = random.choice(batchSize)\n",
    "    patien=30\n",
    "    epoch=3000\n",
    "    saveD = 'model/'+feature_type+'/'\n",
    "    opt = Adam(lr=0.0001,decay=1e-6)#Nadam() #Adam(lr=2e-3,decay=1e-20)\n",
    "    \n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "#         featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "#         featurewise_std_normalization=True,\n",
    "        width_shift_range=0.05+0.3*random.random(),\n",
    "        height_shift_range=0.05+0.3*random.random(),\n",
    "        shear_range=0.084375+0.253125*random.random(),\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(X_semi), v_h=np.max(X_semi)) # Trainset's boundaries.\n",
    "    )\n",
    "#     datagen.fit(X_semi)\n",
    "    test_datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True)\n",
    "    generator = MixupGenerator(X_semi, Y_semi, alpha=0.4+0.6*random.random(), \n",
    "                               batch_size=batchSize, datagen=datagen)\n",
    "    \n",
    "\n",
    "    model.compile(loss=['categorical_crossentropy'],optimizer=opt, metrics=['acc']) \n",
    "    logD = './logs/'+feature_type+'/'\n",
    "    history = History()\n",
    "    callback=[\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/4),min_lr=4e-6,\n",
    "                          mode='min', cooldown=1,verbose=1 ), #0.2,/25 #0.5/5/3e-6 #0.3/10/1e-6\n",
    "        EarlyStopping(patience=patien,monitor='val_loss',verbose=1,\n",
    "                      mode='min'),\n",
    "        ModelCheckpoint(saveD+'LGD_fold'+str(fold)+'_co_resnet'+'.h5',\n",
    "                        monitor='val_acc',verbose=1,save_best_only=True, \n",
    "                        save_weights_only=False,\n",
    "                        mode='max'),\n",
    "        TensorBoard(log_dir=logD+'LGD_fold'+str(fold)+'_co_resnet'),\n",
    "        history\n",
    "    ]\n",
    "    model.fit_generator(generator(),\n",
    "                        steps_per_epoch=2*X_semi.shape[0] // batchSize,\n",
    "                        shuffle=True,\n",
    "                        callbacks=callback, \n",
    "                        class_weight='auto',\n",
    "                        validation_data=(X_valid,Y_valid),\n",
    "                        max_queue_size = 32,\n",
    "                        workers = 11,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         batch_size=batchSize,\n",
    "                        epochs=epoch,\n",
    "#                         initial_epoch = int(patien/20)\n",
    "                       )\n",
    "#     model.fit(X_semi,Y_semi,\n",
    "#               shuffle=True,\n",
    "#               callbacks=callback, \n",
    "#               class_weight='auto',\n",
    "#               validation_data=(X_valid,Y_valid),\n",
    "#               batch_size=batchSize,\n",
    "#               epochs=epoch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semi_data(X_train,Y_train):\n",
    "    X = np.load('feature/'+feature_type+'/semi/'+who+'/X_co.npy')\n",
    "#     X_test_ver = np.load('feature/'+feature_type+'/semi/'+who+'/X_test_ver.npy')\n",
    "#     X = np.concatenate((X_un_ver,X_test_ver))\n",
    "    Y = np.load('feature/'+feature_type+'/semi/'+who+'/Y_co.npy')\n",
    "#     Y_test_ver = np.load('feature/'+feature_type+'/semi/'+who+'/Y_test_ver.npy')\n",
    "#     Y = np.concatenate((Y_un_ver,Y_test_ver))\n",
    "    Y = to_categorical(Y,num_classes=41)\n",
    "    X_semi = np.concatenate((X_train,X))\n",
    "    Y_semi = np.concatenate((Y_train,Y))\n",
    "    X_semi , Y_semi = _shuffle(X_semi,Y_semi)\n",
    "    print(X_semi.shape , Y_semi.shape)\n",
    "    return X_semi , Y_semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = 'mfcc6'\n",
    "who = 'jerry_mfcc3_resnet34_mixup_cotrain_Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'feature/'+feature_type+'/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/'+feature_type+'/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10\n",
    "\n",
    "val_set_num = [0,1,2,3,4,5]#str(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "(6360, 60, 259, 1) (6360, 41)\n",
      "===train semi_9===\n",
      "semi loading: model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 30, 130, 64)  3200        input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 30, 130, 64)  256         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 30, 130, 64)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 15, 65, 64)   0           activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 15, 65, 64)   36928       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 15, 65, 64)   256         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 15, 65, 64)   0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 15, 65, 64)   36928       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 15, 65, 64)   0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 15, 65, 64)   256         add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 15, 65, 64)   0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 15, 65, 64)   36928       activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 15, 65, 64)   256         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 15, 65, 64)   0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 15, 65, 64)   36928       activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 15, 65, 64)   0           add_228[0][0]                    \n",
      "                                                                 conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 15, 65, 64)   256         add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 15, 65, 64)   0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 15, 65, 64)   36928       activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 15, 65, 64)   256         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 15, 65, 64)   0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 15, 65, 64)   36928       activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 15, 65, 64)   0           add_229[0][0]                    \n",
      "                                                                 conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 15, 65, 64)   256         add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 15, 65, 64)   0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 8, 33, 128)   73856       activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 8, 33, 128)   512         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 8, 33, 128)   0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 8, 33, 128)   8320        add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 8, 33, 128)   147584      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 8, 33, 128)   0           conv2d_669[0][0]                 \n",
      "                                                                 conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 8, 33, 128)   512         add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 8, 33, 128)   0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 8, 33, 128)   147584      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 8, 33, 128)   512         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 8, 33, 128)   0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 8, 33, 128)   147584      activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 8, 33, 128)   0           add_231[0][0]                    \n",
      "                                                                 conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 8, 33, 128)   512         add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 8, 33, 128)   0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 8, 33, 128)   147584      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 8, 33, 128)   512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 8, 33, 128)   0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 8, 33, 128)   147584      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 8, 33, 128)   0           add_232[0][0]                    \n",
      "                                                                 conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 8, 33, 128)   512         add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 8, 33, 128)   0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 8, 33, 128)   147584      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 8, 33, 128)   512         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 8, 33, 128)   0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 8, 33, 128)   147584      activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 8, 33, 128)   0           add_233[0][0]                    \n",
      "                                                                 conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 8, 33, 128)   512         add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 8, 33, 128)   0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 4, 17, 256)   295168      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 4, 17, 256)   1024        conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 4, 17, 256)   0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 4, 17, 256)   33024       add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 4, 17, 256)   590080      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 4, 17, 256)   0           conv2d_678[0][0]                 \n",
      "                                                                 conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 4, 17, 256)   1024        add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 4, 17, 256)   0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 4, 17, 256)   590080      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 4, 17, 256)   1024        conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 4, 17, 256)   0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 4, 17, 256)   590080      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 4, 17, 256)   0           add_235[0][0]                    \n",
      "                                                                 conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 4, 17, 256)   1024        add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 4, 17, 256)   0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 4, 17, 256)   590080      activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 4, 17, 256)   1024        conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 4, 17, 256)   0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 4, 17, 256)   590080      activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 4, 17, 256)   0           add_236[0][0]                    \n",
      "                                                                 conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 4, 17, 256)   1024        add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 4, 17, 256)   0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 4, 17, 256)   590080      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 4, 17, 256)   1024        conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 4, 17, 256)   0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 4, 17, 256)   590080      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 4, 17, 256)   0           add_237[0][0]                    \n",
      "                                                                 conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 4, 17, 256)   1024        add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 4, 17, 256)   0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 4, 17, 256)   590080      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 4, 17, 256)   1024        conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 4, 17, 256)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 4, 17, 256)   590080      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 4, 17, 256)   0           add_238[0][0]                    \n",
      "                                                                 conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 4, 17, 256)   1024        add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 4, 17, 256)   0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 4, 17, 256)   590080      activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 4, 17, 256)   1024        conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 4, 17, 256)   0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 4, 17, 256)   590080      activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 4, 17, 256)   0           add_239[0][0]                    \n",
      "                                                                 conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 4, 17, 256)   1024        add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 4, 17, 256)   0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 2, 9, 512)    1180160     activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 2, 9, 512)    2048        conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 2, 9, 512)    0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 2, 9, 512)    131584      add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 2, 9, 512)    2359808     activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_241 (Add)                   (None, 2, 9, 512)    0           conv2d_691[0][0]                 \n",
      "                                                                 conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 2, 9, 512)    2048        add_241[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 2, 9, 512)    0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 2, 9, 512)    2359808     activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 2, 9, 512)    2048        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 2, 9, 512)    0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 2, 9, 512)    2359808     activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_242 (Add)                   (None, 2, 9, 512)    0           add_241[0][0]                    \n",
      "                                                                 conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 2, 9, 512)    2048        add_242[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 2, 9, 512)    0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 2, 9, 512)    2359808     activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 2, 9, 512)    2048        conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 2, 9, 512)    0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 2, 9, 512)    2359808     activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_243 (Add)                   (None, 2, 9, 512)    0           add_242[0][0]                    \n",
      "                                                                 conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 2, 9, 512)    2048        add_243[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 2, 9, 512)    0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 1, 1, 512)    0           activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 512)          0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 43)           22059       flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 43)           172         dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 43)           0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 41)           1804        dropout_28[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,324,387\n",
      "Trainable params: 21,309,069\n",
      "Non-trainable params: 15,318\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "198/198 [==============================] - 24s 121ms/step - loss: 1.4865 - acc: 0.8028 - val_loss: 0.6869 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88949, saving model to model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4630 - acc: 0.8086 - val_loss: 0.6689 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.88949 to 0.89218, saving model to model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "Epoch 3/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4465 - acc: 0.8163 - val_loss: 0.6682 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.89218 to 0.89757, saving model to model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4410 - acc: 0.8194 - val_loss: 0.6729 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.89757\n",
      "Epoch 5/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4474 - acc: 0.8070 - val_loss: 0.6625 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.89757 to 0.89757, saving model to model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "Epoch 6/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4345 - acc: 0.8164 - val_loss: 0.6767 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.89757 to 0.90566, saving model to model/mfcc7/LGD_fold9_co_resnet.h5\n",
      "Epoch 7/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4424 - acc: 0.8190 - val_loss: 0.6577 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.90566\n",
      "Epoch 8/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.4404 - acc: 0.8209 - val_loss: 0.6938 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.90566\n",
      "Epoch 9/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.4233 - acc: 0.8210 - val_loss: 0.6971 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.90566\n",
      "Epoch 10/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4291 - acc: 0.8224 - val_loss: 0.7340 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90566\n",
      "Epoch 11/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.4169 - acc: 0.8159 - val_loss: 0.7053 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90566\n",
      "Epoch 12/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4191 - acc: 0.8214 - val_loss: 0.7147 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90566\n",
      "Epoch 13/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.4116 - acc: 0.8251 - val_loss: 0.6773 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90566\n",
      "Epoch 14/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.4104 - acc: 0.8243 - val_loss: 0.7062 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90566\n",
      "Epoch 15/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.4085 - acc: 0.8258 - val_loss: 0.6779 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90566\n",
      "Epoch 16/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3966 - acc: 0.8211 - val_loss: 0.6906 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90566\n",
      "Epoch 17/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3897 - acc: 0.8265 - val_loss: 0.7130 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.90566\n",
      "Epoch 18/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3946 - acc: 0.8229 - val_loss: 0.6673 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.90566\n",
      "Epoch 19/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3976 - acc: 0.8207 - val_loss: 0.6962 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.90566\n",
      "Epoch 20/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3894 - acc: 0.8234 - val_loss: 0.7036 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90566\n",
      "Epoch 21/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3837 - acc: 0.8257 - val_loss: 0.6688 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90566\n",
      "Epoch 22/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3887 - acc: 0.8270 - val_loss: 0.7068 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90566\n",
      "Epoch 23/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3754 - acc: 0.8294 - val_loss: 0.6697 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90566\n",
      "Epoch 24/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3765 - acc: 0.8275 - val_loss: 0.6508 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90566\n",
      "Epoch 25/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3801 - acc: 0.8300 - val_loss: 0.6817 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90566\n",
      "Epoch 26/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3729 - acc: 0.8292 - val_loss: 0.6685 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90566\n",
      "Epoch 27/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3651 - acc: 0.8313 - val_loss: 0.6848 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90566\n",
      "Epoch 28/3000\n",
      "198/198 [==============================] - 19s 96ms/step - loss: 1.3548 - acc: 0.8388 - val_loss: 0.6767 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90566\n",
      "Epoch 29/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3603 - acc: 0.8353 - val_loss: 0.7090 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90566\n",
      "Epoch 30/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3593 - acc: 0.8325 - val_loss: 0.6950 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90566\n",
      "Epoch 31/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3594 - acc: 0.8295 - val_loss: 0.6779 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.90566\n",
      "Epoch 32/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3563 - acc: 0.8341 - val_loss: 0.7055 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90566\n",
      "Epoch 33/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3643 - acc: 0.8257 - val_loss: 0.6714 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90566\n",
      "Epoch 34/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3469 - acc: 0.8327 - val_loss: 0.6746 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90566\n",
      "Epoch 35/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3485 - acc: 0.8339 - val_loss: 0.6947 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.90566\n",
      "Epoch 36/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3656 - acc: 0.8337 - val_loss: 0.6819 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90566\n",
      "Epoch 37/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3513 - acc: 0.8325 - val_loss: 0.6691 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90566\n",
      "Epoch 38/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3523 - acc: 0.8338 - val_loss: 0.6601 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90566\n",
      "Epoch 39/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3331 - acc: 0.8355 - val_loss: 0.6536 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90566\n",
      "Epoch 40/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3506 - acc: 0.8342 - val_loss: 0.6441 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90566\n",
      "Epoch 41/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3286 - acc: 0.8353 - val_loss: 0.6838 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.90566\n",
      "Epoch 42/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3344 - acc: 0.8337 - val_loss: 0.6375 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.90566\n",
      "Epoch 43/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3451 - acc: 0.8337 - val_loss: 0.6874 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.90566\n",
      "Epoch 44/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3311 - acc: 0.8360 - val_loss: 0.6809 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.90566\n",
      "Epoch 45/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3375 - acc: 0.8378 - val_loss: 0.6383 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.90566\n",
      "Epoch 46/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3301 - acc: 0.8401 - val_loss: 0.6702 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90566\n",
      "Epoch 47/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3335 - acc: 0.8342 - val_loss: 0.6625 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90566\n",
      "Epoch 48/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3178 - acc: 0.8425 - val_loss: 0.6631 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90566\n",
      "Epoch 49/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3187 - acc: 0.8366 - val_loss: 0.6601 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90566\n",
      "Epoch 50/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3201 - acc: 0.8363 - val_loss: 0.6832 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90566\n",
      "Epoch 51/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3158 - acc: 0.8396 - val_loss: 0.6966 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90566\n",
      "Epoch 52/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3030 - acc: 0.8442 - val_loss: 0.6733 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90566\n",
      "Epoch 53/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3245 - acc: 0.8321 - val_loss: 0.6486 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90566\n",
      "Epoch 54/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3200 - acc: 0.8331 - val_loss: 0.6940 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90566\n",
      "Epoch 55/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3207 - acc: 0.8384 - val_loss: 0.6683 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90566\n",
      "Epoch 56/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2975 - acc: 0.8390 - val_loss: 0.6658 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90566\n",
      "Epoch 57/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3190 - acc: 0.8361 - val_loss: 0.6760 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90566\n",
      "Epoch 58/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3055 - acc: 0.8381 - val_loss: 0.6759 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90566\n",
      "Epoch 59/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3047 - acc: 0.8363 - val_loss: 0.6306 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90566\n",
      "Epoch 60/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3087 - acc: 0.8383 - val_loss: 0.6934 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90566\n",
      "Epoch 61/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2972 - acc: 0.8401 - val_loss: 0.6929 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90566\n",
      "Epoch 62/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3102 - acc: 0.8336 - val_loss: 0.6785 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90566\n",
      "Epoch 63/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.3056 - acc: 0.8463 - val_loss: 0.6765 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90566\n",
      "Epoch 64/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2927 - acc: 0.8407 - val_loss: 0.6777 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90566\n",
      "Epoch 65/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2888 - acc: 0.8436 - val_loss: 0.6739 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90566\n",
      "Epoch 66/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2937 - acc: 0.8370 - val_loss: 0.6862 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90566\n",
      "Epoch 67/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2942 - acc: 0.8458 - val_loss: 0.6912 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90566\n",
      "Epoch 68/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2933 - acc: 0.8382 - val_loss: 0.6602 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90566\n",
      "Epoch 69/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2888 - acc: 0.8421 - val_loss: 0.6314 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90566\n",
      "Epoch 70/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2960 - acc: 0.8361 - val_loss: 0.6756 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90566\n",
      "Epoch 71/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2906 - acc: 0.8438 - val_loss: 0.6958 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90566\n",
      "Epoch 72/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2823 - acc: 0.8408 - val_loss: 0.6583 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90566\n",
      "Epoch 73/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2927 - acc: 0.8393 - val_loss: 0.6629 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90566\n",
      "Epoch 74/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2833 - acc: 0.8437 - val_loss: 0.6476 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90566\n",
      "Epoch 75/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2894 - acc: 0.8445 - val_loss: 0.6462 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90566\n",
      "Epoch 76/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2830 - acc: 0.8380 - val_loss: 0.6537 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90566\n",
      "Epoch 77/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2722 - acc: 0.8438 - val_loss: 0.6866 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90566\n",
      "Epoch 78/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2733 - acc: 0.8480 - val_loss: 0.6757 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90566\n",
      "Epoch 79/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2809 - acc: 0.8432 - val_loss: 0.6851 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90566\n",
      "Epoch 80/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2724 - acc: 0.8444 - val_loss: 0.6722 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90566\n",
      "Epoch 81/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2708 - acc: 0.8407 - val_loss: 0.6645 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90566\n",
      "Epoch 82/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2730 - acc: 0.8414 - val_loss: 0.6699 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90566\n",
      "Epoch 83/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2722 - acc: 0.8464 - val_loss: 0.6708 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90566\n",
      "Epoch 84/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2757 - acc: 0.8457 - val_loss: 0.6727 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90566\n",
      "Epoch 85/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2755 - acc: 0.8438 - val_loss: 0.6281 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90566\n",
      "Epoch 86/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2608 - acc: 0.8432 - val_loss: 0.6643 - val_acc: 0.8787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90566\n",
      "Epoch 87/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2578 - acc: 0.8461 - val_loss: 0.6619 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90566\n",
      "Epoch 88/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2562 - acc: 0.8461 - val_loss: 0.6546 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90566\n",
      "Epoch 89/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2591 - acc: 0.8446 - val_loss: 0.7028 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90566\n",
      "Epoch 90/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2661 - acc: 0.8484 - val_loss: 0.6662 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90566\n",
      "Epoch 91/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2506 - acc: 0.8494 - val_loss: 0.6743 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90566\n",
      "Epoch 92/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2602 - acc: 0.8412 - val_loss: 0.6455 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90566\n",
      "Epoch 93/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2609 - acc: 0.8434 - val_loss: 0.6828 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90566\n",
      "Epoch 94/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2634 - acc: 0.8435 - val_loss: 0.6320 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90566\n",
      "Epoch 95/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2575 - acc: 0.8470 - val_loss: 0.6754 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90566\n",
      "Epoch 96/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2532 - acc: 0.8493 - val_loss: 0.6741 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90566\n",
      "Epoch 97/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2443 - acc: 0.8509 - val_loss: 0.6818 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90566\n",
      "Epoch 98/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2611 - acc: 0.8448 - val_loss: 0.6608 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90566\n",
      "Epoch 99/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2504 - acc: 0.8474 - val_loss: 0.6660 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90566\n",
      "Epoch 100/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2583 - acc: 0.8483 - val_loss: 0.6377 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90566\n",
      "Epoch 101/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2500 - acc: 0.8480 - val_loss: 0.6644 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90566\n",
      "Epoch 102/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2462 - acc: 0.8430 - val_loss: 0.6728 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90566\n",
      "Epoch 103/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2478 - acc: 0.8468 - val_loss: 0.6430 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90566\n",
      "Epoch 104/3000\n",
      "198/198 [==============================] - 19s 97ms/step - loss: 1.2496 - acc: 0.8396 - val_loss: 0.6357 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90566\n",
      "Epoch 105/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2418 - acc: 0.8505 - val_loss: 0.6586 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90566\n",
      "Epoch 106/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2357 - acc: 0.8429 - val_loss: 0.6500 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90566\n",
      "Epoch 107/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2253 - acc: 0.8534 - val_loss: 0.6573 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90566\n",
      "Epoch 108/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2282 - acc: 0.8470 - val_loss: 0.6446 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90566\n",
      "Epoch 109/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2134 - acc: 0.8566 - val_loss: 0.6525 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90566\n",
      "Epoch 110/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2163 - acc: 0.8526 - val_loss: 0.6439 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90566\n",
      "Epoch 111/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2302 - acc: 0.8518 - val_loss: 0.6491 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90566\n",
      "Epoch 112/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2174 - acc: 0.8556 - val_loss: 0.6370 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90566\n",
      "Epoch 113/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2102 - acc: 0.8562 - val_loss: 0.6387 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90566\n",
      "Epoch 114/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2265 - acc: 0.8523 - val_loss: 0.6461 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90566\n",
      "Epoch 115/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2164 - acc: 0.8584 - val_loss: 0.6266 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90566\n",
      "Epoch 116/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2157 - acc: 0.8545 - val_loss: 0.6588 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90566\n",
      "Epoch 117/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2085 - acc: 0.8480 - val_loss: 0.6427 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90566\n",
      "Epoch 118/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2107 - acc: 0.8569 - val_loss: 0.6559 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90566\n",
      "Epoch 119/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2166 - acc: 0.8524 - val_loss: 0.6511 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90566\n",
      "Epoch 120/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2139 - acc: 0.8548 - val_loss: 0.6351 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90566\n",
      "Epoch 121/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1994 - acc: 0.8607 - val_loss: 0.6260 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90566\n",
      "Epoch 122/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2075 - acc: 0.8495 - val_loss: 0.6417 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90566\n",
      "Epoch 123/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2174 - acc: 0.8509 - val_loss: 0.6217 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90566\n",
      "Epoch 124/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2011 - acc: 0.8602 - val_loss: 0.6136 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90566\n",
      "Epoch 125/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2075 - acc: 0.8494 - val_loss: 0.6353 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90566\n",
      "Epoch 126/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2060 - acc: 0.8540 - val_loss: 0.6348 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90566\n",
      "Epoch 127/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1973 - acc: 0.8524 - val_loss: 0.6320 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90566\n",
      "Epoch 128/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2015 - acc: 0.8567 - val_loss: 0.6234 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90566\n",
      "Epoch 129/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2056 - acc: 0.8527 - val_loss: 0.6551 - val_acc: 0.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90566\n",
      "Epoch 130/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1998 - acc: 0.8551 - val_loss: 0.6455 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90566\n",
      "Epoch 131/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1974 - acc: 0.8512 - val_loss: 0.6298 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90566\n",
      "Epoch 132/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2047 - acc: 0.8550 - val_loss: 0.6392 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90566\n",
      "Epoch 133/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.2063 - acc: 0.8595 - val_loss: 0.6277 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90566\n",
      "Epoch 134/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1990 - acc: 0.8573 - val_loss: 0.6507 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90566\n",
      "Epoch 135/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1919 - acc: 0.8552 - val_loss: 0.6271 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90566\n",
      "Epoch 136/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1848 - acc: 0.8582 - val_loss: 0.6219 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90566\n",
      "Epoch 137/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1941 - acc: 0.8531 - val_loss: 0.6183 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90566\n",
      "Epoch 138/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1869 - acc: 0.8519 - val_loss: 0.6248 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90566\n",
      "Epoch 139/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1850 - acc: 0.8580 - val_loss: 0.6417 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90566\n",
      "Epoch 140/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1895 - acc: 0.8541 - val_loss: 0.6306 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90566\n",
      "Epoch 141/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1886 - acc: 0.8536 - val_loss: 0.6348 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90566\n",
      "Epoch 142/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1831 - acc: 0.8572 - val_loss: 0.6291 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90566\n",
      "Epoch 143/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1909 - acc: 0.8598 - val_loss: 0.6292 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90566\n",
      "Epoch 144/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1858 - acc: 0.8583 - val_loss: 0.6185 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90566\n",
      "Epoch 145/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1866 - acc: 0.8550 - val_loss: 0.6275 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90566\n",
      "Epoch 146/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1864 - acc: 0.8561 - val_loss: 0.6218 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90566\n",
      "Epoch 147/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1845 - acc: 0.8578 - val_loss: 0.6097 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90566\n",
      "Epoch 148/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1894 - acc: 0.8561 - val_loss: 0.6202 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90566\n",
      "Epoch 149/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1827 - acc: 0.8565 - val_loss: 0.6248 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90566\n",
      "Epoch 150/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1798 - acc: 0.8641 - val_loss: 0.6259 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90566\n",
      "Epoch 151/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1908 - acc: 0.8584 - val_loss: 0.6229 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.90566\n",
      "Epoch 152/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1760 - acc: 0.8655 - val_loss: 0.6170 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.90566\n",
      "Epoch 153/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1860 - acc: 0.8572 - val_loss: 0.6289 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.90566\n",
      "Epoch 154/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1754 - acc: 0.8572 - val_loss: 0.6221 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.90566\n",
      "Epoch 155/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1729 - acc: 0.8588 - val_loss: 0.6124 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.90566\n",
      "Epoch 156/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1810 - acc: 0.8558 - val_loss: 0.6263 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.90566\n",
      "Epoch 157/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1746 - acc: 0.8587 - val_loss: 0.6243 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.90566\n",
      "Epoch 158/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1821 - acc: 0.8512 - val_loss: 0.6109 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.90566\n",
      "Epoch 159/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1738 - acc: 0.8559 - val_loss: 0.6128 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.90566\n",
      "Epoch 160/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1826 - acc: 0.8556 - val_loss: 0.6182 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.90566\n",
      "Epoch 161/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1784 - acc: 0.8572 - val_loss: 0.5992 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.90566\n",
      "Epoch 162/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1826 - acc: 0.8611 - val_loss: 0.6147 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.90566\n",
      "Epoch 163/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1788 - acc: 0.8550 - val_loss: 0.6230 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.90566\n",
      "Epoch 164/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1726 - acc: 0.8550 - val_loss: 0.6186 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.90566\n",
      "Epoch 165/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1735 - acc: 0.8640 - val_loss: 0.6164 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.90566\n",
      "Epoch 166/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1638 - acc: 0.8630 - val_loss: 0.6183 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.90566\n",
      "Epoch 167/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1696 - acc: 0.8580 - val_loss: 0.6126 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.90566\n",
      "Epoch 168/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1738 - acc: 0.8607 - val_loss: 0.6201 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.90566\n",
      "Epoch 169/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1793 - acc: 0.8551 - val_loss: 0.6233 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.90566\n",
      "Epoch 170/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1719 - acc: 0.8604 - val_loss: 0.6232 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.90566\n",
      "Epoch 171/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1721 - acc: 0.8598 - val_loss: 0.6222 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.90566\n",
      "Epoch 172/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1739 - acc: 0.8557 - val_loss: 0.6207 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.90566\n",
      "Epoch 173/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1616 - acc: 0.8631 - val_loss: 0.6250 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.90566\n",
      "Epoch 174/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1626 - acc: 0.8622 - val_loss: 0.6207 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.90566\n",
      "Epoch 175/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1655 - acc: 0.8602 - val_loss: 0.6230 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.90566\n",
      "Epoch 176/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1603 - acc: 0.8591 - val_loss: 0.6192 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.90566\n",
      "Epoch 177/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1643 - acc: 0.8598 - val_loss: 0.6214 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.90566\n",
      "Epoch 178/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1758 - acc: 0.8603 - val_loss: 0.6194 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.90566\n",
      "Epoch 179/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1737 - acc: 0.8598 - val_loss: 0.6188 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.90566\n",
      "Epoch 180/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1654 - acc: 0.8681 - val_loss: 0.6206 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.90566\n",
      "Epoch 181/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1657 - acc: 0.8625 - val_loss: 0.6258 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.90566\n",
      "Epoch 182/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1549 - acc: 0.8633 - val_loss: 0.6282 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.90566\n",
      "Epoch 183/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1596 - acc: 0.8616 - val_loss: 0.6336 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.90566\n",
      "Epoch 184/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1637 - acc: 0.8597 - val_loss: 0.6283 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.90566\n",
      "Epoch 185/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1635 - acc: 0.8613 - val_loss: 0.6269 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.90566\n",
      "Epoch 186/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1680 - acc: 0.8558 - val_loss: 0.6222 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.90566\n",
      "Epoch 187/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1635 - acc: 0.8638 - val_loss: 0.6326 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.90566\n",
      "Epoch 188/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1735 - acc: 0.8580 - val_loss: 0.6287 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.90566\n",
      "Epoch 189/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1570 - acc: 0.8614 - val_loss: 0.6229 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.90566\n",
      "Epoch 190/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1626 - acc: 0.8632 - val_loss: 0.6226 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.90566\n",
      "Epoch 191/3000\n",
      "198/198 [==============================] - 19s 98ms/step - loss: 1.1622 - acc: 0.8605 - val_loss: 0.6202 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.90566\n",
      "Epoch 00191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 60, 259, 1) (3339, 41)\n",
      "(6360, 60, 259, 1) (6360, 41)\n",
      "===train semi_8===\n",
      "semi loading: model/mfcc7/LGD_semi_fold8_resnet3.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 30, 130, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_509 (BatchN (None, 30, 130, 64)  256         conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 30, 130, 64)  0           batch_normalization_509[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 15, 65, 64)   0           activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_510 (BatchN (None, 15, 65, 64)   256         conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 15, 65, 64)   0           batch_normalization_510[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 15, 65, 64)   36928       activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_511 (BatchN (None, 15, 65, 64)   256         conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 15, 65, 64)   0           batch_normalization_511[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 15, 65, 256)  16640       activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 15, 65, 256)  0           conv2d_540[0][0]                 \n",
      "                                                                 conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_512 (BatchN (None, 15, 65, 256)  1024        add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 15, 65, 256)  0           batch_normalization_512[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 15, 65, 64)   16448       activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_513 (BatchN (None, 15, 65, 64)   256         conv2d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 15, 65, 64)   0           batch_normalization_513[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 15, 65, 64)   36928       activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_514 (BatchN (None, 15, 65, 64)   256         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 15, 65, 64)   0           batch_normalization_514[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 15, 65, 256)  16640       activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 15, 65, 256)  0           add_187[0][0]                    \n",
      "                                                                 conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_515 (BatchN (None, 15, 65, 256)  1024        add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 15, 65, 256)  0           batch_normalization_515[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 15, 65, 64)   16448       activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_516 (BatchN (None, 15, 65, 64)   256         conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 15, 65, 64)   0           batch_normalization_516[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 15, 65, 64)   36928       activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_517 (BatchN (None, 15, 65, 64)   256         conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 15, 65, 64)   0           batch_normalization_517[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 15, 65, 256)  16640       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 15, 65, 256)  0           add_188[0][0]                    \n",
      "                                                                 conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_518 (BatchN (None, 15, 65, 256)  1024        add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 15, 65, 256)  0           batch_normalization_518[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 8, 33, 128)   32896       activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_519 (BatchN (None, 8, 33, 128)   512         conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 8, 33, 128)   0           batch_normalization_519[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 8, 33, 128)   147584      activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_520 (BatchN (None, 8, 33, 128)   512         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 8, 33, 128)   0           batch_normalization_520[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 8, 33, 512)   131584      add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 8, 33, 512)   66048       activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 8, 33, 512)   0           conv2d_550[0][0]                 \n",
      "                                                                 conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_521 (BatchN (None, 8, 33, 512)   2048        add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 8, 33, 512)   0           batch_normalization_521[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_551 (Conv2D)             (None, 8, 33, 128)   65664       activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_522 (BatchN (None, 8, 33, 128)   512         conv2d_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 8, 33, 128)   0           batch_normalization_522[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 8, 33, 128)   147584      activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_523 (BatchN (None, 8, 33, 128)   512         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 8, 33, 128)   0           batch_normalization_523[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 8, 33, 512)   66048       activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 8, 33, 512)   0           add_190[0][0]                    \n",
      "                                                                 conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_524 (BatchN (None, 8, 33, 512)   2048        add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 8, 33, 512)   0           batch_normalization_524[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_554 (Conv2D)             (None, 8, 33, 128)   65664       activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_525 (BatchN (None, 8, 33, 128)   512         conv2d_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 8, 33, 128)   0           batch_normalization_525[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 8, 33, 128)   147584      activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_526 (BatchN (None, 8, 33, 128)   512         conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 8, 33, 128)   0           batch_normalization_526[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 8, 33, 512)   66048       activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 8, 33, 512)   0           add_191[0][0]                    \n",
      "                                                                 conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_527 (BatchN (None, 8, 33, 512)   2048        add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 8, 33, 512)   0           batch_normalization_527[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 8, 33, 128)   65664       activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_528 (BatchN (None, 8, 33, 128)   512         conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 8, 33, 128)   0           batch_normalization_528[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_558 (Conv2D)             (None, 8, 33, 128)   147584      activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_529 (BatchN (None, 8, 33, 128)   512         conv2d_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 8, 33, 128)   0           batch_normalization_529[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 8, 33, 512)   66048       activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 8, 33, 512)   0           add_192[0][0]                    \n",
      "                                                                 conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 8, 33, 512)   2048        add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 8, 33, 512)   0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 4, 17, 256)   131328      activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_531 (BatchN (None, 4, 17, 256)   1024        conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 4, 17, 256)   0           batch_normalization_531[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_561 (Conv2D)             (None, 4, 17, 256)   590080      activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 4, 17, 256)   1024        conv2d_561[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 4, 17, 256)   0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 4, 17, 1024)  525312      add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 4, 17, 1024)  263168      activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 4, 17, 1024)  0           conv2d_563[0][0]                 \n",
      "                                                                 conv2d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 4, 17, 1024)  4096        add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_533[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_564 (Conv2D)             (None, 4, 17, 256)   262400      activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 4, 17, 256)   1024        conv2d_564[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 4, 17, 256)   0           batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 4, 17, 256)   590080      activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_535 (BatchN (None, 4, 17, 256)   1024        conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 4, 17, 256)   0           batch_normalization_535[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 4, 17, 1024)  263168      activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 4, 17, 1024)  0           add_194[0][0]                    \n",
      "                                                                 conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 4, 17, 1024)  4096        add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_567 (Conv2D)             (None, 4, 17, 256)   262400      activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 4, 17, 256)   1024        conv2d_567[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 4, 17, 256)   0           batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 4, 17, 256)   590080      activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_538 (BatchN (None, 4, 17, 256)   1024        conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 4, 17, 256)   0           batch_normalization_538[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 4, 17, 1024)  263168      activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 4, 17, 1024)  0           add_195[0][0]                    \n",
      "                                                                 conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 4, 17, 1024)  4096        add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_570 (Conv2D)             (None, 4, 17, 256)   262400      activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 4, 17, 256)   1024        conv2d_570[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 4, 17, 256)   0           batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 4, 17, 256)   590080      activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_541 (BatchN (None, 4, 17, 256)   1024        conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 4, 17, 256)   0           batch_normalization_541[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 4, 17, 1024)  263168      activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 4, 17, 1024)  0           add_196[0][0]                    \n",
      "                                                                 conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 4, 17, 1024)  4096        add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_573 (Conv2D)             (None, 4, 17, 256)   262400      activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 4, 17, 256)   1024        conv2d_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 4, 17, 256)   0           batch_normalization_543[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 4, 17, 256)   590080      activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 4, 17, 256)   1024        conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 4, 17, 256)   0           batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 4, 17, 1024)  263168      activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 4, 17, 1024)  0           add_197[0][0]                    \n",
      "                                                                 conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_545 (BatchN (None, 4, 17, 1024)  4096        add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_545[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 4, 17, 256)   262400      activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 4, 17, 256)   1024        conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 4, 17, 256)   0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_577 (Conv2D)             (None, 4, 17, 256)   590080      activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 4, 17, 256)   1024        conv2d_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 4, 17, 256)   0           batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 4, 17, 1024)  263168      activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 4, 17, 1024)  0           add_198[0][0]                    \n",
      "                                                                 conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_548 (BatchN (None, 4, 17, 1024)  4096        add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_548[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 4, 17, 256)   262400      activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 4, 17, 256)   1024        conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 4, 17, 256)   0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_580 (Conv2D)             (None, 4, 17, 256)   590080      activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 4, 17, 256)   1024        conv2d_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 4, 17, 256)   0           batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 4, 17, 1024)  263168      activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 4, 17, 1024)  0           add_199[0][0]                    \n",
      "                                                                 conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_551 (BatchN (None, 4, 17, 1024)  4096        add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_551[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 4, 17, 256)   262400      activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 4, 17, 256)   1024        conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 4, 17, 256)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_583 (Conv2D)             (None, 4, 17, 256)   590080      activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 4, 17, 256)   1024        conv2d_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 4, 17, 256)   0           batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_584 (Conv2D)             (None, 4, 17, 1024)  263168      activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 4, 17, 1024)  0           add_200[0][0]                    \n",
      "                                                                 conv2d_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_554 (BatchN (None, 4, 17, 1024)  4096        add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_554[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_585 (Conv2D)             (None, 4, 17, 256)   262400      activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 4, 17, 256)   1024        conv2d_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 4, 17, 256)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_586 (Conv2D)             (None, 4, 17, 256)   590080      activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 4, 17, 256)   1024        conv2d_586[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 4, 17, 256)   0           batch_normalization_556[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_587 (Conv2D)             (None, 4, 17, 1024)  263168      activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 4, 17, 1024)  0           add_201[0][0]                    \n",
      "                                                                 conv2d_587[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 4, 17, 1024)  4096        add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_588 (Conv2D)             (None, 4, 17, 256)   262400      activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_558 (BatchN (None, 4, 17, 256)   1024        conv2d_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 4, 17, 256)   0           batch_normalization_558[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_589 (Conv2D)             (None, 4, 17, 256)   590080      activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 4, 17, 256)   1024        conv2d_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 4, 17, 256)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_590 (Conv2D)             (None, 4, 17, 1024)  263168      activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 4, 17, 1024)  0           add_202[0][0]                    \n",
      "                                                                 conv2d_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 4, 17, 1024)  4096        add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_591 (Conv2D)             (None, 4, 17, 256)   262400      activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_561 (BatchN (None, 4, 17, 256)   1024        conv2d_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 4, 17, 256)   0           batch_normalization_561[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_592 (Conv2D)             (None, 4, 17, 256)   590080      activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 4, 17, 256)   1024        conv2d_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 4, 17, 256)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_593 (Conv2D)             (None, 4, 17, 1024)  263168      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 4, 17, 1024)  0           add_203[0][0]                    \n",
      "                                                                 conv2d_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 4, 17, 1024)  4096        add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_594 (Conv2D)             (None, 4, 17, 256)   262400      activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_564 (BatchN (None, 4, 17, 256)   1024        conv2d_594[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 4, 17, 256)   0           batch_normalization_564[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_595 (Conv2D)             (None, 4, 17, 256)   590080      activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 4, 17, 256)   1024        conv2d_595[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 4, 17, 256)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_596 (Conv2D)             (None, 4, 17, 1024)  263168      activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 4, 17, 1024)  0           add_204[0][0]                    \n",
      "                                                                 conv2d_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 4, 17, 1024)  4096        add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_597 (Conv2D)             (None, 4, 17, 256)   262400      activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_567 (BatchN (None, 4, 17, 256)   1024        conv2d_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 4, 17, 256)   0           batch_normalization_567[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_598 (Conv2D)             (None, 4, 17, 256)   590080      activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 4, 17, 256)   1024        conv2d_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 4, 17, 256)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_599 (Conv2D)             (None, 4, 17, 1024)  263168      activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 4, 17, 1024)  0           add_205[0][0]                    \n",
      "                                                                 conv2d_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 4, 17, 1024)  4096        add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_600 (Conv2D)             (None, 4, 17, 256)   262400      activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_570 (BatchN (None, 4, 17, 256)   1024        conv2d_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 4, 17, 256)   0           batch_normalization_570[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_601 (Conv2D)             (None, 4, 17, 256)   590080      activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 4, 17, 256)   1024        conv2d_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 4, 17, 256)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_602 (Conv2D)             (None, 4, 17, 1024)  263168      activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 4, 17, 1024)  0           add_206[0][0]                    \n",
      "                                                                 conv2d_602[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 4, 17, 1024)  4096        add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_603 (Conv2D)             (None, 4, 17, 256)   262400      activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_573 (BatchN (None, 4, 17, 256)   1024        conv2d_603[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 4, 17, 256)   0           batch_normalization_573[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_604 (Conv2D)             (None, 4, 17, 256)   590080      activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 4, 17, 256)   1024        conv2d_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 4, 17, 256)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_605 (Conv2D)             (None, 4, 17, 1024)  263168      activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_208 (Add)                   (None, 4, 17, 1024)  0           add_207[0][0]                    \n",
      "                                                                 conv2d_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 4, 17, 1024)  4096        add_208[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_575[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_606 (Conv2D)             (None, 4, 17, 256)   262400      activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 4, 17, 256)   1024        conv2d_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 4, 17, 256)   0           batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_607 (Conv2D)             (None, 4, 17, 256)   590080      activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_577 (BatchN (None, 4, 17, 256)   1024        conv2d_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 4, 17, 256)   0           batch_normalization_577[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_608 (Conv2D)             (None, 4, 17, 1024)  263168      activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_209 (Add)                   (None, 4, 17, 1024)  0           add_208[0][0]                    \n",
      "                                                                 conv2d_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 4, 17, 1024)  4096        add_209[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_609 (Conv2D)             (None, 4, 17, 256)   262400      activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 4, 17, 256)   1024        conv2d_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 4, 17, 256)   0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_610 (Conv2D)             (None, 4, 17, 256)   590080      activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 4, 17, 256)   1024        conv2d_610[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 4, 17, 256)   0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_611 (Conv2D)             (None, 4, 17, 1024)  263168      activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_210 (Add)                   (None, 4, 17, 1024)  0           add_209[0][0]                    \n",
      "                                                                 conv2d_611[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 4, 17, 1024)  4096        add_210[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 4, 17, 256)   262400      activation_570[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 4, 17, 256)   1024        conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 4, 17, 256)   0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 4, 17, 256)   590080      activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 4, 17, 256)   1024        conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 4, 17, 256)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 4, 17, 1024)  263168      activation_572[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_211 (Add)                   (None, 4, 17, 1024)  0           add_210[0][0]                    \n",
      "                                                                 conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 4, 17, 1024)  4096        add_211[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_615 (Conv2D)             (None, 4, 17, 256)   262400      activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 4, 17, 256)   1024        conv2d_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 4, 17, 256)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_616 (Conv2D)             (None, 4, 17, 256)   590080      activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 4, 17, 256)   1024        conv2d_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 4, 17, 256)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_617 (Conv2D)             (None, 4, 17, 1024)  263168      activation_575[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_212 (Add)                   (None, 4, 17, 1024)  0           add_211[0][0]                    \n",
      "                                                                 conv2d_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 4, 17, 1024)  4096        add_212[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_618 (Conv2D)             (None, 4, 17, 256)   262400      activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 4, 17, 256)   1024        conv2d_618[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 4, 17, 256)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_619 (Conv2D)             (None, 4, 17, 256)   590080      activation_577[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 4, 17, 256)   1024        conv2d_619[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 4, 17, 256)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_620 (Conv2D)             (None, 4, 17, 1024)  263168      activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_213 (Add)                   (None, 4, 17, 1024)  0           add_212[0][0]                    \n",
      "                                                                 conv2d_620[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 4, 17, 1024)  4096        add_213[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_621 (Conv2D)             (None, 4, 17, 256)   262400      activation_579[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 4, 17, 256)   1024        conv2d_621[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 4, 17, 256)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_622 (Conv2D)             (None, 4, 17, 256)   590080      activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 4, 17, 256)   1024        conv2d_622[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 4, 17, 256)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_623 (Conv2D)             (None, 4, 17, 1024)  263168      activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_214 (Add)                   (None, 4, 17, 1024)  0           add_213[0][0]                    \n",
      "                                                                 conv2d_623[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 4, 17, 1024)  4096        add_214[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_624 (Conv2D)             (None, 4, 17, 256)   262400      activation_582[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 4, 17, 256)   1024        conv2d_624[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 4, 17, 256)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_625 (Conv2D)             (None, 4, 17, 256)   590080      activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 4, 17, 256)   1024        conv2d_625[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 4, 17, 256)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_626 (Conv2D)             (None, 4, 17, 1024)  263168      activation_584[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_215 (Add)                   (None, 4, 17, 1024)  0           add_214[0][0]                    \n",
      "                                                                 conv2d_626[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 4, 17, 1024)  4096        add_215[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_627 (Conv2D)             (None, 4, 17, 256)   262400      activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 4, 17, 256)   1024        conv2d_627[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 4, 17, 256)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_628 (Conv2D)             (None, 4, 17, 256)   590080      activation_586[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 4, 17, 256)   1024        conv2d_628[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 4, 17, 256)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_629 (Conv2D)             (None, 4, 17, 1024)  263168      activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_216 (Add)                   (None, 4, 17, 1024)  0           add_215[0][0]                    \n",
      "                                                                 conv2d_629[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 4, 17, 1024)  4096        add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_630 (Conv2D)             (None, 2, 9, 512)    524800      activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 2, 9, 512)    2048        conv2d_630[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 2, 9, 512)    0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_631 (Conv2D)             (None, 2, 9, 512)    2359808     activation_589[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 2, 9, 512)    2048        conv2d_631[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 2, 9, 512)    0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_633 (Conv2D)             (None, 2, 9, 2048)   2099200     add_216[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_632 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_217 (Add)                   (None, 2, 9, 2048)   0           conv2d_633[0][0]                 \n",
      "                                                                 conv2d_632[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 2, 9, 2048)   8192        add_217[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_634 (Conv2D)             (None, 2, 9, 512)    1049088     activation_591[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 2, 9, 512)    2048        conv2d_634[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 2, 9, 512)    0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_635 (Conv2D)             (None, 2, 9, 512)    2359808     activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 2, 9, 512)    2048        conv2d_635[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 2, 9, 512)    0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_636 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_218 (Add)                   (None, 2, 9, 2048)   0           add_217[0][0]                    \n",
      "                                                                 conv2d_636[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 2, 9, 2048)   8192        add_218[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_637 (Conv2D)             (None, 2, 9, 512)    1049088     activation_594[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 2, 9, 512)    2048        conv2d_637[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 2, 9, 512)    0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_638 (Conv2D)             (None, 2, 9, 512)    2359808     activation_595[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 2, 9, 512)    2048        conv2d_638[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 2, 9, 512)    0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_639 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_219 (Add)                   (None, 2, 9, 2048)   0           add_218[0][0]                    \n",
      "                                                                 conv2d_639[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 2, 9, 2048)   8192        add_219[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 2048)   0           activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 53)           108597      flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 53)           212         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 53)           0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           2214        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,747,567\n",
      "Trainable params: 42,649,797\n",
      "Non-trainable params: 97,770\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "198/198 [==============================] - 66s 332ms/step - loss: 2.2143 - acc: 0.5950 - val_loss: 0.9784 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80593, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "198/198 [==============================] - 53s 268ms/step - loss: 1.9740 - acc: 0.6676 - val_loss: 0.9554 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.80593 to 0.80863, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 3/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.9026 - acc: 0.6884 - val_loss: 0.9152 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80863 to 0.81132, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.8588 - acc: 0.7074 - val_loss: 0.9790 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81132\n",
      "Epoch 5/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.8250 - acc: 0.7214 - val_loss: 1.0043 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81132\n",
      "Epoch 6/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7906 - acc: 0.7285 - val_loss: 0.9194 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81132 to 0.81402, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 7/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7754 - acc: 0.7291 - val_loss: 0.9189 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81402\n",
      "Epoch 8/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.7655 - acc: 0.7393 - val_loss: 0.9201 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81402\n",
      "Epoch 9/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7523 - acc: 0.7408 - val_loss: 0.9236 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81402\n",
      "Epoch 10/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7398 - acc: 0.7408 - val_loss: 0.9454 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81402\n",
      "Epoch 11/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7297 - acc: 0.7503 - val_loss: 0.9517 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81402 to 0.82210, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 12/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.7311 - acc: 0.7506 - val_loss: 0.9292 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82210\n",
      "Epoch 13/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7160 - acc: 0.7528 - val_loss: 0.9061 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82210\n",
      "Epoch 14/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.7081 - acc: 0.7509 - val_loss: 0.8861 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82210\n",
      "Epoch 15/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6953 - acc: 0.7548 - val_loss: 0.8737 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.82210 to 0.83288, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 16/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6737 - acc: 0.7625 - val_loss: 0.8569 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83288\n",
      "Epoch 17/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6747 - acc: 0.7629 - val_loss: 0.8718 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.83288 to 0.83558, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 18/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6713 - acc: 0.7620 - val_loss: 0.8908 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83558\n",
      "Epoch 19/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6673 - acc: 0.7649 - val_loss: 0.9144 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83558\n",
      "Epoch 20/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6705 - acc: 0.7633 - val_loss: 0.8429 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83558\n",
      "Epoch 21/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6520 - acc: 0.7697 - val_loss: 0.8567 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83558\n",
      "Epoch 22/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6532 - acc: 0.7704 - val_loss: 0.8571 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83558\n",
      "Epoch 23/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6482 - acc: 0.7694 - val_loss: 0.8681 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83558\n",
      "Epoch 24/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6400 - acc: 0.7711 - val_loss: 0.8965 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83558\n",
      "Epoch 25/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6456 - acc: 0.7686 - val_loss: 0.8815 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83558\n",
      "Epoch 26/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6441 - acc: 0.7702 - val_loss: 0.8614 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83558\n",
      "Epoch 27/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6328 - acc: 0.7746 - val_loss: 0.8768 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83558\n",
      "Epoch 28/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6250 - acc: 0.7737 - val_loss: 0.8568 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83558\n",
      "Epoch 29/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6267 - acc: 0.7732 - val_loss: 0.8596 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.83558 to 0.84097, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 30/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6114 - acc: 0.7815 - val_loss: 0.8535 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84097\n",
      "Epoch 31/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6198 - acc: 0.7768 - val_loss: 0.8643 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84097\n",
      "Epoch 32/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6125 - acc: 0.7808 - val_loss: 0.8935 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84097\n",
      "Epoch 33/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.6248 - acc: 0.7731 - val_loss: 0.8737 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84097\n",
      "Epoch 34/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5943 - acc: 0.7830 - val_loss: 0.8593 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84097\n",
      "Epoch 35/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.6038 - acc: 0.7790 - val_loss: 0.8529 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84097\n",
      "Epoch 36/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5975 - acc: 0.7778 - val_loss: 0.8564 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84097\n",
      "Epoch 37/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5923 - acc: 0.7808 - val_loss: 0.8946 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84097\n",
      "Epoch 38/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5867 - acc: 0.7875 - val_loss: 0.8891 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84097\n",
      "Epoch 39/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5901 - acc: 0.7857 - val_loss: 0.8672 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84097\n",
      "Epoch 40/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5810 - acc: 0.7879 - val_loss: 0.8578 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.84097 to 0.84636, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 41/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5760 - acc: 0.7869 - val_loss: 0.8386 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84636\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5836 - acc: 0.7792 - val_loss: 0.8920 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84636\n",
      "Epoch 43/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5714 - acc: 0.7914 - val_loss: 0.8664 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84636\n",
      "Epoch 44/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5759 - acc: 0.7877 - val_loss: 0.8836 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84636\n",
      "Epoch 45/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5641 - acc: 0.7863 - val_loss: 0.8466 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84636\n",
      "Epoch 46/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5638 - acc: 0.7884 - val_loss: 0.9197 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84636\n",
      "Epoch 47/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5767 - acc: 0.7842 - val_loss: 0.8677 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.84636 to 0.84906, saving model to model/mfcc7/LGD_fold8_co_resnet.h5\n",
      "Epoch 48/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5694 - acc: 0.7839 - val_loss: 0.8785 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84906\n",
      "Epoch 49/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5656 - acc: 0.7913 - val_loss: 0.8747 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84906\n",
      "Epoch 50/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5471 - acc: 0.7974 - val_loss: 0.8741 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84906\n",
      "Epoch 51/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5383 - acc: 0.7961 - val_loss: 0.9107 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84906\n",
      "Epoch 52/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5610 - acc: 0.7907 - val_loss: 0.8546 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84906\n",
      "Epoch 53/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5453 - acc: 0.7981 - val_loss: 0.9177 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84906\n",
      "Epoch 54/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5490 - acc: 0.7921 - val_loss: 0.8935 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84906\n",
      "Epoch 55/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5364 - acc: 0.7970 - val_loss: 0.8779 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84906\n",
      "Epoch 56/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5561 - acc: 0.7959 - val_loss: 0.8399 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84906\n",
      "Epoch 57/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.5316 - acc: 0.7955 - val_loss: 0.8449 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84906\n",
      "Epoch 58/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5276 - acc: 0.7982 - val_loss: 0.9528 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84906\n",
      "Epoch 59/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5471 - acc: 0.7919 - val_loss: 0.8772 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84906\n",
      "Epoch 60/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5399 - acc: 0.7902 - val_loss: 0.8774 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84906\n",
      "Epoch 61/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5435 - acc: 0.7904 - val_loss: 0.8857 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84906\n",
      "Epoch 62/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5535 - acc: 0.7929 - val_loss: 0.8271 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84906\n",
      "Epoch 63/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5477 - acc: 0.7958 - val_loss: 0.8258 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84906\n",
      "Epoch 64/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5228 - acc: 0.7983 - val_loss: 0.8424 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84906\n",
      "Epoch 65/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5278 - acc: 0.7966 - val_loss: 0.8098 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84906\n",
      "Epoch 66/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5447 - acc: 0.7914 - val_loss: 0.8637 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84906\n",
      "Epoch 67/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5314 - acc: 0.7943 - val_loss: 0.8517 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84906\n",
      "Epoch 68/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5171 - acc: 0.7993 - val_loss: 0.8536 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84906\n",
      "Epoch 69/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5165 - acc: 0.8004 - val_loss: 0.8758 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84906\n",
      "Epoch 70/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5174 - acc: 0.8041 - val_loss: 0.8333 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84906\n",
      "Epoch 71/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5257 - acc: 0.8011 - val_loss: 0.8439 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84906\n",
      "Epoch 72/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5084 - acc: 0.7986 - val_loss: 0.8505 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84906\n",
      "Epoch 73/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5217 - acc: 0.8009 - val_loss: 0.8217 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84906\n",
      "Epoch 74/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5222 - acc: 0.7984 - val_loss: 0.8199 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84906\n",
      "Epoch 75/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5029 - acc: 0.8012 - val_loss: 0.8305 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84906\n",
      "Epoch 76/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5108 - acc: 0.8016 - val_loss: 0.8411 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84906\n",
      "Epoch 77/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5037 - acc: 0.8062 - val_loss: 0.8881 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84906\n",
      "Epoch 78/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5121 - acc: 0.8030 - val_loss: 0.8506 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84906\n",
      "Epoch 79/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5155 - acc: 0.7981 - val_loss: 0.9250 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84906\n",
      "Epoch 80/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5004 - acc: 0.8048 - val_loss: 0.8248 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84906\n",
      "Epoch 81/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5166 - acc: 0.7992 - val_loss: 0.8591 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84906\n",
      "Epoch 82/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4885 - acc: 0.8121 - val_loss: 0.8479 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.84906\n",
      "Epoch 83/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.5002 - acc: 0.8027 - val_loss: 0.8476 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.84906\n",
      "Epoch 84/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4918 - acc: 0.8061 - val_loss: 0.8708 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.84906\n",
      "Epoch 85/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4902 - acc: 0.8097 - val_loss: 0.8888 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.84906\n",
      "Epoch 86/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.4790 - acc: 0.8070 - val_loss: 0.8783 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.84906\n",
      "Epoch 87/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.4743 - acc: 0.8073 - val_loss: 0.8607 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.84906\n",
      "Epoch 88/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.4993 - acc: 0.7976 - val_loss: 0.8515 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.84906\n",
      "Epoch 89/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4739 - acc: 0.8107 - val_loss: 0.8437 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.84906\n",
      "Epoch 90/3000\n",
      "198/198 [==============================] - 53s 269ms/step - loss: 1.4800 - acc: 0.8060 - val_loss: 0.9006 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.84906\n",
      "Epoch 91/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4859 - acc: 0.8115 - val_loss: 0.8584 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.84906\n",
      "Epoch 92/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4785 - acc: 0.8094 - val_loss: 0.8753 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.84906\n",
      "Epoch 93/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4878 - acc: 0.8075 - val_loss: 0.8140 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.84906\n",
      "Epoch 94/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4801 - acc: 0.8078 - val_loss: 0.8408 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.84906\n",
      "Epoch 95/3000\n",
      "198/198 [==============================] - 53s 270ms/step - loss: 1.4787 - acc: 0.8106 - val_loss: 0.8530 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.84906\n",
      "Epoch 00095: early stopping\n",
      "(3339, 60, 259, 1) (3339, 41)\n",
      "(6360, 60, 259, 1) (6360, 41)\n",
      "===train semi_7===\n",
      "semi loading: model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 60, 259, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 30, 130, 64)  3200        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 30, 130, 64)  256         conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 30, 130, 64)  0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 15, 65, 64)   0           activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 15, 65, 64)   4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 15, 65, 64)   256         conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 15, 65, 64)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 15, 65, 64)   36928       activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 15, 65, 64)   256         conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 15, 65, 64)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 15, 65, 256)  16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 15, 65, 256)  16640       activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 15, 65, 256)  0           conv2d_327[0][0]                 \n",
      "                                                                 conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 15, 65, 256)  1024        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 15, 65, 256)  0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 15, 65, 64)   16448       activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 15, 65, 64)   256         conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 15, 65, 64)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 15, 65, 64)   36928       activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 15, 65, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 15, 65, 64)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 15, 65, 256)  16640       activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 15, 65, 256)  0           add_114[0][0]                    \n",
      "                                                                 conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 15, 65, 256)  1024        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 15, 65, 256)  0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 15, 65, 64)   16448       activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 15, 65, 64)   256         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 15, 65, 64)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 15, 65, 64)   36928       activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 15, 65, 64)   256         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 15, 65, 64)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 15, 65, 256)  16640       activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 15, 65, 256)  0           add_115[0][0]                    \n",
      "                                                                 conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 15, 65, 256)  1024        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 15, 65, 256)  0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 8, 33, 128)   32896       activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 8, 33, 128)   512         conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 8, 33, 128)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 8, 33, 128)   147584      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 8, 33, 128)   512         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 8, 33, 128)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 8, 33, 512)   131584      add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 8, 33, 512)   66048       activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 8, 33, 512)   0           conv2d_337[0][0]                 \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 8, 33, 512)   2048        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 8, 33, 512)   0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 8, 33, 128)   65664       activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 8, 33, 128)   512         conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 8, 33, 128)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 8, 33, 128)   147584      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 8, 33, 128)   512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 8, 33, 128)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 8, 33, 512)   66048       activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 8, 33, 512)   0           add_117[0][0]                    \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 8, 33, 512)   2048        add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 8, 33, 512)   0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 8, 33, 128)   65664       activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 8, 33, 128)   512         conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 8, 33, 128)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 8, 33, 128)   147584      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 8, 33, 128)   512         conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 8, 33, 128)   0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 8, 33, 512)   66048       activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 8, 33, 512)   0           add_118[0][0]                    \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 8, 33, 512)   2048        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 8, 33, 512)   0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 8, 33, 128)   65664       activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 8, 33, 128)   512         conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 8, 33, 128)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 8, 33, 128)   147584      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 8, 33, 128)   512         conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 8, 33, 128)   0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 8, 33, 512)   66048       activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 8, 33, 512)   0           add_119[0][0]                    \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 8, 33, 512)   2048        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 8, 33, 512)   0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 17, 256)   131328      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 17, 256)   1024        conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 17, 256)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 17, 256)   590080      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 17, 256)   1024        conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 17, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 17, 1024)  525312      add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 17, 1024)  263168      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 4, 17, 1024)  0           conv2d_350[0][0]                 \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 17, 1024)  4096        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 17, 256)   262400      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 17, 256)   1024        conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 17, 256)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 17, 256)   590080      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 17, 256)   1024        conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 17, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 17, 1024)  263168      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 4, 17, 1024)  0           add_121[0][0]                    \n",
      "                                                                 conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 17, 1024)  4096        add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 17, 256)   262400      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 17, 256)   1024        conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 17, 256)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 17, 256)   590080      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 17, 256)   1024        conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 17, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 17, 1024)  263168      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 4, 17, 1024)  0           add_122[0][0]                    \n",
      "                                                                 conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 17, 1024)  4096        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 17, 256)   262400      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 17, 256)   1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 17, 256)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 17, 256)   590080      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 17, 256)   1024        conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 17, 256)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 4, 17, 1024)  263168      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_124 (Add)                   (None, 4, 17, 1024)  0           add_123[0][0]                    \n",
      "                                                                 conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 17, 1024)  4096        add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 4, 17, 256)   262400      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 17, 256)   1024        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 17, 256)   0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 4, 17, 256)   590080      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 17, 256)   1024        conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 17, 256)   0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 4, 17, 1024)  263168      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_125 (Add)                   (None, 4, 17, 1024)  0           add_124[0][0]                    \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 17, 1024)  4096        add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 4, 17, 256)   262400      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 17, 256)   1024        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 17, 256)   0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 4, 17, 256)   590080      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 17, 256)   1024        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 17, 256)   0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 4, 17, 1024)  263168      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_126 (Add)                   (None, 4, 17, 1024)  0           add_125[0][0]                    \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 17, 1024)  4096        add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 4, 17, 256)   262400      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 4, 17, 256)   1024        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 17, 256)   0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 4, 17, 256)   590080      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 4, 17, 256)   1024        conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 17, 256)   0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 4, 17, 1024)  263168      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_127 (Add)                   (None, 4, 17, 1024)  0           add_126[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 4, 17, 1024)  4096        add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 4, 17, 256)   262400      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 4, 17, 256)   1024        conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 4, 17, 256)   0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 4, 17, 256)   590080      activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 4, 17, 256)   1024        conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 4, 17, 256)   0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 4, 17, 1024)  263168      activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_128 (Add)                   (None, 4, 17, 1024)  0           add_127[0][0]                    \n",
      "                                                                 conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 4, 17, 1024)  4096        add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 4, 17, 256)   262400      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 4, 17, 256)   1024        conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 4, 17, 256)   0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 4, 17, 256)   590080      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 4, 17, 256)   1024        conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 4, 17, 256)   0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 4, 17, 1024)  263168      activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_129 (Add)                   (None, 4, 17, 1024)  0           add_128[0][0]                    \n",
      "                                                                 conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 4, 17, 1024)  4096        add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 4, 17, 256)   262400      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 4, 17, 256)   1024        conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 4, 17, 256)   0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_376 (Conv2D)             (None, 4, 17, 256)   590080      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 4, 17, 256)   1024        conv2d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 4, 17, 256)   0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_377 (Conv2D)             (None, 4, 17, 1024)  263168      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_130 (Add)                   (None, 4, 17, 1024)  0           add_129[0][0]                    \n",
      "                                                                 conv2d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 4, 17, 1024)  4096        add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_378 (Conv2D)             (None, 4, 17, 256)   262400      activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 4, 17, 256)   1024        conv2d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 4, 17, 256)   0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_379 (Conv2D)             (None, 4, 17, 256)   590080      activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 4, 17, 256)   1024        conv2d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 4, 17, 256)   0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_380 (Conv2D)             (None, 4, 17, 1024)  263168      activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_131 (Add)                   (None, 4, 17, 1024)  0           add_130[0][0]                    \n",
      "                                                                 conv2d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 4, 17, 1024)  4096        add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_381 (Conv2D)             (None, 4, 17, 256)   262400      activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 4, 17, 256)   1024        conv2d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 4, 17, 256)   0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_382 (Conv2D)             (None, 4, 17, 256)   590080      activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 4, 17, 256)   1024        conv2d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 4, 17, 256)   0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_383 (Conv2D)             (None, 4, 17, 1024)  263168      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 4, 17, 1024)  0           add_131[0][0]                    \n",
      "                                                                 conv2d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 4, 17, 1024)  4096        add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_384 (Conv2D)             (None, 4, 17, 256)   262400      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 4, 17, 256)   1024        conv2d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 4, 17, 256)   0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_385 (Conv2D)             (None, 4, 17, 256)   590080      activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 4, 17, 256)   1024        conv2d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 4, 17, 256)   0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_386 (Conv2D)             (None, 4, 17, 1024)  263168      activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 4, 17, 1024)  0           add_132[0][0]                    \n",
      "                                                                 conv2d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 4, 17, 1024)  4096        add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_387 (Conv2D)             (None, 4, 17, 256)   262400      activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 4, 17, 256)   1024        conv2d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 4, 17, 256)   0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_388 (Conv2D)             (None, 4, 17, 256)   590080      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 4, 17, 256)   1024        conv2d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 4, 17, 256)   0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 4, 17, 1024)  263168      activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 4, 17, 1024)  0           add_133[0][0]                    \n",
      "                                                                 conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 4, 17, 1024)  4096        add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 4, 17, 256)   262400      activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 4, 17, 256)   1024        conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 4, 17, 256)   0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 4, 17, 256)   590080      activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 4, 17, 256)   1024        conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 4, 17, 256)   0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 4, 17, 1024)  263168      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 4, 17, 1024)  0           add_134[0][0]                    \n",
      "                                                                 conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 4, 17, 1024)  4096        add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 4, 17, 256)   262400      activation_365[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 4, 17, 256)   1024        conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 4, 17, 256)   0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 4, 17, 256)   590080      activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 4, 17, 256)   1024        conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 4, 17, 256)   0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 4, 17, 1024)  263168      activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 4, 17, 1024)  0           add_135[0][0]                    \n",
      "                                                                 conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 4, 17, 1024)  4096        add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 4, 17, 256)   262400      activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 4, 17, 256)   1024        conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 4, 17, 256)   0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 4, 17, 256)   590080      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 4, 17, 256)   1024        conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 4, 17, 256)   0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 4, 17, 1024)  263168      activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 4, 17, 1024)  0           add_136[0][0]                    \n",
      "                                                                 conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 4, 17, 1024)  4096        add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 4, 17, 256)   262400      activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 4, 17, 256)   1024        conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 4, 17, 256)   0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 4, 17, 256)   590080      activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 4, 17, 256)   1024        conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 4, 17, 256)   0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 4, 17, 1024)  263168      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 4, 17, 1024)  0           add_137[0][0]                    \n",
      "                                                                 conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 4, 17, 1024)  4096        add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 4, 17, 256)   262400      activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 4, 17, 256)   1024        conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 4, 17, 256)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 4, 17, 256)   590080      activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 4, 17, 256)   1024        conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 4, 17, 256)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 4, 17, 1024)  263168      activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 4, 17, 1024)  0           add_138[0][0]                    \n",
      "                                                                 conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 4, 17, 1024)  4096        add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 4, 17, 256)   262400      activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 4, 17, 256)   1024        conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 4, 17, 256)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 4, 17, 256)   590080      activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 4, 17, 256)   1024        conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 4, 17, 256)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 4, 17, 1024)  263168      activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 4, 17, 1024)  0           add_139[0][0]                    \n",
      "                                                                 conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 4, 17, 1024)  4096        add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 4, 17, 256)   262400      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 4, 17, 256)   1024        conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 4, 17, 256)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 4, 17, 256)   590080      activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 4, 17, 256)   1024        conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 4, 17, 256)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 4, 17, 1024)  263168      activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 4, 17, 1024)  0           add_140[0][0]                    \n",
      "                                                                 conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 4, 17, 1024)  4096        add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 4, 17, 256)   262400      activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 4, 17, 256)   1024        conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 4, 17, 256)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 4, 17, 256)   590080      activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 4, 17, 256)   1024        conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 4, 17, 256)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 4, 17, 1024)  263168      activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 4, 17, 1024)  0           add_141[0][0]                    \n",
      "                                                                 conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 4, 17, 1024)  4096        add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 4, 17, 256)   262400      activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 4, 17, 256)   1024        conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 4, 17, 256)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 4, 17, 256)   590080      activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 4, 17, 256)   1024        conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 4, 17, 256)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 4, 17, 1024)  263168      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 4, 17, 1024)  0           add_142[0][0]                    \n",
      "                                                                 conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 4, 17, 1024)  4096        add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 4, 17, 1024)  0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 2, 9, 512)    524800      activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 2, 9, 512)    2048        conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 2, 9, 512)    0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 2, 9, 512)    2359808     activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 2, 9, 512)    2048        conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 2, 9, 512)    0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 2, 9, 2048)   2099200     add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 2, 9, 2048)   0           conv2d_420[0][0]                 \n",
      "                                                                 conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 2, 9, 2048)   8192        add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 2, 9, 512)    1049088     activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 2, 9, 512)    2048        conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 2, 9, 512)    0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 2, 9, 512)    2359808     activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 2, 9, 512)    2048        conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 2, 9, 512)    0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 2, 9, 2048)   0           add_144[0][0]                    \n",
      "                                                                 conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 2, 9, 2048)   8192        add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 2, 9, 512)    1049088     activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 2, 9, 512)    2048        conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 2, 9, 512)    0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 2, 9, 512)    2359808     activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 2, 9, 512)    2048        conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 2, 9, 512)    0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 2, 9, 2048)   1050624     activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 2, 9, 2048)   0           add_145[0][0]                    \n",
      "                                                                 conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 2, 9, 2048)   8192        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 2, 9, 2048)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1, 1, 2048)   0           activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 44)           90156       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 44)           176         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 44)           0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 41)           1845        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,728,721\n",
      "Trainable params: 42,630,969\n",
      "Non-trainable params: 97,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "397/397 [==============================] - 83s 210ms/step - loss: 1.5498 - acc: 0.8025 - val_loss: 0.8631 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81671, saving model to model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "397/397 [==============================] - 70s 177ms/step - loss: 1.5352 - acc: 0.8068 - val_loss: 0.8651 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.81671\n",
      "Epoch 3/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5311 - acc: 0.8049 - val_loss: 0.8314 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81671 to 0.82210, saving model to model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5257 - acc: 0.8071 - val_loss: 0.8863 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82210\n",
      "Epoch 5/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5174 - acc: 0.8063 - val_loss: 0.8201 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82210 to 0.83558, saving model to model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "Epoch 6/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5101 - acc: 0.8088 - val_loss: 0.8129 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.83558\n",
      "Epoch 7/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5140 - acc: 0.8038 - val_loss: 0.8578 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83558\n",
      "Epoch 8/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5143 - acc: 0.8058 - val_loss: 0.8192 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83558\n",
      "Epoch 9/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5059 - acc: 0.8107 - val_loss: 0.8378 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.83558 to 0.84097, saving model to model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "Epoch 10/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4985 - acc: 0.8120 - val_loss: 0.8252 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84097\n",
      "Epoch 11/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4987 - acc: 0.8121 - val_loss: 0.8276 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.84097\n",
      "Epoch 12/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.5030 - acc: 0.8070 - val_loss: 0.8162 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.84097\n",
      "Epoch 13/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4819 - acc: 0.8146 - val_loss: 0.8542 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84097\n",
      "Epoch 14/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4839 - acc: 0.8147 - val_loss: 0.8230 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84097\n",
      "Epoch 15/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4852 - acc: 0.8080 - val_loss: 0.8179 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84097\n",
      "Epoch 16/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4897 - acc: 0.8112 - val_loss: 0.8591 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84097\n",
      "Epoch 17/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4782 - acc: 0.8115 - val_loss: 0.8390 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84097\n",
      "Epoch 18/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4819 - acc: 0.8127 - val_loss: 0.8279 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84097\n",
      "Epoch 19/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4739 - acc: 0.8190 - val_loss: 0.8095 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.84097 to 0.84906, saving model to model/mfcc7/LGD_fold7_co_resnet.h5\n",
      "Epoch 20/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4786 - acc: 0.8126 - val_loss: 0.8564 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84906\n",
      "Epoch 21/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4751 - acc: 0.8127 - val_loss: 0.8315 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84906\n",
      "Epoch 22/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4644 - acc: 0.8198 - val_loss: 0.8450 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84906\n",
      "Epoch 23/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4668 - acc: 0.8101 - val_loss: 0.8285 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84906\n",
      "Epoch 24/3000\n",
      "397/397 [==============================] - 71s 178ms/step - loss: 1.4708 - acc: 0.8106 - val_loss: 0.8177 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84906\n",
      "Epoch 25/3000\n",
      "180/397 [============>.................] - ETA: 38s - loss: 1.4760 - acc: 0.8144"
     ]
    }
   ],
   "source": [
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "#     print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "#     model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(X_semi,Y_semi,fold)\n",
    "\n",
    "#MFCC7\n",
    "#7=>\n",
    "#8=>0.84906\n",
    "#9=>0.90566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_9===\n",
      "semi loading: model/mfcc6/LGD_semi_fold9_resnet2.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 216, 64)  3200        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 216, 64)  256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 216, 64)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 108, 64)  4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 108, 64)  256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 108, 64)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 108, 64)  36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 108, 64)  256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 108, 64)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 108, 256) 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 108, 256) 16640       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 108, 256) 0           conv2d_61[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 108, 256) 1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 108, 256) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 108, 64)  16448       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 108, 64)  256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 108, 64)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 108, 64)  36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 108, 64)  256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 108, 64)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 108, 256) 16640       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 108, 256) 0           add_25[0][0]                     \n",
      "                                                                 conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 108, 256) 1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 108, 256) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 108, 64)  16448       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 108, 64)  256         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 108, 64)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 108, 64)  36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 108, 64)  256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 108, 64)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 108, 256) 16640       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 108, 256) 0           add_26[0][0]                     \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 108, 256) 1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 108, 256) 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 8, 54, 128)   32896       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 54, 128)   512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 54, 128)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 8, 54, 128)   147584      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 54, 128)   512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 54, 128)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 54, 512)   131584      add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 54, 512)   66048       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 8, 54, 512)   0           conv2d_71[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 54, 512)   2048        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 54, 512)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 54, 128)   65664       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 54, 128)   512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 8, 54, 128)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 54, 128)   147584      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 54, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 8, 54, 128)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 54, 512)   66048       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 8, 54, 512)   0           add_28[0][0]                     \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 54, 512)   2048        add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 8, 54, 512)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 8, 54, 128)   65664       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 54, 128)   512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 8, 54, 128)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 54, 128)   147584      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 54, 128)   512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 8, 54, 128)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 54, 512)   66048       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 54, 512)   0           add_29[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 54, 512)   2048        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 8, 54, 512)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 54, 128)   65664       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 54, 128)   512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 8, 54, 128)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 54, 128)   147584      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 54, 128)   512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 8, 54, 128)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 54, 512)   66048       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 54, 512)   0           add_30[0][0]                     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 54, 512)   2048        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 54, 512)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 27, 256)   131328      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 27, 256)   1024        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 4, 27, 256)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 27, 256)   590080      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 27, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 27, 256)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 27, 1024)  525312      add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 27, 1024)  263168      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 4, 27, 1024)  0           conv2d_84[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 27, 1024)  4096        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 27, 256)   262400      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 27, 256)   1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 27, 256)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 27, 256)   590080      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 27, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 27, 256)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 27, 1024)  263168      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 27, 1024)  0           add_32[0][0]                     \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 27, 1024)  4096        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 27, 256)   262400      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 27, 256)   1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 27, 256)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 27, 256)   590080      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 27, 256)   1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 27, 256)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 27, 1024)  263168      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 27, 1024)  0           add_33[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 27, 1024)  4096        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 27, 256)   262400      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 27, 256)   1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 27, 256)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 27, 256)   590080      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 27, 256)   1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 27, 256)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 27, 1024)  263168      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 27, 1024)  0           add_34[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 27, 1024)  4096        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 27, 256)   262400      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 27, 256)   1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 27, 256)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 27, 256)   590080      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 27, 256)   1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 27, 256)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 27, 1024)  263168      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 4, 27, 1024)  0           add_35[0][0]                     \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 27, 1024)  4096        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 4, 27, 256)   262400      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 27, 256)   1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 27, 256)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 4, 27, 256)   590080      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 27, 256)   1024        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 27, 256)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 4, 27, 1024)  263168      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 4, 27, 1024)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 27, 1024)  4096        add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 27, 1024)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 14, 512)   524800      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 2, 14, 512)   2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 14, 512)   0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 14, 512)   2359808     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 2, 14, 512)   2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 14, 512)   0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 14, 2048)  2099200     add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 2, 14, 2048)  0           conv2d_103[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 2, 14, 2048)  8192        add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 14, 512)   1049088     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 2, 14, 512)   2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 14, 512)   0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 14, 512)   2359808     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 2, 14, 512)   2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 14, 512)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 2, 14, 2048)  0           add_38[0][0]                     \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 2, 14, 2048)  8192        add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 14, 512)   1049088     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 2, 14, 512)   2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 14, 512)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 14, 512)   2359808     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 2, 14, 512)   2048        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 14, 512)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 2, 14, 2048)  0           add_39[0][0]                     \n",
      "                                                                 conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 2, 14, 2048)  8192        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 14, 2048)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 2048)   0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 43)           88107       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 43)           172         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 43)           0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 41)           1804        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,656,163\n",
      "Trainable params: 23,610,637\n",
      "Non-trainable params: 45,526\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "390/390 [==============================] - 60s 154ms/step - loss: 1.7796 - acc: 0.7538 - val_loss: 0.9671 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84367, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.6576 - acc: 0.7913 - val_loss: 0.9901 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.84367\n",
      "Epoch 3/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.6115 - acc: 0.7986 - val_loss: 0.9155 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84367 to 0.85445, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5790 - acc: 0.8039 - val_loss: 1.0235 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85445\n",
      "Epoch 5/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5633 - acc: 0.8026 - val_loss: 0.9366 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85445\n",
      "Epoch 6/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5405 - acc: 0.8131 - val_loss: 0.9503 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85445 to 0.85984, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 7/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5209 - acc: 0.8135 - val_loss: 0.9732 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85984\n",
      "Epoch 8/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5149 - acc: 0.8099 - val_loss: 0.9145 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85984\n",
      "Epoch 9/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.5013 - acc: 0.8151 - val_loss: 0.9339 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85984\n",
      "Epoch 10/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4766 - acc: 0.8168 - val_loss: 0.9063 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85984\n",
      "Epoch 11/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4649 - acc: 0.8179 - val_loss: 0.9461 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85984\n",
      "Epoch 12/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4570 - acc: 0.8264 - val_loss: 0.9294 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85984\n",
      "Epoch 13/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4461 - acc: 0.8252 - val_loss: 0.9511 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85984\n",
      "Epoch 14/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4486 - acc: 0.8239 - val_loss: 0.9450 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85984\n",
      "Epoch 15/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4433 - acc: 0.8280 - val_loss: 0.9308 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85984\n",
      "Epoch 16/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4224 - acc: 0.8321 - val_loss: 0.8678 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85984\n",
      "Epoch 17/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4132 - acc: 0.8331 - val_loss: 0.9318 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85984\n",
      "Epoch 18/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4108 - acc: 0.8344 - val_loss: 0.8598 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.85984 to 0.85984, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 19/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.4063 - acc: 0.8290 - val_loss: 0.8692 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85984\n",
      "Epoch 20/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3863 - acc: 0.8361 - val_loss: 0.8963 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85984\n",
      "Epoch 21/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3964 - acc: 0.8318 - val_loss: 0.8441 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85984\n",
      "Epoch 22/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3915 - acc: 0.8330 - val_loss: 0.9524 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85984\n",
      "Epoch 23/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3852 - acc: 0.8334 - val_loss: 0.9044 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85984\n",
      "Epoch 24/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3629 - acc: 0.8432 - val_loss: 0.8950 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85984\n",
      "Epoch 25/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3684 - acc: 0.8323 - val_loss: 0.8375 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.85984 to 0.85984, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 26/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3708 - acc: 0.8387 - val_loss: 0.8698 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85984\n",
      "Epoch 27/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3519 - acc: 0.8389 - val_loss: 0.8151 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.85984 to 0.87062, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 28/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3454 - acc: 0.8401 - val_loss: 0.8517 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87062\n",
      "Epoch 29/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3600 - acc: 0.8353 - val_loss: 0.9038 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87062\n",
      "Epoch 30/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3428 - acc: 0.8410 - val_loss: 0.8449 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87062\n",
      "Epoch 31/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3311 - acc: 0.8408 - val_loss: 0.8867 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87062\n",
      "Epoch 32/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3450 - acc: 0.8327 - val_loss: 0.9149 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87062\n",
      "Epoch 33/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3438 - acc: 0.8365 - val_loss: 0.8943 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87062\n",
      "Epoch 34/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3254 - acc: 0.8417 - val_loss: 0.9443 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87062\n",
      "Epoch 35/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3171 - acc: 0.8396 - val_loss: 0.8758 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87062\n",
      "Epoch 36/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3206 - acc: 0.8425 - val_loss: 0.8642 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87062\n",
      "Epoch 37/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3197 - acc: 0.8469 - val_loss: 0.8228 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87062\n",
      "Epoch 38/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3223 - acc: 0.8379 - val_loss: 0.9943 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87062\n",
      "Epoch 39/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3107 - acc: 0.8430 - val_loss: 0.8461 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87062\n",
      "Epoch 40/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2949 - acc: 0.8456 - val_loss: 0.8711 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87062\n",
      "Epoch 41/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2932 - acc: 0.8462 - val_loss: 0.8298 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87062\n",
      "Epoch 42/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.3077 - acc: 0.8436 - val_loss: 0.9555 - val_acc: 0.8194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87062\n",
      "Epoch 43/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2969 - acc: 0.8453 - val_loss: 0.8839 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87062\n",
      "Epoch 44/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2952 - acc: 0.8416 - val_loss: 0.9055 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87062\n",
      "Epoch 45/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2802 - acc: 0.8503 - val_loss: 0.8181 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87062\n",
      "Epoch 46/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2922 - acc: 0.8463 - val_loss: 0.8916 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87062\n",
      "Epoch 47/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2750 - acc: 0.8454 - val_loss: 0.8337 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87062\n",
      "Epoch 48/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2854 - acc: 0.8443 - val_loss: 0.8116 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87062\n",
      "Epoch 49/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2771 - acc: 0.8470 - val_loss: 0.8406 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87062\n",
      "Epoch 50/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2660 - acc: 0.8482 - val_loss: 0.8600 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87062\n",
      "Epoch 51/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2806 - acc: 0.8424 - val_loss: 0.8114 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87062\n",
      "Epoch 52/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2751 - acc: 0.8520 - val_loss: 0.9653 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87062\n",
      "Epoch 53/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2738 - acc: 0.8453 - val_loss: 0.8639 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87062\n",
      "Epoch 54/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2596 - acc: 0.8514 - val_loss: 0.8402 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87062\n",
      "Epoch 55/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2679 - acc: 0.8482 - val_loss: 0.8458 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87062\n",
      "Epoch 56/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2582 - acc: 0.8522 - val_loss: 0.7907 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87062\n",
      "Epoch 57/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2582 - acc: 0.8507 - val_loss: 0.8766 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87062\n",
      "Epoch 58/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2478 - acc: 0.8506 - val_loss: 0.7541 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87062\n",
      "Epoch 59/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2465 - acc: 0.8526 - val_loss: 0.8996 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87062\n",
      "Epoch 60/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2453 - acc: 0.8505 - val_loss: 0.8057 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87062\n",
      "Epoch 61/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2477 - acc: 0.8505 - val_loss: 0.9072 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87062\n",
      "Epoch 62/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2469 - acc: 0.8513 - val_loss: 0.8089 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87062\n",
      "Epoch 63/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2555 - acc: 0.8467 - val_loss: 0.7795 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87062\n",
      "Epoch 64/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2363 - acc: 0.8519 - val_loss: 0.7775 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.87062 to 0.87332, saving model to model/mfcc6/LGD_fold9_co_resnet.h5\n",
      "Epoch 65/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2381 - acc: 0.8556 - val_loss: 0.9138 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87332\n",
      "Epoch 66/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2367 - acc: 0.8493 - val_loss: 0.8930 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87332\n",
      "Epoch 67/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2365 - acc: 0.8566 - val_loss: 0.7901 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87332\n",
      "Epoch 68/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2385 - acc: 0.8498 - val_loss: 0.8838 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87332\n",
      "Epoch 69/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2160 - acc: 0.8574 - val_loss: 0.8981 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87332\n",
      "Epoch 70/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2350 - acc: 0.8509 - val_loss: 0.8531 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87332\n",
      "Epoch 71/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2365 - acc: 0.8486 - val_loss: 0.8211 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87332\n",
      "Epoch 72/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2313 - acc: 0.8486 - val_loss: 0.9160 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87332\n",
      "Epoch 73/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2132 - acc: 0.8566 - val_loss: 0.8254 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87332\n",
      "Epoch 74/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2205 - acc: 0.8543 - val_loss: 0.9099 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87332\n",
      "Epoch 75/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2115 - acc: 0.8533 - val_loss: 0.8040 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87332\n",
      "Epoch 76/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2190 - acc: 0.8506 - val_loss: 0.7997 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87332\n",
      "Epoch 77/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2128 - acc: 0.8551 - val_loss: 0.7941 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87332\n",
      "Epoch 78/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2137 - acc: 0.8575 - val_loss: 0.8338 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87332\n",
      "Epoch 79/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2085 - acc: 0.8495 - val_loss: 0.8218 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87332\n",
      "Epoch 80/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2268 - acc: 0.8471 - val_loss: 0.8947 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87332\n",
      "Epoch 81/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2203 - acc: 0.8523 - val_loss: 0.8415 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87332\n",
      "Epoch 82/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2087 - acc: 0.8558 - val_loss: 0.8216 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.87332\n",
      "Epoch 83/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2051 - acc: 0.8526 - val_loss: 0.7876 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.87332\n",
      "Epoch 84/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2128 - acc: 0.8555 - val_loss: 0.7926 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.87332\n",
      "Epoch 85/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.1995 - acc: 0.8620 - val_loss: 0.8263 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.87332\n",
      "Epoch 86/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2006 - acc: 0.8604 - val_loss: 0.8219 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87332\n",
      "Epoch 87/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2013 - acc: 0.8579 - val_loss: 0.8531 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87332\n",
      "Epoch 88/3000\n",
      "390/390 [==============================] - 53s 136ms/step - loss: 1.2078 - acc: 0.8529 - val_loss: 0.8652 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87332\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_8===\n",
      "semi loading: model/mfcc6/LGD_fold8_resnet1-.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 32, 216, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 32, 216, 64)  256         conv2d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 32, 216, 64)  0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 16, 108, 64)  0           activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 16, 108, 64)  36928       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 16, 108, 64)  256         conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 16, 108, 64)  0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 16, 108, 64)  36928       activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_255 (Add)                   (None, 16, 108, 64)  0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 16, 108, 64)  256         add_255[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 16, 108, 64)  0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 16, 108, 64)  36928       activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 16, 108, 64)  256         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 16, 108, 64)  0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 16, 108, 64)  36928       activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_256 (Add)                   (None, 16, 108, 64)  0           add_255[0][0]                    \n",
      "                                                                 conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 16, 108, 64)  256         add_256[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 16, 108, 64)  0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 16, 108, 64)  36928       activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 16, 108, 64)  256         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 16, 108, 64)  0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 16, 108, 64)  36928       activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_257 (Add)                   (None, 16, 108, 64)  0           add_256[0][0]                    \n",
      "                                                                 conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 16, 108, 64)  256         add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 16, 108, 64)  0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 8, 54, 128)   73856       activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 8, 54, 128)   512         conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 8, 54, 128)   0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 8, 54, 128)   8320        add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 8, 54, 128)   147584      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_258 (Add)                   (None, 8, 54, 128)   0           conv2d_766[0][0]                 \n",
      "                                                                 conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 8, 54, 128)   512         add_258[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 8, 54, 128)   0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 8, 54, 128)   147584      activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 8, 54, 128)   512         conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 8, 54, 128)   0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 8, 54, 128)   147584      activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_259 (Add)                   (None, 8, 54, 128)   0           add_258[0][0]                    \n",
      "                                                                 conv2d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 8, 54, 128)   512         add_259[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 8, 54, 128)   0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 8, 54, 128)   147584      activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 8, 54, 128)   512         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 8, 54, 128)   0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 8, 54, 128)   147584      activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_260 (Add)                   (None, 8, 54, 128)   0           add_259[0][0]                    \n",
      "                                                                 conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 8, 54, 128)   512         add_260[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 8, 54, 128)   0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 8, 54, 128)   147584      activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 8, 54, 128)   512         conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 8, 54, 128)   0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 8, 54, 128)   147584      activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_261 (Add)                   (None, 8, 54, 128)   0           add_260[0][0]                    \n",
      "                                                                 conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 8, 54, 128)   512         add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 8, 54, 128)   0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 4, 27, 256)   295168      activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 4, 27, 256)   1024        conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 4, 27, 256)   0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 4, 27, 256)   33024       add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 4, 27, 256)   590080      activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_262 (Add)                   (None, 4, 27, 256)   0           conv2d_775[0][0]                 \n",
      "                                                                 conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 4, 27, 256)   1024        add_262[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 4, 27, 256)   0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 4, 27, 256)   590080      activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 4, 27, 256)   1024        conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 4, 27, 256)   0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 4, 27, 256)   590080      activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_263 (Add)                   (None, 4, 27, 256)   0           add_262[0][0]                    \n",
      "                                                                 conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 4, 27, 256)   1024        add_263[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 4, 27, 256)   0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 4, 27, 256)   590080      activation_736[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 4, 27, 256)   1024        conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 4, 27, 256)   0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 4, 27, 256)   590080      activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_264 (Add)                   (None, 4, 27, 256)   0           add_263[0][0]                    \n",
      "                                                                 conv2d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 4, 27, 256)   1024        add_264[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 4, 27, 256)   0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 4, 27, 256)   590080      activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 4, 27, 256)   1024        conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 4, 27, 256)   0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 4, 27, 256)   590080      activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_265 (Add)                   (None, 4, 27, 256)   0           add_264[0][0]                    \n",
      "                                                                 conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 4, 27, 256)   1024        add_265[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 4, 27, 256)   0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 4, 27, 256)   590080      activation_740[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 4, 27, 256)   1024        conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_741 (Activation)     (None, 4, 27, 256)   0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 4, 27, 256)   590080      activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_266 (Add)                   (None, 4, 27, 256)   0           add_265[0][0]                    \n",
      "                                                                 conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 4, 27, 256)   1024        add_266[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 4, 27, 256)   0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 4, 27, 256)   590080      activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 4, 27, 256)   1024        conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 4, 27, 256)   0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 4, 27, 256)   590080      activation_743[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_267 (Add)                   (None, 4, 27, 256)   0           add_266[0][0]                    \n",
      "                                                                 conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 4, 27, 256)   1024        add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 4, 27, 256)   0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 2, 14, 512)   1180160     activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 2, 14, 512)   2048        conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 2, 14, 512)   0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 2, 14, 512)   131584      add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 2, 14, 512)   2359808     activation_745[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_268 (Add)                   (None, 2, 14, 512)   0           conv2d_788[0][0]                 \n",
      "                                                                 conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 2, 14, 512)   2048        add_268[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 2, 14, 512)   0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 2, 14, 512)   2359808     activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 2, 14, 512)   2048        conv2d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 2, 14, 512)   0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 2, 14, 512)   2359808     activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_269 (Add)                   (None, 2, 14, 512)   0           add_268[0][0]                    \n",
      "                                                                 conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 2, 14, 512)   2048        add_269[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 2, 14, 512)   0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 2, 14, 512)   2359808     activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 2, 14, 512)   2048        conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 2, 14, 512)   0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 2, 14, 512)   2359808     activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_270 (Add)                   (None, 2, 14, 512)   0           add_269[0][0]                    \n",
      "                                                                 conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 2, 14, 512)   2048        add_270[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 2, 14, 512)   0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 512)    0           activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 512)          0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 45)           23085       flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 45)           180         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 45)           0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           1886        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,325,503\n",
      "Trainable params: 21,310,181\n",
      "Non-trainable params: 15,322\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "390/390 [==============================] - 37s 96ms/step - loss: 1.3156 - acc: 0.8712 - val_loss: 0.9211 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85984, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "390/390 [==============================] - 33s 83ms/step - loss: 1.2251 - acc: 0.8881 - val_loss: 0.9366 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.85984\n",
      "Epoch 3/3000\n",
      "390/390 [==============================] - 33s 83ms/step - loss: 1.2078 - acc: 0.8881 - val_loss: 0.8958 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85984 to 0.86253, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1976 - acc: 0.8889 - val_loss: 0.8850 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86253\n",
      "Epoch 5/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1710 - acc: 0.8917 - val_loss: 0.8905 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86253\n",
      "Epoch 6/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1711 - acc: 0.8890 - val_loss: 0.8725 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86253\n",
      "Epoch 7/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1606 - acc: 0.8901 - val_loss: 0.8568 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86253\n",
      "Epoch 8/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1381 - acc: 0.8965 - val_loss: 0.9162 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86253\n",
      "Epoch 9/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1316 - acc: 0.8953 - val_loss: 0.8516 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86253\n",
      "Epoch 10/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1299 - acc: 0.8931 - val_loss: 0.8471 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86253 to 0.87062, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 11/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1208 - acc: 0.8972 - val_loss: 0.8855 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87062\n",
      "Epoch 12/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.1179 - acc: 0.8944 - val_loss: 0.8965 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87062\n",
      "Epoch 13/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0953 - acc: 0.8990 - val_loss: 0.8771 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87062\n",
      "Epoch 14/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0952 - acc: 0.8979 - val_loss: 0.8569 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87062\n",
      "Epoch 15/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0883 - acc: 0.8992 - val_loss: 0.8699 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87062\n",
      "Epoch 16/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0901 - acc: 0.8944 - val_loss: 0.8514 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87062\n",
      "Epoch 17/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0853 - acc: 0.8988 - val_loss: 0.8357 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87062\n",
      "Epoch 18/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0770 - acc: 0.8958 - val_loss: 0.8570 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87062\n",
      "Epoch 19/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0669 - acc: 0.8978 - val_loss: 0.8419 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.87062 to 0.87332, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 20/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0583 - acc: 0.9026 - val_loss: 0.8476 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87332\n",
      "Epoch 21/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0620 - acc: 0.8970 - val_loss: 0.8454 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87332\n",
      "Epoch 22/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0537 - acc: 0.8956 - val_loss: 0.8592 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87332\n",
      "Epoch 23/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0565 - acc: 0.8998 - val_loss: 0.8273 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87332\n",
      "Epoch 24/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0505 - acc: 0.8944 - val_loss: 0.8643 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87332\n",
      "Epoch 25/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0466 - acc: 0.8988 - val_loss: 0.8362 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87332\n",
      "Epoch 26/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0411 - acc: 0.8978 - val_loss: 0.8410 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87332\n",
      "Epoch 27/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0404 - acc: 0.9002 - val_loss: 0.8368 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87332\n",
      "Epoch 28/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0358 - acc: 0.8979 - val_loss: 0.8026 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87332\n",
      "Epoch 29/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0301 - acc: 0.8964 - val_loss: 0.8118 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87332\n",
      "Epoch 30/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0314 - acc: 0.8969 - val_loss: 0.8386 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87332\n",
      "Epoch 31/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0184 - acc: 0.8995 - val_loss: 0.8035 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87332\n",
      "Epoch 32/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0219 - acc: 0.8991 - val_loss: 0.8208 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87332\n",
      "Epoch 33/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0107 - acc: 0.9006 - val_loss: 0.7918 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87332\n",
      "Epoch 34/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0214 - acc: 0.8998 - val_loss: 0.8331 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87332\n",
      "Epoch 35/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0042 - acc: 0.9014 - val_loss: 0.8356 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87332\n",
      "Epoch 36/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0220 - acc: 0.8946 - val_loss: 0.8144 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87332\n",
      "Epoch 37/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0021 - acc: 0.8997 - val_loss: 0.7689 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87332\n",
      "Epoch 38/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9939 - acc: 0.9021 - val_loss: 0.7835 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87332\n",
      "Epoch 39/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0000 - acc: 0.8994 - val_loss: 0.7463 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.87332 to 0.87601, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 40/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0007 - acc: 0.9008 - val_loss: 0.7987 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87601\n",
      "Epoch 41/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0007 - acc: 0.9043 - val_loss: 0.8341 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87601\n",
      "Epoch 42/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 1.0054 - acc: 0.8970 - val_loss: 0.8232 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87601\n",
      "Epoch 43/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9990 - acc: 0.8958 - val_loss: 0.8161 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87601\n",
      "Epoch 44/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9891 - acc: 0.8998 - val_loss: 0.8066 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87601\n",
      "Epoch 45/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9863 - acc: 0.9021 - val_loss: 0.8131 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87601\n",
      "Epoch 46/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9815 - acc: 0.9036 - val_loss: 0.8071 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87601\n",
      "Epoch 47/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9768 - acc: 0.9011 - val_loss: 0.8042 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87601\n",
      "Epoch 48/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9776 - acc: 0.8993 - val_loss: 0.7849 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87601\n",
      "Epoch 49/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9811 - acc: 0.9003 - val_loss: 0.7765 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87601\n",
      "Epoch 50/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9777 - acc: 0.9002 - val_loss: 0.8280 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87601\n",
      "Epoch 51/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9703 - acc: 0.9022 - val_loss: 0.8632 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87601\n",
      "Epoch 52/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9773 - acc: 0.9021 - val_loss: 0.7775 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87601\n",
      "Epoch 53/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9709 - acc: 0.8991 - val_loss: 0.8032 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87601\n",
      "Epoch 54/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9701 - acc: 0.9042 - val_loss: 0.7954 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.87601 to 0.87871, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 55/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9675 - acc: 0.9027 - val_loss: 0.8096 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87871\n",
      "Epoch 56/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9597 - acc: 0.9017 - val_loss: 0.7555 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87871\n",
      "Epoch 57/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9508 - acc: 0.9038 - val_loss: 0.7711 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.87871 to 0.88410, saving model to model/mfcc6/LGD_fold8_co_resnet.h5\n",
      "Epoch 58/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9557 - acc: 0.9016 - val_loss: 0.7523 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88410\n",
      "Epoch 59/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9591 - acc: 0.9030 - val_loss: 0.7937 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88410\n",
      "Epoch 60/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9456 - acc: 0.9006 - val_loss: 0.7868 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88410\n",
      "Epoch 61/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9502 - acc: 0.9056 - val_loss: 0.7844 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88410\n",
      "Epoch 62/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9484 - acc: 0.9009 - val_loss: 0.7387 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88410\n",
      "Epoch 63/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9480 - acc: 0.9003 - val_loss: 0.7797 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88410\n",
      "Epoch 64/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9516 - acc: 0.8984 - val_loss: 0.7546 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88410\n",
      "Epoch 65/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9539 - acc: 0.9012 - val_loss: 0.7771 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88410\n",
      "Epoch 66/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9490 - acc: 0.9027 - val_loss: 0.7713 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88410\n",
      "Epoch 67/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9440 - acc: 0.9066 - val_loss: 0.7728 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88410\n",
      "Epoch 68/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9386 - acc: 0.9050 - val_loss: 0.7058 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88410\n",
      "Epoch 69/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9385 - acc: 0.9014 - val_loss: 0.7185 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88410\n",
      "Epoch 70/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9338 - acc: 0.8974 - val_loss: 0.7303 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88410\n",
      "Epoch 71/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9305 - acc: 0.9027 - val_loss: 0.7817 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88410\n",
      "Epoch 72/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9287 - acc: 0.8986 - val_loss: 0.8204 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88410\n",
      "Epoch 73/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9368 - acc: 0.9031 - val_loss: 0.7946 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88410\n",
      "Epoch 74/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9386 - acc: 0.9014 - val_loss: 0.8032 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88410\n",
      "Epoch 75/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9362 - acc: 0.9034 - val_loss: 0.7348 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88410\n",
      "Epoch 76/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9328 - acc: 0.9010 - val_loss: 0.7658 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88410\n",
      "Epoch 77/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9220 - acc: 0.9028 - val_loss: 0.7558 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88410\n",
      "Epoch 78/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9194 - acc: 0.9032 - val_loss: 0.7697 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88410\n",
      "Epoch 79/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9284 - acc: 0.9019 - val_loss: 0.7840 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88410\n",
      "Epoch 80/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9310 - acc: 0.9035 - val_loss: 0.7855 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88410\n",
      "Epoch 81/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9178 - acc: 0.9044 - val_loss: 0.7730 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88410\n",
      "Epoch 82/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9140 - acc: 0.9075 - val_loss: 0.7907 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88410\n",
      "Epoch 83/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9185 - acc: 0.8990 - val_loss: 0.7763 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88410\n",
      "Epoch 84/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9223 - acc: 0.9026 - val_loss: 0.7839 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88410\n",
      "Epoch 85/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9142 - acc: 0.9021 - val_loss: 0.7911 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88410\n",
      "Epoch 86/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9228 - acc: 0.8998 - val_loss: 0.7352 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88410\n",
      "Epoch 87/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9075 - acc: 0.9021 - val_loss: 0.8053 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88410\n",
      "Epoch 88/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9141 - acc: 0.9078 - val_loss: 0.8692 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88410\n",
      "Epoch 89/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9087 - acc: 0.9058 - val_loss: 0.8006 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88410\n",
      "Epoch 90/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9094 - acc: 0.9021 - val_loss: 0.8134 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88410\n",
      "Epoch 91/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9081 - acc: 0.9035 - val_loss: 0.7681 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88410\n",
      "Epoch 92/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9168 - acc: 0.9038 - val_loss: 0.7583 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88410\n",
      "Epoch 93/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9169 - acc: 0.9042 - val_loss: 0.7768 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88410\n",
      "Epoch 94/3000\n",
      "390/390 [==============================] - 33s 84ms/step - loss: 0.9153 - acc: 0.8997 - val_loss: 0.8133 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88410\n",
      "Epoch 95/3000\n",
      "390/390 [==============================] - 32s 83ms/step - loss: 0.9067 - acc: 0.9057 - val_loss: 0.7713 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88410\n",
      "Epoch 96/3000\n",
      "390/390 [==============================] - 32s 83ms/step - loss: 0.8865 - acc: 0.9075 - val_loss: 0.7769 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88410\n",
      "Epoch 97/3000\n",
      "390/390 [==============================] - 32s 83ms/step - loss: 0.8802 - acc: 0.9085 - val_loss: 0.7720 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88410\n",
      "Epoch 98/3000\n",
      "390/390 [==============================] - 32s 83ms/step - loss: 0.8841 - acc: 0.9088 - val_loss: 0.7915 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88410\n",
      "Epoch 00098: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_7===\n",
      "semi loading: model/mfcc6/LGD_semi_fold7_resnet2.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 32, 216, 64)  3200        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 32, 216, 64)  256         conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 32, 216, 64)  0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 16, 108, 64)  4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 16, 108, 64)  256         conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 16, 108, 64)  0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 16, 108, 64)  36928       activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 16, 108, 64)  256         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 16, 108, 64)  0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 16, 108, 256) 16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 16, 108, 256) 16640       activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 16, 108, 256) 0           conv2d_429[0][0]                 \n",
      "                                                                 conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 16, 108, 256) 1024        add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 16, 108, 256) 0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 16, 108, 64)  16448       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 16, 108, 64)  256         conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 16, 108, 64)  0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 16, 108, 64)  36928       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 16, 108, 64)  256         conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 16, 108, 64)  0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 16, 108, 256) 16640       activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 16, 108, 256) 0           add_148[0][0]                    \n",
      "                                                                 conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 16, 108, 256) 1024        add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 16, 108, 256) 0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 16, 108, 64)  16448       activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 16, 108, 64)  256         conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 16, 108, 64)  0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 16, 108, 64)  36928       activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 16, 108, 64)  256         conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 16, 108, 64)  0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 16, 108, 256) 16640       activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 16, 108, 256) 0           add_149[0][0]                    \n",
      "                                                                 conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 16, 108, 256) 1024        add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 16, 108, 256) 0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 8, 54, 128)   32896       activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 8, 54, 128)   512         conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 8, 54, 128)   0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 8, 54, 128)   147584      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 8, 54, 128)   512         conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 8, 54, 128)   0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 8, 54, 512)   131584      add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 8, 54, 512)   66048       activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 8, 54, 512)   0           conv2d_439[0][0]                 \n",
      "                                                                 conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 8, 54, 512)   2048        add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 8, 54, 512)   0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 8, 54, 128)   65664       activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 8, 54, 128)   512         conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 8, 54, 128)   0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 8, 54, 128)   147584      activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 8, 54, 128)   512         conv2d_441[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 8, 54, 128)   0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 8, 54, 512)   66048       activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_152 (Add)                   (None, 8, 54, 512)   0           add_151[0][0]                    \n",
      "                                                                 conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 8, 54, 512)   2048        add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 8, 54, 512)   0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 8, 54, 128)   65664       activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 8, 54, 128)   512         conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 8, 54, 128)   0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 8, 54, 128)   147584      activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 8, 54, 128)   512         conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 8, 54, 128)   0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 8, 54, 512)   66048       activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_153 (Add)                   (None, 8, 54, 512)   0           add_152[0][0]                    \n",
      "                                                                 conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 8, 54, 512)   2048        add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 8, 54, 512)   0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 8, 54, 128)   65664       activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 8, 54, 128)   512         conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 8, 54, 128)   0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 8, 54, 128)   147584      activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 8, 54, 128)   512         conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 8, 54, 128)   0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 8, 54, 512)   66048       activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_154 (Add)                   (None, 8, 54, 512)   0           add_153[0][0]                    \n",
      "                                                                 conv2d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 8, 54, 512)   2048        add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 8, 54, 512)   0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 4, 27, 256)   131328      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 4, 27, 256)   1024        conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 4, 27, 256)   0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 4, 27, 256)   590080      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 4, 27, 256)   1024        conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 4, 27, 256)   0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 4, 27, 1024)  525312      add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 4, 27, 1024)  263168      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_155 (Add)                   (None, 4, 27, 1024)  0           conv2d_452[0][0]                 \n",
      "                                                                 conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 4, 27, 1024)  4096        add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 4, 27, 256)   262400      activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 4, 27, 256)   1024        conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 4, 27, 256)   0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 4, 27, 256)   590080      activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 4, 27, 256)   1024        conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 4, 27, 256)   0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 4, 27, 1024)  263168      activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_156 (Add)                   (None, 4, 27, 1024)  0           add_155[0][0]                    \n",
      "                                                                 conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 4, 27, 1024)  4096        add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 4, 27, 256)   262400      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 4, 27, 256)   1024        conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 4, 27, 256)   0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 4, 27, 256)   590080      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 4, 27, 256)   1024        conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 4, 27, 256)   0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 4, 27, 1024)  263168      activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, 4, 27, 1024)  0           add_156[0][0]                    \n",
      "                                                                 conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 4, 27, 1024)  4096        add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 4, 27, 256)   262400      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 4, 27, 256)   1024        conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 4, 27, 256)   0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 4, 27, 256)   590080      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 4, 27, 256)   1024        conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 4, 27, 256)   0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 4, 27, 1024)  263168      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, 4, 27, 1024)  0           add_157[0][0]                    \n",
      "                                                                 conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 4, 27, 1024)  4096        add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 4, 27, 256)   262400      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 4, 27, 256)   1024        conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 4, 27, 256)   0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 4, 27, 256)   590080      activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 4, 27, 256)   1024        conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 4, 27, 256)   0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 4, 27, 1024)  263168      activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_159 (Add)                   (None, 4, 27, 1024)  0           add_158[0][0]                    \n",
      "                                                                 conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 4, 27, 1024)  4096        add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 4, 27, 256)   262400      activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 4, 27, 256)   1024        conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 4, 27, 256)   0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 4, 27, 256)   590080      activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 4, 27, 256)   1024        conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 4, 27, 256)   0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 4, 27, 1024)  263168      activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, 4, 27, 1024)  0           add_159[0][0]                    \n",
      "                                                                 conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 4, 27, 1024)  4096        add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 2, 14, 512)   524800      activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 2, 14, 512)   2048        conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 2, 14, 512)   0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 2, 14, 512)   2359808     activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 2, 14, 512)   2048        conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 2, 14, 512)   0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 2, 14, 2048)  2099200     add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, 2, 14, 2048)  0           conv2d_471[0][0]                 \n",
      "                                                                 conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 2, 14, 2048)  8192        add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 2, 14, 512)   1049088     activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 2, 14, 512)   2048        conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 2, 14, 512)   0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 2, 14, 512)   2359808     activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 2, 14, 512)   2048        conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 2, 14, 512)   0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, 2, 14, 2048)  0           add_161[0][0]                    \n",
      "                                                                 conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 2, 14, 2048)  8192        add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 2, 14, 512)   1049088     activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 2, 14, 512)   2048        conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 2, 14, 512)   0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 2, 14, 512)   2359808     activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 2, 14, 512)   2048        conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 2, 14, 512)   0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, 2, 14, 2048)  0           add_162[0][0]                    \n",
      "                                                                 conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 2, 14, 2048)  8192        add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1, 1, 2048)   0           activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 45)           92205       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 45)           180         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 45)           0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 41)           1886        dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 23,660,351\n",
      "Trainable params: 23,614,821\n",
      "Non-trainable params: 45,530\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "195/195 [==============================] - 54s 274ms/step - loss: 1.4623 - acc: 0.8401 - val_loss: 0.8603 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85445, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "195/195 [==============================] - 46s 237ms/step - loss: 1.4106 - acc: 0.8461 - val_loss: 0.8707 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85445 to 0.85714, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 3/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3995 - acc: 0.8476 - val_loss: 0.8238 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85714 to 0.86523, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3922 - acc: 0.8494 - val_loss: 0.8164 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86523\n",
      "Epoch 5/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3860 - acc: 0.8487 - val_loss: 0.8337 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86523\n",
      "Epoch 6/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3763 - acc: 0.8474 - val_loss: 0.8139 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86523 to 0.86792, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 7/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3708 - acc: 0.8470 - val_loss: 0.8416 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86792\n",
      "Epoch 8/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3545 - acc: 0.8560 - val_loss: 0.8279 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86792\n",
      "Epoch 9/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3602 - acc: 0.8442 - val_loss: 0.8337 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86792\n",
      "Epoch 10/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3452 - acc: 0.8511 - val_loss: 0.8149 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86792\n",
      "Epoch 11/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3489 - acc: 0.8490 - val_loss: 0.8353 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86792\n",
      "Epoch 12/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3401 - acc: 0.8531 - val_loss: 0.8192 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.86792 to 0.87332, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 13/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3317 - acc: 0.8486 - val_loss: 0.8157 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87332\n",
      "Epoch 14/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3382 - acc: 0.8527 - val_loss: 0.8300 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87332\n",
      "Epoch 15/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3343 - acc: 0.8500 - val_loss: 0.8195 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87332\n",
      "Epoch 16/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3222 - acc: 0.8601 - val_loss: 0.8086 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87332\n",
      "Epoch 17/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3429 - acc: 0.8501 - val_loss: 0.8353 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87332\n",
      "Epoch 18/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3032 - acc: 0.8536 - val_loss: 0.8109 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87332\n",
      "Epoch 19/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3162 - acc: 0.8578 - val_loss: 0.8300 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87332\n",
      "Epoch 20/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3118 - acc: 0.8612 - val_loss: 0.8355 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87332\n",
      "Epoch 21/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3110 - acc: 0.8515 - val_loss: 0.8201 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87332\n",
      "Epoch 22/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3115 - acc: 0.8574 - val_loss: 0.7962 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87332\n",
      "Epoch 23/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3046 - acc: 0.8533 - val_loss: 0.7986 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.87332 to 0.87601, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 24/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2910 - acc: 0.8590 - val_loss: 0.8250 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87601\n",
      "Epoch 25/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.3029 - acc: 0.8508 - val_loss: 0.8446 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87601\n",
      "Epoch 26/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2898 - acc: 0.8521 - val_loss: 0.8008 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87601\n",
      "Epoch 27/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2855 - acc: 0.8576 - val_loss: 0.7678 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.87601 to 0.88679, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 28/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2888 - acc: 0.8500 - val_loss: 0.7741 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88679\n",
      "Epoch 29/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2826 - acc: 0.8556 - val_loss: 0.7763 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88679\n",
      "Epoch 30/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2948 - acc: 0.8465 - val_loss: 0.7838 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88679\n",
      "Epoch 31/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2964 - acc: 0.8539 - val_loss: 0.8232 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88679\n",
      "Epoch 32/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2750 - acc: 0.8558 - val_loss: 0.7952 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88679\n",
      "Epoch 33/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2827 - acc: 0.8518 - val_loss: 0.8072 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88679\n",
      "Epoch 34/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2781 - acc: 0.8522 - val_loss: 0.8005 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88679\n",
      "Epoch 35/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2737 - acc: 0.8538 - val_loss: 0.7924 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88679\n",
      "Epoch 36/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2749 - acc: 0.8554 - val_loss: 0.7970 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88679\n",
      "Epoch 37/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2711 - acc: 0.8570 - val_loss: 0.8205 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88679\n",
      "Epoch 38/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2660 - acc: 0.8545 - val_loss: 0.7977 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88679\n",
      "Epoch 39/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2650 - acc: 0.8546 - val_loss: 0.7933 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88679\n",
      "Epoch 40/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2631 - acc: 0.8614 - val_loss: 0.7846 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88679\n",
      "Epoch 41/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2581 - acc: 0.8559 - val_loss: 0.8119 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88679\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2588 - acc: 0.8522 - val_loss: 0.8064 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88679\n",
      "Epoch 43/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2450 - acc: 0.8578 - val_loss: 0.7956 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88679\n",
      "Epoch 44/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2478 - acc: 0.8589 - val_loss: 0.7742 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88679\n",
      "Epoch 45/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2575 - acc: 0.8620 - val_loss: 0.7865 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88679\n",
      "Epoch 46/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2497 - acc: 0.8619 - val_loss: 0.8105 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88679\n",
      "Epoch 47/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2526 - acc: 0.8571 - val_loss: 0.8133 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88679\n",
      "Epoch 48/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2456 - acc: 0.8549 - val_loss: 0.7868 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88679\n",
      "Epoch 49/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2446 - acc: 0.8568 - val_loss: 0.7820 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88679\n",
      "Epoch 50/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2520 - acc: 0.8571 - val_loss: 0.7868 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88679\n",
      "Epoch 51/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2496 - acc: 0.8511 - val_loss: 0.7881 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88679\n",
      "Epoch 52/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2432 - acc: 0.8558 - val_loss: 0.7673 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88679\n",
      "Epoch 53/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2450 - acc: 0.8505 - val_loss: 0.7922 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88679\n",
      "Epoch 54/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2368 - acc: 0.8550 - val_loss: 0.7738 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88679\n",
      "Epoch 55/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2273 - acc: 0.8571 - val_loss: 0.7659 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88679\n",
      "Epoch 56/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2340 - acc: 0.8601 - val_loss: 0.7766 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88679\n",
      "Epoch 57/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2317 - acc: 0.8559 - val_loss: 0.7722 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88679\n",
      "Epoch 58/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2343 - acc: 0.8556 - val_loss: 0.7609 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88679\n",
      "Epoch 59/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2215 - acc: 0.8611 - val_loss: 0.7630 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88679\n",
      "Epoch 60/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2296 - acc: 0.8557 - val_loss: 0.7646 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88679\n",
      "Epoch 61/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2321 - acc: 0.8579 - val_loss: 0.7455 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88679\n",
      "Epoch 62/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2170 - acc: 0.8646 - val_loss: 0.7536 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88679\n",
      "Epoch 63/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2307 - acc: 0.8553 - val_loss: 0.7377 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88679\n",
      "Epoch 64/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2268 - acc: 0.8568 - val_loss: 0.7509 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88679\n",
      "Epoch 65/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2285 - acc: 0.8565 - val_loss: 0.7624 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88679\n",
      "Epoch 66/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2164 - acc: 0.8638 - val_loss: 0.7815 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88679\n",
      "Epoch 67/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2219 - acc: 0.8585 - val_loss: 0.7459 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88679\n",
      "Epoch 68/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2152 - acc: 0.8619 - val_loss: 0.7728 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88679\n",
      "Epoch 69/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2105 - acc: 0.8645 - val_loss: 0.7471 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88679\n",
      "Epoch 70/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2118 - acc: 0.8554 - val_loss: 0.7497 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88679\n",
      "Epoch 71/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2202 - acc: 0.8583 - val_loss: 0.7665 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88679\n",
      "Epoch 72/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2189 - acc: 0.8549 - val_loss: 0.7512 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88679\n",
      "Epoch 73/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2144 - acc: 0.8579 - val_loss: 0.7467 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88679\n",
      "Epoch 74/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2083 - acc: 0.8559 - val_loss: 0.7647 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88679\n",
      "Epoch 75/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2108 - acc: 0.8584 - val_loss: 0.7633 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88679\n",
      "Epoch 76/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2028 - acc: 0.8549 - val_loss: 0.7569 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88679\n",
      "Epoch 77/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2071 - acc: 0.8602 - val_loss: 0.7426 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88679\n",
      "Epoch 78/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2121 - acc: 0.8563 - val_loss: 0.7579 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88679\n",
      "Epoch 79/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2050 - acc: 0.8599 - val_loss: 0.7336 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88679\n",
      "Epoch 80/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1936 - acc: 0.8598 - val_loss: 0.7380 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88679\n",
      "Epoch 81/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2022 - acc: 0.8566 - val_loss: 0.7922 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88679\n",
      "Epoch 82/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2008 - acc: 0.8628 - val_loss: 0.7250 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88679\n",
      "Epoch 83/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1964 - acc: 0.8607 - val_loss: 0.7507 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88679\n",
      "Epoch 84/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2000 - acc: 0.8550 - val_loss: 0.7471 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88679\n",
      "Epoch 85/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1948 - acc: 0.8562 - val_loss: 0.7295 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88679\n",
      "Epoch 86/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.2009 - acc: 0.8546 - val_loss: 0.7395 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88679\n",
      "Epoch 87/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1919 - acc: 0.8598 - val_loss: 0.7392 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88679\n",
      "Epoch 88/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1931 - acc: 0.8581 - val_loss: 0.7494 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88679\n",
      "Epoch 89/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1929 - acc: 0.8575 - val_loss: 0.7710 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88679\n",
      "Epoch 90/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1841 - acc: 0.8652 - val_loss: 0.7452 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88679\n",
      "Epoch 91/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1871 - acc: 0.8615 - val_loss: 0.7587 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88679\n",
      "Epoch 92/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1894 - acc: 0.8575 - val_loss: 0.7521 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88679\n",
      "Epoch 93/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1936 - acc: 0.8587 - val_loss: 0.7379 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88679\n",
      "Epoch 94/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1764 - acc: 0.8606 - val_loss: 0.7512 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88679\n",
      "Epoch 95/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1836 - acc: 0.8592 - val_loss: 0.7350 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88679\n",
      "Epoch 96/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1762 - acc: 0.8600 - val_loss: 0.7466 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88679\n",
      "Epoch 97/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1927 - acc: 0.8561 - val_loss: 0.7453 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88679\n",
      "Epoch 98/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1867 - acc: 0.8587 - val_loss: 0.7108 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88679\n",
      "Epoch 99/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1738 - acc: 0.8569 - val_loss: 0.7584 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.88679\n",
      "Epoch 100/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1764 - acc: 0.8621 - val_loss: 0.7405 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.88679\n",
      "Epoch 101/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1746 - acc: 0.8594 - val_loss: 0.7378 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.88679\n",
      "Epoch 102/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1728 - acc: 0.8593 - val_loss: 0.7375 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.88679\n",
      "Epoch 103/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1759 - acc: 0.8645 - val_loss: 0.7437 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.88679\n",
      "Epoch 104/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1791 - acc: 0.8616 - val_loss: 0.7120 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.88679\n",
      "Epoch 105/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1639 - acc: 0.8608 - val_loss: 0.7005 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.88679\n",
      "Epoch 106/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1732 - acc: 0.8631 - val_loss: 0.7358 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.88679\n",
      "Epoch 107/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1705 - acc: 0.8617 - val_loss: 0.7427 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.88679\n",
      "Epoch 108/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1694 - acc: 0.8612 - val_loss: 0.7264 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.88679\n",
      "Epoch 109/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1686 - acc: 0.8615 - val_loss: 0.7633 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.88679\n",
      "Epoch 110/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1776 - acc: 0.8632 - val_loss: 0.7402 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00110: val_acc improved from 0.88679 to 0.88949, saving model to model/mfcc6/LGD_fold7_co_resnet.h5\n",
      "Epoch 111/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1676 - acc: 0.8578 - val_loss: 0.7335 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.88949\n",
      "Epoch 112/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1702 - acc: 0.8671 - val_loss: 0.7761 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.88949\n",
      "Epoch 113/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1532 - acc: 0.8636 - val_loss: 0.7274 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.88949\n",
      "Epoch 114/3000\n",
      "195/195 [==============================] - 47s 239ms/step - loss: 1.1613 - acc: 0.8603 - val_loss: 0.7356 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.88949\n",
      "Epoch 115/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1594 - acc: 0.8612 - val_loss: 0.7087 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.88949\n",
      "Epoch 116/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1605 - acc: 0.8640 - val_loss: 0.7322 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.88949\n",
      "Epoch 117/3000\n",
      "195/195 [==============================] - 47s 238ms/step - loss: 1.1448 - acc: 0.8628 - val_loss: 0.7173 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.88949\n",
      "Epoch 118/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1386 - acc: 0.8660 - val_loss: 0.7195 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.88949\n",
      "Epoch 119/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1665 - acc: 0.8593 - val_loss: 0.7060 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.88949\n",
      "Epoch 120/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1541 - acc: 0.8619 - val_loss: 0.7147 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.88949\n",
      "Epoch 121/3000\n",
      "195/195 [==============================] - 47s 239ms/step - loss: 1.1531 - acc: 0.8633 - val_loss: 0.7168 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.88949\n",
      "Epoch 122/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1562 - acc: 0.8619 - val_loss: 0.7058 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.88949\n",
      "Epoch 123/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1530 - acc: 0.8543 - val_loss: 0.7036 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.88949\n",
      "Epoch 124/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1456 - acc: 0.8604 - val_loss: 0.7039 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.88949\n",
      "Epoch 125/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1538 - acc: 0.8638 - val_loss: 0.7133 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.88949\n",
      "Epoch 126/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1359 - acc: 0.8681 - val_loss: 0.7242 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.88949\n",
      "Epoch 127/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1435 - acc: 0.8606 - val_loss: 0.7142 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.88949\n",
      "Epoch 128/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1319 - acc: 0.8654 - val_loss: 0.7055 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.88949\n",
      "Epoch 129/3000\n",
      "195/195 [==============================] - 47s 239ms/step - loss: 1.1437 - acc: 0.8582 - val_loss: 0.7174 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.88949\n",
      "Epoch 130/3000\n",
      "195/195 [==============================] - 47s 238ms/step - loss: 1.1402 - acc: 0.8667 - val_loss: 0.7105 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.88949\n",
      "Epoch 131/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1417 - acc: 0.8553 - val_loss: 0.7143 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.88949\n",
      "Epoch 132/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1473 - acc: 0.8632 - val_loss: 0.7146 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.88949\n",
      "Epoch 133/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1376 - acc: 0.8619 - val_loss: 0.7037 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.88949\n",
      "Epoch 134/3000\n",
      "195/195 [==============================] - 46s 238ms/step - loss: 1.1450 - acc: 0.8598 - val_loss: 0.7012 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.88949\n",
      "Epoch 135/3000\n",
      "195/195 [==============================] - 47s 239ms/step - loss: 1.1493 - acc: 0.8583 - val_loss: 0.7023 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.88949\n",
      "Epoch 00135: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_6===\n",
      "semi loading: model/mfcc6/LGD_semi_fold6_resnet4.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 32, 216, 64)  3200        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 32, 216, 64)  256         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 32, 216, 64)  0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 16, 108, 64)  4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 16, 108, 64)  256         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 16, 108, 64)  0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 16, 108, 64)  36928       activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 16, 108, 64)  256         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 16, 108, 64)  0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 16, 108, 256) 16640       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 16, 108, 256) 16640       activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 16, 108, 256) 0           conv2d_218[0][0]                 \n",
      "                                                                 conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 16, 108, 256) 1024        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 16, 108, 256) 0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 16, 108, 64)  16448       activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 16, 108, 64)  256         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 16, 108, 64)  0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 16, 108, 64)  36928       activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 16, 108, 64)  256         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 16, 108, 64)  0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 16, 108, 256) 16640       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 108, 256) 0           add_74[0][0]                     \n",
      "                                                                 conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 16, 108, 256) 1024        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 16, 108, 256) 0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 16, 108, 64)  16448       activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 16, 108, 64)  256         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 16, 108, 64)  0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 16, 108, 64)  36928       activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 16, 108, 64)  256         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 16, 108, 64)  0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 16, 108, 256) 16640       activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 16, 108, 256) 0           add_75[0][0]                     \n",
      "                                                                 conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 16, 108, 256) 1024        add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 16, 108, 256) 0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 8, 54, 128)   32896       activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 8, 54, 128)   512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 8, 54, 128)   0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 8, 54, 128)   147584      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 8, 54, 128)   512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 8, 54, 128)   0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 8, 54, 512)   131584      add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 8, 54, 512)   66048       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 8, 54, 512)   0           conv2d_228[0][0]                 \n",
      "                                                                 conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 8, 54, 512)   2048        add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 8, 54, 512)   0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 8, 54, 128)   65664       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 54, 128)   512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 8, 54, 128)   0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 8, 54, 128)   147584      activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 8, 54, 128)   512         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 8, 54, 128)   0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 8, 54, 512)   66048       activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 8, 54, 512)   0           add_77[0][0]                     \n",
      "                                                                 conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 8, 54, 512)   2048        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 8, 54, 512)   0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 8, 54, 128)   65664       activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 8, 54, 128)   512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 8, 54, 128)   0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 8, 54, 128)   147584      activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 8, 54, 128)   512         conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 8, 54, 128)   0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 8, 54, 512)   66048       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 8, 54, 512)   0           add_78[0][0]                     \n",
      "                                                                 conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 8, 54, 512)   2048        add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 8, 54, 512)   0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 8, 54, 128)   65664       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 8, 54, 128)   512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 8, 54, 128)   0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 8, 54, 128)   147584      activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 8, 54, 128)   512         conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 8, 54, 128)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 8, 54, 512)   66048       activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 8, 54, 512)   0           add_79[0][0]                     \n",
      "                                                                 conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 8, 54, 512)   2048        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 8, 54, 512)   0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 8, 54, 128)   65664       activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 8, 54, 128)   512         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 8, 54, 128)   0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 8, 54, 128)   147584      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 8, 54, 128)   512         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 8, 54, 128)   0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 8, 54, 512)   66048       activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 8, 54, 512)   0           add_80[0][0]                     \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 8, 54, 512)   2048        add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, 8, 54, 512)   0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 8, 54, 128)   65664       activation_224[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 8, 54, 128)   512         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, 8, 54, 128)   0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 8, 54, 128)   147584      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 8, 54, 128)   512         conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, 8, 54, 128)   0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 8, 54, 512)   66048       activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 8, 54, 512)   0           add_81[0][0]                     \n",
      "                                                                 conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 8, 54, 512)   2048        add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, 8, 54, 512)   0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 8, 54, 128)   65664       activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 8, 54, 128)   512         conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, 8, 54, 128)   0           batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 8, 54, 128)   147584      activation_228[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 8, 54, 128)   512         conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, 8, 54, 128)   0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 8, 54, 512)   66048       activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 8, 54, 512)   0           add_82[0][0]                     \n",
      "                                                                 conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 8, 54, 512)   2048        add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, 8, 54, 512)   0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 8, 54, 128)   65664       activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 8, 54, 128)   512         conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, 8, 54, 128)   0           batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 8, 54, 128)   147584      activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 8, 54, 128)   512         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, 8, 54, 128)   0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 8, 54, 512)   66048       activation_232[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 8, 54, 512)   0           add_83[0][0]                     \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 8, 54, 512)   2048        add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, 8, 54, 512)   0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 4, 27, 256)   131328      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 4, 27, 256)   1024        conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, 4, 27, 256)   0           batch_normalization_238[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 4, 27, 256)   590080      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 4, 27, 256)   1024        conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, 4, 27, 256)   0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 4, 27, 1024)  525312      add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 4, 27, 1024)  263168      activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 4, 27, 1024)  0           conv2d_253[0][0]                 \n",
      "                                                                 conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 4, 27, 1024)  4096        add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 4, 27, 256)   262400      activation_236[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 4, 27, 256)   1024        conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, 4, 27, 256)   0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 4, 27, 256)   590080      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 4, 27, 256)   1024        conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, 4, 27, 256)   0           batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 4, 27, 1024)  263168      activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 4, 27, 1024)  0           add_85[0][0]                     \n",
      "                                                                 conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 4, 27, 1024)  4096        add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 4, 27, 256)   262400      activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 4, 27, 256)   1024        conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, 4, 27, 256)   0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 4, 27, 256)   590080      activation_240[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 4, 27, 256)   1024        conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, 4, 27, 256)   0           batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 4, 27, 1024)  263168      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 4, 27, 1024)  0           add_86[0][0]                     \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 4, 27, 1024)  4096        add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 4, 27, 256)   262400      activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 4, 27, 256)   1024        conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 4, 27, 256)   0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 4, 27, 256)   590080      activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 4, 27, 256)   1024        conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 4, 27, 256)   0           batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, 4, 27, 1024)  263168      activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 4, 27, 1024)  0           add_87[0][0]                     \n",
      "                                                                 conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchN (None, 4, 27, 1024)  4096        add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_249[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 4, 27, 256)   262400      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 4, 27, 256)   1024        conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 4, 27, 256)   0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 4, 27, 256)   590080      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 4, 27, 256)   1024        conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 4, 27, 256)   0           batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, 4, 27, 1024)  263168      activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 4, 27, 1024)  0           add_88[0][0]                     \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchN (None, 4, 27, 1024)  4096        add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_252[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, 4, 27, 256)   262400      activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 4, 27, 256)   1024        conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 4, 27, 256)   0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, 4, 27, 256)   590080      activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 4, 27, 256)   1024        conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 4, 27, 256)   0           batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, 4, 27, 1024)  263168      activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 4, 27, 1024)  0           add_89[0][0]                     \n",
      "                                                                 conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchN (None, 4, 27, 1024)  4096        add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_255[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, 4, 27, 256)   262400      activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 4, 27, 256)   1024        conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 4, 27, 256)   0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, 4, 27, 256)   590080      activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 4, 27, 256)   1024        conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 4, 27, 256)   0           batch_normalization_257[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, 4, 27, 1024)  263168      activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 4, 27, 1024)  0           add_90[0][0]                     \n",
      "                                                                 conv2d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 4, 27, 1024)  4096        add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, 4, 27, 256)   262400      activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchN (None, 4, 27, 256)   1024        conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 4, 27, 256)   0           batch_normalization_259[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, 4, 27, 256)   590080      activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 4, 27, 256)   1024        conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 4, 27, 256)   0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, 4, 27, 1024)  263168      activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 4, 27, 1024)  0           add_91[0][0]                     \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 4, 27, 1024)  4096        add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, 4, 27, 256)   262400      activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchN (None, 4, 27, 256)   1024        conv2d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 4, 27, 256)   0           batch_normalization_262[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, 4, 27, 256)   590080      activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 4, 27, 256)   1024        conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 4, 27, 256)   0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, 4, 27, 1024)  263168      activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 4, 27, 1024)  0           add_92[0][0]                     \n",
      "                                                                 conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 4, 27, 1024)  4096        add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, 4, 27, 256)   262400      activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchN (None, 4, 27, 256)   1024        conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 4, 27, 256)   0           batch_normalization_265[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 4, 27, 256)   590080      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchN (None, 4, 27, 256)   1024        conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 4, 27, 256)   0           batch_normalization_266[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 4, 27, 1024)  263168      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 4, 27, 1024)  0           add_93[0][0]                     \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchN (None, 4, 27, 1024)  4096        add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_267[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, 4, 27, 256)   262400      activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchN (None, 4, 27, 256)   1024        conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 4, 27, 256)   0           batch_normalization_268[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 4, 27, 256)   590080      activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchN (None, 4, 27, 256)   1024        conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, 4, 27, 256)   0           batch_normalization_269[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)             (None, 4, 27, 1024)  263168      activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 4, 27, 1024)  0           add_94[0][0]                     \n",
      "                                                                 conv2d_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchN (None, 4, 27, 1024)  4096        add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_270[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)             (None, 4, 27, 256)   262400      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchN (None, 4, 27, 256)   1024        conv2d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, 4, 27, 256)   0           batch_normalization_271[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 4, 27, 256)   590080      activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchN (None, 4, 27, 256)   1024        conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, 4, 27, 256)   0           batch_normalization_272[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 4, 27, 1024)  263168      activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 4, 27, 1024)  0           add_95[0][0]                     \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchN (None, 4, 27, 1024)  4096        add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_273[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 4, 27, 256)   262400      activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchN (None, 4, 27, 256)   1024        conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, 4, 27, 256)   0           batch_normalization_274[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 4, 27, 256)   590080      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchN (None, 4, 27, 256)   1024        conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, 4, 27, 256)   0           batch_normalization_275[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 4, 27, 1024)  263168      activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 4, 27, 1024)  0           add_96[0][0]                     \n",
      "                                                                 conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchN (None, 4, 27, 1024)  4096        add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_276[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 4, 27, 256)   262400      activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchN (None, 4, 27, 256)   1024        conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, 4, 27, 256)   0           batch_normalization_277[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 4, 27, 256)   590080      activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchN (None, 4, 27, 256)   1024        conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, 4, 27, 256)   0           batch_normalization_278[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 4, 27, 1024)  263168      activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 4, 27, 1024)  0           add_97[0][0]                     \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchN (None, 4, 27, 1024)  4096        add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_279[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 4, 27, 256)   262400      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchN (None, 4, 27, 256)   1024        conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, 4, 27, 256)   0           batch_normalization_280[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 4, 27, 256)   590080      activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 4, 27, 256)   1024        conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, 4, 27, 256)   0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 4, 27, 1024)  263168      activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 4, 27, 1024)  0           add_98[0][0]                     \n",
      "                                                                 conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 4, 27, 1024)  4096        add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 4, 27, 256)   262400      activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 4, 27, 256)   1024        conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, 4, 27, 256)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 4, 27, 256)   590080      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 4, 27, 256)   1024        conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 4, 27, 256)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 4, 27, 1024)  263168      activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 4, 27, 1024)  0           add_99[0][0]                     \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 4, 27, 1024)  4096        add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 4, 27, 256)   262400      activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 4, 27, 256)   1024        conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 4, 27, 256)   0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 4, 27, 256)   590080      activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 4, 27, 256)   1024        conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 4, 27, 256)   0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 4, 27, 1024)  263168      activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 4, 27, 1024)  0           add_100[0][0]                    \n",
      "                                                                 conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 4, 27, 1024)  4096        add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 4, 27, 256)   262400      activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 4, 27, 256)   1024        conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 4, 27, 256)   0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 4, 27, 256)   590080      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 4, 27, 256)   1024        conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 4, 27, 256)   0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 4, 27, 1024)  263168      activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 4, 27, 1024)  0           add_101[0][0]                    \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 4, 27, 1024)  4096        add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 4, 27, 256)   262400      activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 4, 27, 256)   1024        conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 4, 27, 256)   0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 4, 27, 256)   590080      activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 4, 27, 256)   1024        conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 4, 27, 256)   0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 4, 27, 1024)  263168      activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 4, 27, 1024)  0           add_102[0][0]                    \n",
      "                                                                 conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 4, 27, 1024)  4096        add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 4, 27, 256)   262400      activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 4, 27, 256)   1024        conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 4, 27, 256)   0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 4, 27, 256)   590080      activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 4, 27, 256)   1024        conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 4, 27, 256)   0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 4, 27, 1024)  263168      activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 4, 27, 1024)  0           add_103[0][0]                    \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 4, 27, 1024)  4096        add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 4, 27, 256)   262400      activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 4, 27, 256)   1024        conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 4, 27, 256)   0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 4, 27, 256)   590080      activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 4, 27, 256)   1024        conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 4, 27, 256)   0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 4, 27, 1024)  263168      activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 4, 27, 1024)  0           add_104[0][0]                    \n",
      "                                                                 conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 4, 27, 1024)  4096        add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 4, 27, 256)   262400      activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 4, 27, 256)   1024        conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 4, 27, 256)   0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 4, 27, 256)   590080      activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 4, 27, 256)   1024        conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 4, 27, 256)   0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 4, 27, 1024)  263168      activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 4, 27, 1024)  0           add_105[0][0]                    \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 4, 27, 1024)  4096        add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 4, 27, 256)   262400      activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 4, 27, 256)   1024        conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 4, 27, 256)   0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 4, 27, 256)   590080      activation_300[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 4, 27, 256)   1024        conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 4, 27, 256)   0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 4, 27, 1024)  263168      activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 4, 27, 1024)  0           add_106[0][0]                    \n",
      "                                                                 conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 4, 27, 1024)  4096        add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 4, 27, 256)   262400      activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 4, 27, 256)   1024        conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 4, 27, 256)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 4, 27, 256)   590080      activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 4, 27, 256)   1024        conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 4, 27, 256)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 4, 27, 1024)  263168      activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 4, 27, 1024)  0           add_107[0][0]                    \n",
      "                                                                 conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 4, 27, 1024)  4096        add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 4, 27, 256)   262400      activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 4, 27, 256)   1024        conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 4, 27, 256)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 4, 27, 256)   590080      activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 4, 27, 256)   1024        conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 4, 27, 256)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 4, 27, 1024)  263168      activation_307[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 4, 27, 1024)  0           add_108[0][0]                    \n",
      "                                                                 conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 4, 27, 1024)  4096        add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 4, 27, 256)   262400      activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 4, 27, 256)   1024        conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 4, 27, 256)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 4, 27, 256)   590080      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 4, 27, 256)   1024        conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 4, 27, 256)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 4, 27, 1024)  263168      activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 4, 27, 1024)  0           add_109[0][0]                    \n",
      "                                                                 conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 4, 27, 1024)  4096        add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 4, 27, 256)   262400      activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 4, 27, 256)   1024        conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 4, 27, 256)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 4, 27, 256)   590080      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 4, 27, 256)   1024        conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 4, 27, 256)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 4, 27, 1024)  263168      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 4, 27, 1024)  0           add_110[0][0]                    \n",
      "                                                                 conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 4, 27, 1024)  4096        add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 4, 27, 256)   262400      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 4, 27, 256)   1024        conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 4, 27, 256)   0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 4, 27, 256)   590080      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 4, 27, 256)   1024        conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 4, 27, 256)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 4, 27, 1024)  263168      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 4, 27, 1024)  0           add_111[0][0]                    \n",
      "                                                                 conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 4, 27, 1024)  4096        add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 4, 27, 256)   262400      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 4, 27, 256)   1024        conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 4, 27, 256)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 4, 27, 256)   590080      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 4, 27, 256)   1024        conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 4, 27, 256)   0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 4, 27, 1024)  263168      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 4, 27, 1024)  0           add_112[0][0]                    \n",
      "                                                                 conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 4, 27, 1024)  4096        add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 4, 27, 256)   262400      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 4, 27, 256)   1024        conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 4, 27, 256)   0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 4, 27, 256)   590080      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 4, 27, 256)   1024        conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 4, 27, 256)   0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 4, 27, 1024)  263168      activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 4, 27, 1024)  0           add_113[0][0]                    \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 4, 27, 1024)  4096        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 4, 27, 256)   262400      activation_323[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 4, 27, 256)   1024        conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 4, 27, 256)   0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 4, 27, 256)   590080      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 4, 27, 256)   1024        conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 4, 27, 256)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 4, 27, 1024)  263168      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 4, 27, 1024)  0           add_114[0][0]                    \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 4, 27, 1024)  4096        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 4, 27, 256)   262400      activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 4, 27, 256)   1024        conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 4, 27, 256)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 4, 27, 256)   590080      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 4, 27, 256)   1024        conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 4, 27, 256)   0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 4, 27, 1024)  263168      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 4, 27, 1024)  0           add_115[0][0]                    \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 4, 27, 1024)  4096        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 4, 27, 256)   262400      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 4, 27, 256)   1024        conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 4, 27, 256)   0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 4, 27, 256)   590080      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 4, 27, 256)   1024        conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 4, 27, 256)   0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 4, 27, 1024)  263168      activation_331[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 4, 27, 1024)  0           add_116[0][0]                    \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 4, 27, 1024)  4096        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 4, 27, 256)   262400      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 4, 27, 256)   1024        conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 4, 27, 256)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 4, 27, 256)   590080      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 4, 27, 256)   1024        conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 4, 27, 256)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 4, 27, 1024)  263168      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 4, 27, 1024)  0           add_117[0][0]                    \n",
      "                                                                 conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 4, 27, 1024)  4096        add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 4, 27, 256)   262400      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 4, 27, 256)   1024        conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 4, 27, 256)   0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 4, 27, 256)   590080      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 4, 27, 256)   1024        conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 4, 27, 256)   0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 4, 27, 1024)  263168      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 4, 27, 1024)  0           add_118[0][0]                    \n",
      "                                                                 conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 4, 27, 1024)  4096        add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 4, 27, 256)   262400      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 4, 27, 256)   1024        conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 4, 27, 256)   0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 27, 256)   590080      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 4, 27, 256)   1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 4, 27, 256)   0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 27, 1024)  263168      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 4, 27, 1024)  0           add_119[0][0]                    \n",
      "                                                                 conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 4, 27, 1024)  4096        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 2, 14, 512)   524800      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 2, 14, 512)   2048        conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 2, 14, 512)   0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 2, 14, 512)   2359808     activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 2, 14, 512)   2048        conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 2, 14, 512)   0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 2, 14, 2048)  2099200     add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 2, 14, 2048)  0           conv2d_362[0][0]                 \n",
      "                                                                 conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 2, 14, 2048)  8192        add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 2, 14, 512)   1049088     activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 2, 14, 512)   2048        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 2, 14, 512)   0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 2, 14, 512)   2359808     activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 2, 14, 512)   2048        conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 2, 14, 512)   0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 2, 14, 2048)  0           add_121[0][0]                    \n",
      "                                                                 conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 2, 14, 2048)  8192        add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 2, 14, 512)   1049088     activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 2, 14, 512)   2048        conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 2, 14, 512)   0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 2, 14, 512)   2359808     activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 2, 14, 512)   2048        conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 2, 14, 512)   0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 2, 14, 2048)  0           add_122[0][0]                    \n",
      "                                                                 conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 2, 14, 2048)  8192        add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 2048)   0           activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 52)           106548      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 52)           208         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 52)           0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 41)           2173        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 58,458,241\n",
      "Trainable params: 58,314,393\n",
      "Non-trainable params: 143,848\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 1.7734 - acc: 0.7374 - val_loss: 0.9094 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83827, saving model to model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "390/390 [==============================] - 126s 322ms/step - loss: 1.6849 - acc: 0.7677 - val_loss: 0.8623 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83827 to 0.84906, saving model to model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "Epoch 3/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.6636 - acc: 0.7768 - val_loss: 0.8690 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.84906\n",
      "Epoch 4/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.6496 - acc: 0.7787 - val_loss: 0.8314 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.84906\n",
      "Epoch 5/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.6375 - acc: 0.7853 - val_loss: 0.9574 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.84906\n",
      "Epoch 6/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.6110 - acc: 0.7902 - val_loss: 0.8351 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.84906 to 0.85714, saving model to model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "Epoch 7/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5896 - acc: 0.7941 - val_loss: 0.9275 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85714\n",
      "Epoch 8/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5926 - acc: 0.7942 - val_loss: 0.8856 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85714\n",
      "Epoch 9/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5926 - acc: 0.7933 - val_loss: 0.8827 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85714 to 0.86523, saving model to model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "Epoch 10/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5804 - acc: 0.7961 - val_loss: 0.8680 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86523\n",
      "Epoch 11/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5683 - acc: 0.7957 - val_loss: 0.8830 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86523\n",
      "Epoch 12/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5614 - acc: 0.8008 - val_loss: 0.8660 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86523\n",
      "Epoch 13/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5458 - acc: 0.8024 - val_loss: 0.9447 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86523\n",
      "Epoch 14/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5610 - acc: 0.8017 - val_loss: 0.8736 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86523\n",
      "Epoch 15/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5390 - acc: 0.8047 - val_loss: 0.8790 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86523\n",
      "Epoch 16/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5429 - acc: 0.7958 - val_loss: 0.8350 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86523\n",
      "Epoch 17/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5383 - acc: 0.8045 - val_loss: 0.8521 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86523\n",
      "Epoch 18/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5251 - acc: 0.8054 - val_loss: 0.8717 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86523\n",
      "Epoch 19/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5182 - acc: 0.8068 - val_loss: 0.8350 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86523\n",
      "Epoch 20/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5289 - acc: 0.8060 - val_loss: 0.9176 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86523\n",
      "Epoch 21/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5130 - acc: 0.8084 - val_loss: 0.8283 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86523\n",
      "Epoch 22/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5193 - acc: 0.8046 - val_loss: 0.8290 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86523\n",
      "Epoch 23/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5114 - acc: 0.8071 - val_loss: 0.9019 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86523\n",
      "Epoch 24/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5083 - acc: 0.8076 - val_loss: 0.8455 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86523\n",
      "Epoch 25/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.5029 - acc: 0.8134 - val_loss: 0.8134 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86523\n",
      "Epoch 26/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4935 - acc: 0.8071 - val_loss: 0.8602 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86523\n",
      "Epoch 27/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.5092 - acc: 0.8051 - val_loss: 0.8256 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86523\n",
      "Epoch 28/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4859 - acc: 0.8077 - val_loss: 0.8243 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86523\n",
      "Epoch 29/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4953 - acc: 0.8060 - val_loss: 0.8367 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86523\n",
      "Epoch 30/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4891 - acc: 0.8172 - val_loss: 0.8329 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86523\n",
      "Epoch 31/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4871 - acc: 0.8080 - val_loss: 0.8378 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86523\n",
      "Epoch 32/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4852 - acc: 0.8066 - val_loss: 0.8504 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86523\n",
      "Epoch 33/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4740 - acc: 0.8143 - val_loss: 0.9440 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86523\n",
      "Epoch 34/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4524 - acc: 0.8212 - val_loss: 0.8020 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.86523\n",
      "Epoch 35/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4736 - acc: 0.8139 - val_loss: 0.8346 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.86523\n",
      "Epoch 36/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4795 - acc: 0.8107 - val_loss: 0.8753 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.86523\n",
      "Epoch 37/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4574 - acc: 0.8155 - val_loss: 0.8337 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.86523\n",
      "Epoch 38/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4639 - acc: 0.8091 - val_loss: 0.8598 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.86523\n",
      "Epoch 39/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4514 - acc: 0.8165 - val_loss: 0.8338 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86523\n",
      "Epoch 40/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4575 - acc: 0.8159 - val_loss: 0.8060 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86523\n",
      "Epoch 41/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4677 - acc: 0.8133 - val_loss: 0.8633 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86523\n",
      "Epoch 42/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4450 - acc: 0.8226 - val_loss: 0.8369 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.86523\n",
      "Epoch 43/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4458 - acc: 0.8229 - val_loss: 0.8314 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86523\n",
      "Epoch 44/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4539 - acc: 0.8194 - val_loss: 0.8280 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86523\n",
      "Epoch 45/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4653 - acc: 0.8134 - val_loss: 0.8471 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86523\n",
      "Epoch 46/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4461 - acc: 0.8224 - val_loss: 0.8866 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86523\n",
      "Epoch 47/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4420 - acc: 0.8216 - val_loss: 0.8732 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86523\n",
      "Epoch 48/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4376 - acc: 0.8174 - val_loss: 0.8708 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86523\n",
      "Epoch 49/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4517 - acc: 0.8187 - val_loss: 0.8535 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86523\n",
      "Epoch 50/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4292 - acc: 0.8186 - val_loss: 0.8533 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86523\n",
      "Epoch 51/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4407 - acc: 0.8188 - val_loss: 0.8581 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86523\n",
      "Epoch 52/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4247 - acc: 0.8216 - val_loss: 0.8412 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.86523\n",
      "Epoch 53/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4272 - acc: 0.8225 - val_loss: 0.8511 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.86523\n",
      "Epoch 54/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4290 - acc: 0.8218 - val_loss: 0.7873 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.86523\n",
      "Epoch 55/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4316 - acc: 0.8179 - val_loss: 0.8936 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.86523\n",
      "Epoch 56/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4204 - acc: 0.8252 - val_loss: 0.8663 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.86523\n",
      "Epoch 57/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4377 - acc: 0.8183 - val_loss: 0.7742 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.86523\n",
      "Epoch 58/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4294 - acc: 0.8188 - val_loss: 0.8369 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86523\n",
      "Epoch 59/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4230 - acc: 0.8276 - val_loss: 0.8229 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86523\n",
      "Epoch 60/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4261 - acc: 0.8201 - val_loss: 0.8540 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.86523\n",
      "Epoch 61/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4232 - acc: 0.8285 - val_loss: 0.8749 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.86523\n",
      "Epoch 62/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4084 - acc: 0.8269 - val_loss: 0.8422 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.86523\n",
      "Epoch 63/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4213 - acc: 0.8197 - val_loss: 0.8140 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.86523\n",
      "Epoch 64/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4145 - acc: 0.8248 - val_loss: 0.8273 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86523\n",
      "Epoch 65/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4036 - acc: 0.8238 - val_loss: 0.8296 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.86523\n",
      "Epoch 66/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4190 - acc: 0.8247 - val_loss: 0.8524 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.86523\n",
      "Epoch 67/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4066 - acc: 0.8224 - val_loss: 0.8274 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86523\n",
      "Epoch 68/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4078 - acc: 0.8183 - val_loss: 0.8539 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86523\n",
      "Epoch 69/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4074 - acc: 0.8247 - val_loss: 0.8199 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.86523\n",
      "Epoch 70/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3957 - acc: 0.8234 - val_loss: 0.8179 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.86523\n",
      "Epoch 71/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.4053 - acc: 0.8248 - val_loss: 0.8330 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86523\n",
      "Epoch 72/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3885 - acc: 0.8317 - val_loss: 0.8606 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86523\n",
      "Epoch 73/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3962 - acc: 0.8274 - val_loss: 0.8282 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.86523\n",
      "Epoch 74/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3876 - acc: 0.8293 - val_loss: 0.7776 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.86523 to 0.86792, saving model to model/mfcc6/LGD_fold6_co_resnet.h5\n",
      "Epoch 75/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.4072 - acc: 0.8256 - val_loss: 0.7851 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.86792\n",
      "Epoch 76/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3976 - acc: 0.8212 - val_loss: 0.7872 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.86792\n",
      "Epoch 77/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3779 - acc: 0.8270 - val_loss: 0.7983 - val_acc: 0.8652\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.86792\n",
      "Epoch 78/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3889 - acc: 0.8310 - val_loss: 0.8333 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.86792\n",
      "Epoch 79/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3921 - acc: 0.8276 - val_loss: 0.8665 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.86792\n",
      "Epoch 80/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3870 - acc: 0.8276 - val_loss: 0.8025 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.86792\n",
      "Epoch 81/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3844 - acc: 0.8288 - val_loss: 0.7811 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.86792\n",
      "Epoch 82/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3706 - acc: 0.8305 - val_loss: 0.8508 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86792\n",
      "Epoch 83/3000\n",
      "390/390 [==============================] - 126s 323ms/step - loss: 1.3781 - acc: 0.8270 - val_loss: 0.7901 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86792\n",
      "Epoch 84/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3613 - acc: 0.8357 - val_loss: 0.8080 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86792\n",
      "Epoch 85/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3751 - acc: 0.8296 - val_loss: 0.7950 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.86792\n",
      "Epoch 86/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3779 - acc: 0.8322 - val_loss: 0.8034 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.86792\n",
      "Epoch 87/3000\n",
      "390/390 [==============================] - 126s 324ms/step - loss: 1.3790 - acc: 0.8353 - val_loss: 0.8286 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.86792\n",
      "Epoch 00087: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_5===\n",
      "semi loading: model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1214 (Conv2D)            (None, 32, 216, 64)  3200        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1168 (Batch (None, 32, 216, 64)  256         conv2d_1214[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1150 (Activation)    (None, 32, 216, 64)  0           batch_normalization_1168[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 16, 108, 64)  0           activation_1150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1215 (Conv2D)            (None, 16, 108, 64)  4160        max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1169 (Batch (None, 16, 108, 64)  256         conv2d_1215[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1151 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1169[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1216 (Conv2D)            (None, 16, 108, 64)  36928       activation_1151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1170 (Batch (None, 16, 108, 64)  256         conv2d_1216[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1152 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1170[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1218 (Conv2D)            (None, 16, 108, 256) 16640       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1217 (Conv2D)            (None, 16, 108, 256) 16640       activation_1152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_410 (Add)                   (None, 16, 108, 256) 0           conv2d_1218[0][0]                \n",
      "                                                                 conv2d_1217[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1171 (Batch (None, 16, 108, 256) 1024        add_410[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1153 (Activation)    (None, 16, 108, 256) 0           batch_normalization_1171[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1219 (Conv2D)            (None, 16, 108, 64)  16448       activation_1153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1172 (Batch (None, 16, 108, 64)  256         conv2d_1219[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1154 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1172[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1220 (Conv2D)            (None, 16, 108, 64)  36928       activation_1154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1173 (Batch (None, 16, 108, 64)  256         conv2d_1220[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1155 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1173[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1221 (Conv2D)            (None, 16, 108, 256) 16640       activation_1155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_411 (Add)                   (None, 16, 108, 256) 0           add_410[0][0]                    \n",
      "                                                                 conv2d_1221[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1174 (Batch (None, 16, 108, 256) 1024        add_411[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1156 (Activation)    (None, 16, 108, 256) 0           batch_normalization_1174[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1222 (Conv2D)            (None, 16, 108, 64)  16448       activation_1156[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1175 (Batch (None, 16, 108, 64)  256         conv2d_1222[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1157 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1175[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1223 (Conv2D)            (None, 16, 108, 64)  36928       activation_1157[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1176 (Batch (None, 16, 108, 64)  256         conv2d_1223[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1158 (Activation)    (None, 16, 108, 64)  0           batch_normalization_1176[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1224 (Conv2D)            (None, 16, 108, 256) 16640       activation_1158[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_412 (Add)                   (None, 16, 108, 256) 0           add_411[0][0]                    \n",
      "                                                                 conv2d_1224[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1177 (Batch (None, 16, 108, 256) 1024        add_412[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1159 (Activation)    (None, 16, 108, 256) 0           batch_normalization_1177[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1225 (Conv2D)            (None, 8, 54, 128)   32896       activation_1159[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1178 (Batch (None, 8, 54, 128)   512         conv2d_1225[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1160 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1178[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1226 (Conv2D)            (None, 8, 54, 128)   147584      activation_1160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1179 (Batch (None, 8, 54, 128)   512         conv2d_1226[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1161 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1179[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1228 (Conv2D)            (None, 8, 54, 512)   131584      add_412[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1227 (Conv2D)            (None, 8, 54, 512)   66048       activation_1161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_413 (Add)                   (None, 8, 54, 512)   0           conv2d_1228[0][0]                \n",
      "                                                                 conv2d_1227[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1180 (Batch (None, 8, 54, 512)   2048        add_413[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1162 (Activation)    (None, 8, 54, 512)   0           batch_normalization_1180[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1229 (Conv2D)            (None, 8, 54, 128)   65664       activation_1162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1181 (Batch (None, 8, 54, 128)   512         conv2d_1229[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1163 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1181[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1230 (Conv2D)            (None, 8, 54, 128)   147584      activation_1163[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1182 (Batch (None, 8, 54, 128)   512         conv2d_1230[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1164 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1182[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1231 (Conv2D)            (None, 8, 54, 512)   66048       activation_1164[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_414 (Add)                   (None, 8, 54, 512)   0           add_413[0][0]                    \n",
      "                                                                 conv2d_1231[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1183 (Batch (None, 8, 54, 512)   2048        add_414[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1165 (Activation)    (None, 8, 54, 512)   0           batch_normalization_1183[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1232 (Conv2D)            (None, 8, 54, 128)   65664       activation_1165[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1184 (Batch (None, 8, 54, 128)   512         conv2d_1232[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1166 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1184[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1233 (Conv2D)            (None, 8, 54, 128)   147584      activation_1166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1185 (Batch (None, 8, 54, 128)   512         conv2d_1233[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1167 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1185[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1234 (Conv2D)            (None, 8, 54, 512)   66048       activation_1167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_415 (Add)                   (None, 8, 54, 512)   0           add_414[0][0]                    \n",
      "                                                                 conv2d_1234[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1186 (Batch (None, 8, 54, 512)   2048        add_415[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1168 (Activation)    (None, 8, 54, 512)   0           batch_normalization_1186[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1235 (Conv2D)            (None, 8, 54, 128)   65664       activation_1168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1187 (Batch (None, 8, 54, 128)   512         conv2d_1235[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1169 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1187[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1236 (Conv2D)            (None, 8, 54, 128)   147584      activation_1169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1188 (Batch (None, 8, 54, 128)   512         conv2d_1236[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1170 (Activation)    (None, 8, 54, 128)   0           batch_normalization_1188[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1237 (Conv2D)            (None, 8, 54, 512)   66048       activation_1170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_416 (Add)                   (None, 8, 54, 512)   0           add_415[0][0]                    \n",
      "                                                                 conv2d_1237[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1189 (Batch (None, 8, 54, 512)   2048        add_416[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1171 (Activation)    (None, 8, 54, 512)   0           batch_normalization_1189[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1238 (Conv2D)            (None, 4, 27, 256)   131328      activation_1171[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1190 (Batch (None, 4, 27, 256)   1024        conv2d_1238[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1172 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1190[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1239 (Conv2D)            (None, 4, 27, 256)   590080      activation_1172[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1191 (Batch (None, 4, 27, 256)   1024        conv2d_1239[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1173 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1191[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1241 (Conv2D)            (None, 4, 27, 1024)  525312      add_416[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1240 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1173[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_417 (Add)                   (None, 4, 27, 1024)  0           conv2d_1241[0][0]                \n",
      "                                                                 conv2d_1240[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1192 (Batch (None, 4, 27, 1024)  4096        add_417[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1174 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1192[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1242 (Conv2D)            (None, 4, 27, 256)   262400      activation_1174[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1193 (Batch (None, 4, 27, 256)   1024        conv2d_1242[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1175 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1193[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1243 (Conv2D)            (None, 4, 27, 256)   590080      activation_1175[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1194 (Batch (None, 4, 27, 256)   1024        conv2d_1243[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1176 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1194[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1244 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1176[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_418 (Add)                   (None, 4, 27, 1024)  0           add_417[0][0]                    \n",
      "                                                                 conv2d_1244[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1195 (Batch (None, 4, 27, 1024)  4096        add_418[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1177 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1195[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1245 (Conv2D)            (None, 4, 27, 256)   262400      activation_1177[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1196 (Batch (None, 4, 27, 256)   1024        conv2d_1245[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1178 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1196[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1246 (Conv2D)            (None, 4, 27, 256)   590080      activation_1178[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1197 (Batch (None, 4, 27, 256)   1024        conv2d_1246[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1179 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1197[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1247 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1179[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_419 (Add)                   (None, 4, 27, 1024)  0           add_418[0][0]                    \n",
      "                                                                 conv2d_1247[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1198 (Batch (None, 4, 27, 1024)  4096        add_419[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1180 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1198[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1248 (Conv2D)            (None, 4, 27, 256)   262400      activation_1180[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1199 (Batch (None, 4, 27, 256)   1024        conv2d_1248[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1181 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1199[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1249 (Conv2D)            (None, 4, 27, 256)   590080      activation_1181[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1200 (Batch (None, 4, 27, 256)   1024        conv2d_1249[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1182 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1200[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1250 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1182[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_420 (Add)                   (None, 4, 27, 1024)  0           add_419[0][0]                    \n",
      "                                                                 conv2d_1250[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1201 (Batch (None, 4, 27, 1024)  4096        add_420[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1183 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1201[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1251 (Conv2D)            (None, 4, 27, 256)   262400      activation_1183[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1202 (Batch (None, 4, 27, 256)   1024        conv2d_1251[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1184 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1202[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1252 (Conv2D)            (None, 4, 27, 256)   590080      activation_1184[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1203 (Batch (None, 4, 27, 256)   1024        conv2d_1252[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1185 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1203[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1253 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_421 (Add)                   (None, 4, 27, 1024)  0           add_420[0][0]                    \n",
      "                                                                 conv2d_1253[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1204 (Batch (None, 4, 27, 1024)  4096        add_421[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1186 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1204[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1254 (Conv2D)            (None, 4, 27, 256)   262400      activation_1186[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1205 (Batch (None, 4, 27, 256)   1024        conv2d_1254[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1187 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1205[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1255 (Conv2D)            (None, 4, 27, 256)   590080      activation_1187[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1206 (Batch (None, 4, 27, 256)   1024        conv2d_1255[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1188 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1206[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1256 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_422 (Add)                   (None, 4, 27, 1024)  0           add_421[0][0]                    \n",
      "                                                                 conv2d_1256[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1207 (Batch (None, 4, 27, 1024)  4096        add_422[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1189 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1207[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1257 (Conv2D)            (None, 4, 27, 256)   262400      activation_1189[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1208 (Batch (None, 4, 27, 256)   1024        conv2d_1257[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1190 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1208[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1258 (Conv2D)            (None, 4, 27, 256)   590080      activation_1190[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1209 (Batch (None, 4, 27, 256)   1024        conv2d_1258[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1191 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1209[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1259 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_423 (Add)                   (None, 4, 27, 1024)  0           add_422[0][0]                    \n",
      "                                                                 conv2d_1259[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1210 (Batch (None, 4, 27, 1024)  4096        add_423[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1192 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1210[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1260 (Conv2D)            (None, 4, 27, 256)   262400      activation_1192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1211 (Batch (None, 4, 27, 256)   1024        conv2d_1260[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1193 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1211[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1261 (Conv2D)            (None, 4, 27, 256)   590080      activation_1193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1212 (Batch (None, 4, 27, 256)   1024        conv2d_1261[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1194 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1212[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1262 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_424 (Add)                   (None, 4, 27, 1024)  0           add_423[0][0]                    \n",
      "                                                                 conv2d_1262[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1213 (Batch (None, 4, 27, 1024)  4096        add_424[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1195 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1213[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1263 (Conv2D)            (None, 4, 27, 256)   262400      activation_1195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1214 (Batch (None, 4, 27, 256)   1024        conv2d_1263[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1196 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1214[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1264 (Conv2D)            (None, 4, 27, 256)   590080      activation_1196[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1215 (Batch (None, 4, 27, 256)   1024        conv2d_1264[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1197 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1215[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1265 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_425 (Add)                   (None, 4, 27, 1024)  0           add_424[0][0]                    \n",
      "                                                                 conv2d_1265[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1216 (Batch (None, 4, 27, 1024)  4096        add_425[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1198 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1216[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1266 (Conv2D)            (None, 4, 27, 256)   262400      activation_1198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1217 (Batch (None, 4, 27, 256)   1024        conv2d_1266[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1199 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1217[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1267 (Conv2D)            (None, 4, 27, 256)   590080      activation_1199[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1218 (Batch (None, 4, 27, 256)   1024        conv2d_1267[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1200 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1218[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1268 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_426 (Add)                   (None, 4, 27, 1024)  0           add_425[0][0]                    \n",
      "                                                                 conv2d_1268[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1219 (Batch (None, 4, 27, 1024)  4096        add_426[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1201 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1219[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1269 (Conv2D)            (None, 4, 27, 256)   262400      activation_1201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1220 (Batch (None, 4, 27, 256)   1024        conv2d_1269[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1202 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1220[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1270 (Conv2D)            (None, 4, 27, 256)   590080      activation_1202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1221 (Batch (None, 4, 27, 256)   1024        conv2d_1270[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1203 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1221[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1271 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_427 (Add)                   (None, 4, 27, 1024)  0           add_426[0][0]                    \n",
      "                                                                 conv2d_1271[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1222 (Batch (None, 4, 27, 1024)  4096        add_427[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1204 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1222[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1272 (Conv2D)            (None, 4, 27, 256)   262400      activation_1204[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1223 (Batch (None, 4, 27, 256)   1024        conv2d_1272[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1205 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1223[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1273 (Conv2D)            (None, 4, 27, 256)   590080      activation_1205[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1224 (Batch (None, 4, 27, 256)   1024        conv2d_1273[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1206 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1224[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1274 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_428 (Add)                   (None, 4, 27, 1024)  0           add_427[0][0]                    \n",
      "                                                                 conv2d_1274[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1225 (Batch (None, 4, 27, 1024)  4096        add_428[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1207 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1225[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1275 (Conv2D)            (None, 4, 27, 256)   262400      activation_1207[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1226 (Batch (None, 4, 27, 256)   1024        conv2d_1275[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1208 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1226[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1276 (Conv2D)            (None, 4, 27, 256)   590080      activation_1208[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1227 (Batch (None, 4, 27, 256)   1024        conv2d_1276[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1209 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1227[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1277 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_429 (Add)                   (None, 4, 27, 1024)  0           add_428[0][0]                    \n",
      "                                                                 conv2d_1277[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1228 (Batch (None, 4, 27, 1024)  4096        add_429[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1210 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1228[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1278 (Conv2D)            (None, 4, 27, 256)   262400      activation_1210[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1229 (Batch (None, 4, 27, 256)   1024        conv2d_1278[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1211 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1229[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1279 (Conv2D)            (None, 4, 27, 256)   590080      activation_1211[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1230 (Batch (None, 4, 27, 256)   1024        conv2d_1279[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1212 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1230[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1280 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_430 (Add)                   (None, 4, 27, 1024)  0           add_429[0][0]                    \n",
      "                                                                 conv2d_1280[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1231 (Batch (None, 4, 27, 1024)  4096        add_430[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1213 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1231[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1281 (Conv2D)            (None, 4, 27, 256)   262400      activation_1213[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1232 (Batch (None, 4, 27, 256)   1024        conv2d_1281[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1214 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1232[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1282 (Conv2D)            (None, 4, 27, 256)   590080      activation_1214[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1233 (Batch (None, 4, 27, 256)   1024        conv2d_1282[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1215 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1233[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1283 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_431 (Add)                   (None, 4, 27, 1024)  0           add_430[0][0]                    \n",
      "                                                                 conv2d_1283[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1234 (Batch (None, 4, 27, 1024)  4096        add_431[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1216 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1234[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1284 (Conv2D)            (None, 4, 27, 256)   262400      activation_1216[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1235 (Batch (None, 4, 27, 256)   1024        conv2d_1284[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1217 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1235[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1285 (Conv2D)            (None, 4, 27, 256)   590080      activation_1217[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1236 (Batch (None, 4, 27, 256)   1024        conv2d_1285[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1218 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1236[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1286 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_432 (Add)                   (None, 4, 27, 1024)  0           add_431[0][0]                    \n",
      "                                                                 conv2d_1286[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1237 (Batch (None, 4, 27, 1024)  4096        add_432[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1219 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1237[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1287 (Conv2D)            (None, 4, 27, 256)   262400      activation_1219[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1238 (Batch (None, 4, 27, 256)   1024        conv2d_1287[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1220 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1238[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1288 (Conv2D)            (None, 4, 27, 256)   590080      activation_1220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1239 (Batch (None, 4, 27, 256)   1024        conv2d_1288[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1221 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1239[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1289 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_433 (Add)                   (None, 4, 27, 1024)  0           add_432[0][0]                    \n",
      "                                                                 conv2d_1289[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1240 (Batch (None, 4, 27, 1024)  4096        add_433[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1222 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1240[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1290 (Conv2D)            (None, 4, 27, 256)   262400      activation_1222[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1241 (Batch (None, 4, 27, 256)   1024        conv2d_1290[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1223 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1241[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1291 (Conv2D)            (None, 4, 27, 256)   590080      activation_1223[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1242 (Batch (None, 4, 27, 256)   1024        conv2d_1291[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1224 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1242[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1292 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_434 (Add)                   (None, 4, 27, 1024)  0           add_433[0][0]                    \n",
      "                                                                 conv2d_1292[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1243 (Batch (None, 4, 27, 1024)  4096        add_434[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1225 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1243[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1293 (Conv2D)            (None, 4, 27, 256)   262400      activation_1225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1244 (Batch (None, 4, 27, 256)   1024        conv2d_1293[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1226 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1244[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1294 (Conv2D)            (None, 4, 27, 256)   590080      activation_1226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1245 (Batch (None, 4, 27, 256)   1024        conv2d_1294[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1227 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1245[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1295 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_435 (Add)                   (None, 4, 27, 1024)  0           add_434[0][0]                    \n",
      "                                                                 conv2d_1295[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1246 (Batch (None, 4, 27, 1024)  4096        add_435[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1228 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1246[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1296 (Conv2D)            (None, 4, 27, 256)   262400      activation_1228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1247 (Batch (None, 4, 27, 256)   1024        conv2d_1296[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1229 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1247[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1297 (Conv2D)            (None, 4, 27, 256)   590080      activation_1229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1248 (Batch (None, 4, 27, 256)   1024        conv2d_1297[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1230 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1248[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1298 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_436 (Add)                   (None, 4, 27, 1024)  0           add_435[0][0]                    \n",
      "                                                                 conv2d_1298[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1249 (Batch (None, 4, 27, 1024)  4096        add_436[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1231 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1249[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1299 (Conv2D)            (None, 4, 27, 256)   262400      activation_1231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1250 (Batch (None, 4, 27, 256)   1024        conv2d_1299[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1232 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1250[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1300 (Conv2D)            (None, 4, 27, 256)   590080      activation_1232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1251 (Batch (None, 4, 27, 256)   1024        conv2d_1300[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1233 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1251[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1301 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_437 (Add)                   (None, 4, 27, 1024)  0           add_436[0][0]                    \n",
      "                                                                 conv2d_1301[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1252 (Batch (None, 4, 27, 1024)  4096        add_437[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1234 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1252[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1302 (Conv2D)            (None, 4, 27, 256)   262400      activation_1234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1253 (Batch (None, 4, 27, 256)   1024        conv2d_1302[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1235 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1253[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1303 (Conv2D)            (None, 4, 27, 256)   590080      activation_1235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1254 (Batch (None, 4, 27, 256)   1024        conv2d_1303[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1236 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1254[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1304 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_438 (Add)                   (None, 4, 27, 1024)  0           add_437[0][0]                    \n",
      "                                                                 conv2d_1304[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1255 (Batch (None, 4, 27, 1024)  4096        add_438[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1237 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1255[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1305 (Conv2D)            (None, 4, 27, 256)   262400      activation_1237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1256 (Batch (None, 4, 27, 256)   1024        conv2d_1305[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1238 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1256[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1306 (Conv2D)            (None, 4, 27, 256)   590080      activation_1238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1257 (Batch (None, 4, 27, 256)   1024        conv2d_1306[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1239 (Activation)    (None, 4, 27, 256)   0           batch_normalization_1257[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1307 (Conv2D)            (None, 4, 27, 1024)  263168      activation_1239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_439 (Add)                   (None, 4, 27, 1024)  0           add_438[0][0]                    \n",
      "                                                                 conv2d_1307[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1258 (Batch (None, 4, 27, 1024)  4096        add_439[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1240 (Activation)    (None, 4, 27, 1024)  0           batch_normalization_1258[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1308 (Conv2D)            (None, 2, 14, 512)   524800      activation_1240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1259 (Batch (None, 2, 14, 512)   2048        conv2d_1308[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1241 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1259[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1309 (Conv2D)            (None, 2, 14, 512)   2359808     activation_1241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1260 (Batch (None, 2, 14, 512)   2048        conv2d_1309[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1242 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1260[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1311 (Conv2D)            (None, 2, 14, 2048)  2099200     add_439[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1310 (Conv2D)            (None, 2, 14, 2048)  1050624     activation_1242[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_440 (Add)                   (None, 2, 14, 2048)  0           conv2d_1311[0][0]                \n",
      "                                                                 conv2d_1310[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1261 (Batch (None, 2, 14, 2048)  8192        add_440[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1243 (Activation)    (None, 2, 14, 2048)  0           batch_normalization_1261[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1312 (Conv2D)            (None, 2, 14, 512)   1049088     activation_1243[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1262 (Batch (None, 2, 14, 512)   2048        conv2d_1312[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1244 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1262[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1313 (Conv2D)            (None, 2, 14, 512)   2359808     activation_1244[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1263 (Batch (None, 2, 14, 512)   2048        conv2d_1313[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1245 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1263[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1314 (Conv2D)            (None, 2, 14, 2048)  1050624     activation_1245[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_441 (Add)                   (None, 2, 14, 2048)  0           add_440[0][0]                    \n",
      "                                                                 conv2d_1314[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1264 (Batch (None, 2, 14, 2048)  8192        add_441[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1246 (Activation)    (None, 2, 14, 2048)  0           batch_normalization_1264[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1315 (Conv2D)            (None, 2, 14, 512)   1049088     activation_1246[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1265 (Batch (None, 2, 14, 512)   2048        conv2d_1315[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1247 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1265[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1316 (Conv2D)            (None, 2, 14, 512)   2359808     activation_1247[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1266 (Batch (None, 2, 14, 512)   2048        conv2d_1316[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1248 (Activation)    (None, 2, 14, 512)   0           batch_normalization_1266[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1317 (Conv2D)            (None, 2, 14, 2048)  1050624     activation_1248[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_442 (Add)                   (None, 2, 14, 2048)  0           add_441[0][0]                    \n",
      "                                                                 conv2d_1317[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1267 (Batch (None, 2, 14, 2048)  8192        add_442[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1249 (Activation)    (None, 2, 14, 2048)  0           batch_normalization_1267[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 1, 1, 2048)   0           activation_1249[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 1, 1, 2048)   0           average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2048)         0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 56)           114744      flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1268 (Batch (None, 56)           224         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 56)           0           batch_normalization_1268[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 41)           2337        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 42,753,849\n",
      "Trainable params: 42,656,073\n",
      "Non-trainable params: 97,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "390/390 [==============================] - 103s 265ms/step - loss: 2.7574 - acc: 0.5171 - val_loss: 1.7169 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57143, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 2.6914 - acc: 0.5361 - val_loss: 1.5679 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57143 to 0.60916, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 3/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.6975 - acc: 0.5422 - val_loss: 1.5226 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60916 to 0.63073, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 4/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7276 - acc: 0.5307 - val_loss: 2.6167 - val_acc: 0.4043\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63073\n",
      "Epoch 5/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.8190 - acc: 0.4948 - val_loss: 4.1830 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.63073\n",
      "Epoch 6/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7687 - acc: 0.5031 - val_loss: 1.8521 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.63073\n",
      "Epoch 7/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7192 - acc: 0.5302 - val_loss: 1.6436 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.63073\n",
      "Epoch 8/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.6629 - acc: 0.5374 - val_loss: 1.6072 - val_acc: 0.6011\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.63073\n",
      "Epoch 9/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7585 - acc: 0.5283 - val_loss: 2.2995 - val_acc: 0.5337\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.63073\n",
      "Epoch 10/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7365 - acc: 0.5322 - val_loss: 1.7154 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.63073\n",
      "Epoch 11/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.7448 - acc: 0.5480 - val_loss: 1.8158 - val_acc: 0.5283\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.63073\n",
      "Epoch 12/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.5765 - acc: 0.5496 - val_loss: 2.0263 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.63073\n",
      "Epoch 13/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.6132 - acc: 0.5377 - val_loss: 1.5557 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.63073\n",
      "Epoch 14/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.4347 - acc: 0.5731 - val_loss: 1.6243 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.63073\n",
      "Epoch 15/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3853 - acc: 0.5834 - val_loss: 1.5158 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.63073\n",
      "Epoch 16/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3561 - acc: 0.5959 - val_loss: 1.5091 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.63073\n",
      "Epoch 17/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3145 - acc: 0.5936 - val_loss: 1.4023 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.63073 to 0.67385, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 18/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2715 - acc: 0.6135 - val_loss: 1.4313 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.67385\n",
      "Epoch 19/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3101 - acc: 0.5986 - val_loss: 1.4314 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.67385 to 0.67925, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 20/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3144 - acc: 0.6034 - val_loss: 1.4589 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.67925\n",
      "Epoch 21/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2332 - acc: 0.6175 - val_loss: 1.3993 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.67925\n",
      "Epoch 22/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2916 - acc: 0.6048 - val_loss: 1.4643 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.67925\n",
      "Epoch 23/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2355 - acc: 0.6280 - val_loss: 1.4772 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.67925\n",
      "Epoch 24/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2235 - acc: 0.6332 - val_loss: 1.4434 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.67925\n",
      "Epoch 25/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.5868 - acc: 0.5933 - val_loss: 3.1002 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.67925\n",
      "Epoch 26/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.4733 - acc: 0.5884 - val_loss: 1.5319 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.67925\n",
      "Epoch 27/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3437 - acc: 0.6056 - val_loss: 1.4355 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.67925\n",
      "Epoch 28/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1866 - acc: 0.6343 - val_loss: 1.5308 - val_acc: 0.6119\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.67925\n",
      "Epoch 29/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1915 - acc: 0.6390 - val_loss: 1.5258 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.67925\n",
      "Epoch 30/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2043 - acc: 0.6298 - val_loss: 1.4157 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.67925\n",
      "Epoch 31/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2176 - acc: 0.6388 - val_loss: 1.5352 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.67925\n",
      "Epoch 32/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2790 - acc: 0.6252 - val_loss: 1.3460 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.67925\n",
      "Epoch 33/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1707 - acc: 0.6474 - val_loss: 1.6312 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.67925\n",
      "Epoch 34/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2261 - acc: 0.6314 - val_loss: 1.3676 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.67925\n",
      "Epoch 35/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1542 - acc: 0.6458 - val_loss: 1.3695 - val_acc: 0.6550\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.67925\n",
      "Epoch 36/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1525 - acc: 0.6545 - val_loss: 1.3355 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.67925\n",
      "Epoch 37/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1224 - acc: 0.6574 - val_loss: 1.2856 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.67925 to 0.69003, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 38/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1661 - acc: 0.6461 - val_loss: 1.3975 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.69003\n",
      "Epoch 39/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1226 - acc: 0.6630 - val_loss: 1.3673 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.69003\n",
      "Epoch 40/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.3856 - acc: 0.6183 - val_loss: 1.4382 - val_acc: 0.6253\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.69003\n",
      "Epoch 41/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1534 - acc: 0.6518 - val_loss: 1.3935 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.69003\n",
      "Epoch 42/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1134 - acc: 0.6662 - val_loss: 1.2673 - val_acc: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_acc improved from 0.69003 to 0.71429, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 43/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0881 - acc: 0.6685 - val_loss: 1.3729 - val_acc: 0.6442\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.71429\n",
      "Epoch 44/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0884 - acc: 0.6719 - val_loss: 1.2051 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.71429 to 0.72507, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 45/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0797 - acc: 0.6773 - val_loss: 1.2130 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.72507\n",
      "Epoch 46/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0581 - acc: 0.6797 - val_loss: 1.2255 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.72507\n",
      "Epoch 47/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0487 - acc: 0.6863 - val_loss: 1.3138 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.72507\n",
      "Epoch 48/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1631 - acc: 0.6579 - val_loss: 1.3052 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.72507\n",
      "Epoch 49/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0580 - acc: 0.6757 - val_loss: 1.2285 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.72507\n",
      "Epoch 50/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0321 - acc: 0.6862 - val_loss: 1.4737 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.72507\n",
      "Epoch 51/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0964 - acc: 0.6760 - val_loss: 1.2143 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.72507\n",
      "Epoch 52/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.2377 - acc: 0.6380 - val_loss: 1.3639 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.72507\n",
      "Epoch 53/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0499 - acc: 0.6771 - val_loss: 1.2345 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.72507\n",
      "Epoch 54/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0316 - acc: 0.6893 - val_loss: 1.1098 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.72507 to 0.73854, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 55/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0280 - acc: 0.6895 - val_loss: 1.1536 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.73854 to 0.74933, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 56/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0132 - acc: 0.6913 - val_loss: 1.1755 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.74933\n",
      "Epoch 57/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0139 - acc: 0.6953 - val_loss: 1.2112 - val_acc: 0.7062\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.74933\n",
      "Epoch 58/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1676 - acc: 0.6579 - val_loss: 1.2735 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.74933\n",
      "Epoch 59/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9980 - acc: 0.6926 - val_loss: 1.3093 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.74933\n",
      "Epoch 60/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9979 - acc: 0.6940 - val_loss: 1.1920 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.74933\n",
      "Epoch 61/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0241 - acc: 0.6944 - val_loss: 1.1537 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.74933\n",
      "Epoch 62/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0035 - acc: 0.6854 - val_loss: 1.1572 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.74933\n",
      "Epoch 63/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9727 - acc: 0.7067 - val_loss: 1.1359 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.74933\n",
      "Epoch 64/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.1252 - acc: 0.6777 - val_loss: 1.1262 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.74933\n",
      "Epoch 65/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9736 - acc: 0.7039 - val_loss: 1.1526 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.74933\n",
      "Epoch 66/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9859 - acc: 0.7051 - val_loss: 1.1567 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.74933\n",
      "Epoch 67/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9607 - acc: 0.7132 - val_loss: 1.7267 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.74933\n",
      "Epoch 68/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0147 - acc: 0.6883 - val_loss: 1.0871 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.74933 to 0.76280, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 69/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9979 - acc: 0.7067 - val_loss: 1.0633 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76280\n",
      "Epoch 70/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0023 - acc: 0.7008 - val_loss: 1.1015 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76280\n",
      "Epoch 71/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9489 - acc: 0.7089 - val_loss: 1.1095 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76280\n",
      "Epoch 72/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9777 - acc: 0.7037 - val_loss: 1.1342 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76280\n",
      "Epoch 73/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9595 - acc: 0.7105 - val_loss: 1.0709 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76280\n",
      "Epoch 74/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9528 - acc: 0.7110 - val_loss: 1.0580 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76280\n",
      "Epoch 75/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9128 - acc: 0.7171 - val_loss: 1.1254 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76280\n",
      "Epoch 76/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9414 - acc: 0.7107 - val_loss: 1.0779 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76280\n",
      "Epoch 77/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9168 - acc: 0.7256 - val_loss: 1.1707 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76280\n",
      "Epoch 78/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0627 - acc: 0.6954 - val_loss: 1.1094 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76280\n",
      "Epoch 79/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9686 - acc: 0.7044 - val_loss: 1.0468 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.76280 to 0.77358, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 80/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9203 - acc: 0.7121 - val_loss: 1.0721 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.77358\n",
      "Epoch 81/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9655 - acc: 0.7123 - val_loss: 1.0379 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.77358 to 0.77628, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 82/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.9168 - acc: 0.7196 - val_loss: 1.0040 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.77628\n",
      "Epoch 83/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8891 - acc: 0.7335 - val_loss: 1.0201 - val_acc: 0.7925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_acc improved from 0.77628 to 0.79245, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 84/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8823 - acc: 0.7338 - val_loss: 0.9938 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.79245\n",
      "Epoch 85/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8590 - acc: 0.7377 - val_loss: 1.0154 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.79245\n",
      "Epoch 86/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8610 - acc: 0.7373 - val_loss: 0.9460 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.79245\n",
      "Epoch 87/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8553 - acc: 0.7384 - val_loss: 1.0207 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.79245\n",
      "Epoch 88/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8560 - acc: 0.7417 - val_loss: 0.9788 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.79245\n",
      "Epoch 89/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8246 - acc: 0.7462 - val_loss: 0.9598 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.79245\n",
      "Epoch 90/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8168 - acc: 0.7454 - val_loss: 0.9388 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.79245 to 0.80054, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 91/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8147 - acc: 0.7428 - val_loss: 0.9653 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.80054\n",
      "Epoch 92/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8606 - acc: 0.7424 - val_loss: 0.9733 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.80054\n",
      "Epoch 93/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8279 - acc: 0.7466 - val_loss: 0.9598 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.80054\n",
      "Epoch 94/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8213 - acc: 0.7451 - val_loss: 0.9461 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.80054\n",
      "Epoch 95/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8162 - acc: 0.7446 - val_loss: 0.8893 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.80054\n",
      "Epoch 96/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8165 - acc: 0.7524 - val_loss: 0.9858 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.80054\n",
      "Epoch 97/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7996 - acc: 0.7526 - val_loss: 0.9149 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.80054\n",
      "Epoch 98/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7874 - acc: 0.7534 - val_loss: 0.9111 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00098: val_acc improved from 0.80054 to 0.80863, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 99/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7977 - acc: 0.7467 - val_loss: 1.0125 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.80863\n",
      "Epoch 100/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8208 - acc: 0.7487 - val_loss: 0.9622 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.80863\n",
      "Epoch 101/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8299 - acc: 0.7434 - val_loss: 0.9151 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00101: val_acc improved from 0.80863 to 0.80863, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 102/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7978 - acc: 0.7557 - val_loss: 0.9005 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.80863 to 0.82210, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 103/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8090 - acc: 0.7539 - val_loss: 0.9336 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82210\n",
      "Epoch 104/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7777 - acc: 0.7549 - val_loss: 0.8817 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82210\n",
      "Epoch 105/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7816 - acc: 0.7549 - val_loss: 0.8670 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82210\n",
      "Epoch 106/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7816 - acc: 0.7588 - val_loss: 0.8625 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00106: val_acc improved from 0.82210 to 0.83019, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 107/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7986 - acc: 0.7506 - val_loss: 0.9123 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.83019\n",
      "Epoch 108/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8021 - acc: 0.7498 - val_loss: 0.8541 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.83019\n",
      "Epoch 109/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7995 - acc: 0.7529 - val_loss: 0.8489 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.83019\n",
      "Epoch 110/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7745 - acc: 0.7616 - val_loss: 0.9176 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.83019\n",
      "Epoch 111/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7707 - acc: 0.7613 - val_loss: 0.9269 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.83019\n",
      "Epoch 112/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7805 - acc: 0.7550 - val_loss: 0.8858 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.83019\n",
      "Epoch 113/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7897 - acc: 0.7557 - val_loss: 1.0408 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.83019\n",
      "Epoch 114/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7841 - acc: 0.7541 - val_loss: 0.8924 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.83019\n",
      "Epoch 115/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7661 - acc: 0.7623 - val_loss: 0.8676 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.83019\n",
      "Epoch 116/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7672 - acc: 0.7582 - val_loss: 0.8939 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.83019\n",
      "Epoch 117/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7647 - acc: 0.7581 - val_loss: 0.8520 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.83019\n",
      "Epoch 118/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7624 - acc: 0.7673 - val_loss: 0.8807 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.83019\n",
      "Epoch 119/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8132 - acc: 0.7562 - val_loss: 0.9873 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.83019\n",
      "Epoch 120/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8199 - acc: 0.7514 - val_loss: 0.8942 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.83019\n",
      "Epoch 121/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7931 - acc: 0.7523 - val_loss: 0.9015 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.83019\n",
      "Epoch 122/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7603 - acc: 0.7574 - val_loss: 0.8916 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.83019\n",
      "Epoch 123/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7490 - acc: 0.7594 - val_loss: 0.8845 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.83019\n",
      "Epoch 124/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7591 - acc: 0.7558 - val_loss: 0.8467 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.83019\n",
      "Epoch 125/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7323 - acc: 0.7660 - val_loss: 0.9059 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.83019\n",
      "Epoch 126/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7400 - acc: 0.7675 - val_loss: 0.8748 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.83019\n",
      "Epoch 127/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7474 - acc: 0.7591 - val_loss: 0.9068 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.83019\n",
      "Epoch 128/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7404 - acc: 0.7656 - val_loss: 0.8705 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.83019\n",
      "Epoch 129/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7225 - acc: 0.7669 - val_loss: 0.8434 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.83019\n",
      "Epoch 130/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7052 - acc: 0.7743 - val_loss: 0.8528 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.83019\n",
      "Epoch 131/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7196 - acc: 0.7723 - val_loss: 0.8295 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.83019\n",
      "Epoch 132/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7209 - acc: 0.7745 - val_loss: 0.8418 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.83019\n",
      "Epoch 133/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7545 - acc: 0.7655 - val_loss: 0.9068 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83019\n",
      "Epoch 134/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7412 - acc: 0.7649 - val_loss: 0.9079 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83019\n",
      "Epoch 135/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8065 - acc: 0.7566 - val_loss: 0.8287 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.83019\n",
      "Epoch 136/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7536 - acc: 0.7611 - val_loss: 0.8217 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83019\n",
      "Epoch 137/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7230 - acc: 0.7714 - val_loss: 0.8545 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83019\n",
      "Epoch 138/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7052 - acc: 0.7728 - val_loss: 0.8113 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83019\n",
      "Epoch 139/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7104 - acc: 0.7780 - val_loss: 0.8594 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83019\n",
      "Epoch 140/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7075 - acc: 0.7760 - val_loss: 0.8368 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83019\n",
      "Epoch 141/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7019 - acc: 0.7713 - val_loss: 0.8507 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83019\n",
      "Epoch 142/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6901 - acc: 0.7799 - val_loss: 0.8167 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83019\n",
      "Epoch 143/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6757 - acc: 0.7823 - val_loss: 0.8238 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83019\n",
      "Epoch 144/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6928 - acc: 0.7808 - val_loss: 0.8275 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83019\n",
      "Epoch 145/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6776 - acc: 0.7860 - val_loss: 0.8130 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83019\n",
      "Epoch 146/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6752 - acc: 0.7808 - val_loss: 0.8175 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83019\n",
      "Epoch 147/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6769 - acc: 0.7821 - val_loss: 0.8225 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83019\n",
      "Epoch 148/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6922 - acc: 0.7803 - val_loss: 0.8274 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83019\n",
      "Epoch 149/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6822 - acc: 0.7762 - val_loss: 0.8239 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83019\n",
      "Epoch 150/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6852 - acc: 0.7750 - val_loss: 0.8200 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83019\n",
      "Epoch 151/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6865 - acc: 0.7786 - val_loss: 0.8121 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.83019\n",
      "Epoch 152/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6713 - acc: 0.7815 - val_loss: 0.8545 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83019\n",
      "Epoch 153/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6834 - acc: 0.7805 - val_loss: 0.8307 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83019\n",
      "Epoch 154/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7614 - acc: 0.7720 - val_loss: 1.2308 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83019\n",
      "Epoch 155/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 2.0757 - acc: 0.7474 - val_loss: 1.2472 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83019\n",
      "Epoch 156/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.8060 - acc: 0.7656 - val_loss: 0.8877 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.83019\n",
      "Epoch 157/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.7108 - acc: 0.7738 - val_loss: 0.8354 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83019\n",
      "Epoch 158/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6837 - acc: 0.7797 - val_loss: 0.8333 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83019\n",
      "Epoch 159/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6881 - acc: 0.7809 - val_loss: 0.8123 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00159: val_acc improved from 0.83019 to 0.83558, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 160/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6749 - acc: 0.7856 - val_loss: 0.8225 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83558\n",
      "Epoch 161/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6707 - acc: 0.7854 - val_loss: 0.8201 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83558\n",
      "Epoch 162/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6588 - acc: 0.7827 - val_loss: 0.8021 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83558\n",
      "Epoch 163/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6669 - acc: 0.7881 - val_loss: 0.8244 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83558\n",
      "Epoch 164/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6617 - acc: 0.7896 - val_loss: 0.8201 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83558\n",
      "Epoch 165/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6446 - acc: 0.7905 - val_loss: 0.8101 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83558\n",
      "Epoch 166/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6659 - acc: 0.7910 - val_loss: 0.8079 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.83558\n",
      "Epoch 167/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6405 - acc: 0.7912 - val_loss: 0.8031 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.83558\n",
      "Epoch 168/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6417 - acc: 0.7964 - val_loss: 0.8091 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.83558\n",
      "Epoch 169/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6423 - acc: 0.7889 - val_loss: 0.8084 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.83558\n",
      "Epoch 170/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6534 - acc: 0.7907 - val_loss: 0.8175 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.83558\n",
      "Epoch 171/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6288 - acc: 0.7921 - val_loss: 0.8128 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.83558\n",
      "Epoch 172/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6433 - acc: 0.7865 - val_loss: 0.8033 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.83558\n",
      "Epoch 173/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6264 - acc: 0.7942 - val_loss: 0.8059 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.83558\n",
      "Epoch 174/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6421 - acc: 0.7909 - val_loss: 0.7898 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00174: val_acc improved from 0.83558 to 0.83827, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 175/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6330 - acc: 0.7950 - val_loss: 0.8207 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.83827\n",
      "Epoch 176/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6384 - acc: 0.7919 - val_loss: 0.8222 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.83827\n",
      "Epoch 177/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6295 - acc: 0.7953 - val_loss: 0.8134 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.83827\n",
      "Epoch 178/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6348 - acc: 0.7863 - val_loss: 0.8234 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.83827\n",
      "Epoch 179/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6292 - acc: 0.7929 - val_loss: 0.8013 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.83827\n",
      "Epoch 180/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6189 - acc: 0.7931 - val_loss: 0.7854 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.83827\n",
      "Epoch 181/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6268 - acc: 0.7949 - val_loss: 0.8082 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.83827\n",
      "Epoch 182/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6144 - acc: 0.8013 - val_loss: 0.7885 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.83827\n",
      "Epoch 183/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6343 - acc: 0.7936 - val_loss: 0.7930 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.83827\n",
      "Epoch 184/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6314 - acc: 0.7919 - val_loss: 0.7963 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.83827\n",
      "Epoch 185/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6252 - acc: 0.8000 - val_loss: 0.8039 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.83827\n",
      "Epoch 186/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6430 - acc: 0.7942 - val_loss: 0.7854 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.83827\n",
      "Epoch 187/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6254 - acc: 0.7933 - val_loss: 0.7917 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.83827\n",
      "Epoch 188/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6213 - acc: 0.7920 - val_loss: 0.7962 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.83827\n",
      "Epoch 189/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6353 - acc: 0.7906 - val_loss: 0.8077 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.83827\n",
      "Epoch 190/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6294 - acc: 0.7957 - val_loss: 0.7870 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.83827\n",
      "Epoch 191/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6148 - acc: 0.7976 - val_loss: 0.7787 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.83827\n",
      "Epoch 192/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5991 - acc: 0.8039 - val_loss: 0.7888 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.83827\n",
      "Epoch 193/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6001 - acc: 0.8000 - val_loss: 0.7934 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.83827\n",
      "Epoch 194/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5962 - acc: 0.8004 - val_loss: 0.7818 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.83827\n",
      "Epoch 195/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5985 - acc: 0.8044 - val_loss: 0.7826 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.83827\n",
      "Epoch 196/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6213 - acc: 0.7986 - val_loss: 0.7919 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.83827\n",
      "Epoch 197/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6286 - acc: 0.7932 - val_loss: 0.7826 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.83827\n",
      "Epoch 198/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6027 - acc: 0.8019 - val_loss: 0.7827 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.83827\n",
      "Epoch 199/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5988 - acc: 0.7944 - val_loss: 0.7764 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.83827\n",
      "Epoch 200/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6043 - acc: 0.7953 - val_loss: 0.7759 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.83827\n",
      "Epoch 201/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6032 - acc: 0.8020 - val_loss: 0.7653 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.83827\n",
      "Epoch 202/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6145 - acc: 0.8002 - val_loss: 0.7634 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.83827\n",
      "Epoch 203/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6046 - acc: 0.8017 - val_loss: 0.7800 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.83827\n",
      "Epoch 204/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6037 - acc: 0.7992 - val_loss: 0.7733 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.83827\n",
      "Epoch 205/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5805 - acc: 0.8022 - val_loss: 0.7699 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.83827\n",
      "Epoch 206/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5999 - acc: 0.8004 - val_loss: 0.7704 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.83827\n",
      "Epoch 207/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5969 - acc: 0.7995 - val_loss: 0.7761 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.83827\n",
      "Epoch 208/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6143 - acc: 0.7984 - val_loss: 0.7784 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.83827\n",
      "Epoch 209/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5949 - acc: 0.8027 - val_loss: 0.7777 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.83827\n",
      "Epoch 210/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5881 - acc: 0.8040 - val_loss: 0.7797 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.83827\n",
      "Epoch 211/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6241 - acc: 0.8007 - val_loss: 0.7792 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.83827\n",
      "Epoch 212/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5908 - acc: 0.8090 - val_loss: 0.7800 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.83827\n",
      "Epoch 213/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.6196 - acc: 0.8006 - val_loss: 0.7800 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.83827\n",
      "Epoch 214/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6047 - acc: 0.8067 - val_loss: 0.7673 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.83827\n",
      "Epoch 215/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6027 - acc: 0.8006 - val_loss: 0.7788 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.83827\n",
      "Epoch 216/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5973 - acc: 0.8026 - val_loss: 0.7767 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.83827\n",
      "Epoch 217/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5987 - acc: 0.8031 - val_loss: 0.7855 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.83827\n",
      "Epoch 218/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5792 - acc: 0.8029 - val_loss: 0.7697 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.83827\n",
      "Epoch 219/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5879 - acc: 0.8026 - val_loss: 0.7785 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.83827\n",
      "Epoch 220/3000\n",
      "390/390 [==============================] - 91s 233ms/step - loss: 1.5964 - acc: 0.7994 - val_loss: 0.7666 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00220: val_acc improved from 0.83827 to 0.84636, saving model to model/mfcc6/LGD_fold5_co_resnet.h5\n",
      "Epoch 221/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5939 - acc: 0.7983 - val_loss: 0.7837 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.84636\n",
      "Epoch 222/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5865 - acc: 0.8007 - val_loss: 0.7753 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.84636\n",
      "Epoch 223/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5829 - acc: 0.8023 - val_loss: 0.7757 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.84636\n",
      "Epoch 224/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6018 - acc: 0.8022 - val_loss: 0.7699 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.84636\n",
      "Epoch 225/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5965 - acc: 0.7994 - val_loss: 0.7847 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.84636\n",
      "Epoch 226/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.6034 - acc: 0.7979 - val_loss: 0.7877 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.84636\n",
      "Epoch 227/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5886 - acc: 0.8027 - val_loss: 0.7705 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.84636\n",
      "Epoch 228/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5995 - acc: 0.8022 - val_loss: 0.7751 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.84636\n",
      "Epoch 229/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5864 - acc: 0.8026 - val_loss: 0.7723 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.84636\n",
      "Epoch 230/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5940 - acc: 0.8019 - val_loss: 0.7789 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.84636\n",
      "Epoch 231/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5876 - acc: 0.8110 - val_loss: 0.7842 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.84636\n",
      "Epoch 232/3000\n",
      "390/390 [==============================] - 91s 232ms/step - loss: 1.5918 - acc: 0.7989 - val_loss: 0.7834 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.84636\n",
      "Epoch 00232: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_4===\n",
      "semi loading: model/mfcc6/LGD_fold4_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_757 (Conv2D)             (None, 32, 216, 64)  3200        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_729 (BatchN (None, 32, 216, 64)  256         conv2d_757[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 32, 216, 64)  0           batch_normalization_729[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 16, 108, 64)  0           activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 16, 108, 64)  36928       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_730 (BatchN (None, 16, 108, 64)  256         conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 16, 108, 64)  0           batch_normalization_730[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 16, 108, 64)  36928       activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_255 (Add)                   (None, 16, 108, 64)  0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_731 (BatchN (None, 16, 108, 64)  256         add_255[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 16, 108, 64)  0           batch_normalization_731[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 16, 108, 64)  36928       activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_732 (BatchN (None, 16, 108, 64)  256         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 16, 108, 64)  0           batch_normalization_732[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 16, 108, 64)  36928       activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_256 (Add)                   (None, 16, 108, 64)  0           add_255[0][0]                    \n",
      "                                                                 conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_733 (BatchN (None, 16, 108, 64)  256         add_256[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 16, 108, 64)  0           batch_normalization_733[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 16, 108, 64)  36928       activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_734 (BatchN (None, 16, 108, 64)  256         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 16, 108, 64)  0           batch_normalization_734[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 16, 108, 64)  36928       activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_257 (Add)                   (None, 16, 108, 64)  0           add_256[0][0]                    \n",
      "                                                                 conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_735 (BatchN (None, 16, 108, 64)  256         add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 16, 108, 64)  0           batch_normalization_735[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 8, 54, 128)   73856       activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_736 (BatchN (None, 8, 54, 128)   512         conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 8, 54, 128)   0           batch_normalization_736[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 8, 54, 128)   8320        add_257[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 8, 54, 128)   147584      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_258 (Add)                   (None, 8, 54, 128)   0           conv2d_766[0][0]                 \n",
      "                                                                 conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_737 (BatchN (None, 8, 54, 128)   512         add_258[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 8, 54, 128)   0           batch_normalization_737[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 8, 54, 128)   147584      activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_738 (BatchN (None, 8, 54, 128)   512         conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 8, 54, 128)   0           batch_normalization_738[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 8, 54, 128)   147584      activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_259 (Add)                   (None, 8, 54, 128)   0           add_258[0][0]                    \n",
      "                                                                 conv2d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_739 (BatchN (None, 8, 54, 128)   512         add_259[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 8, 54, 128)   0           batch_normalization_739[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 8, 54, 128)   147584      activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_740 (BatchN (None, 8, 54, 128)   512         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 8, 54, 128)   0           batch_normalization_740[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 8, 54, 128)   147584      activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_260 (Add)                   (None, 8, 54, 128)   0           add_259[0][0]                    \n",
      "                                                                 conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_741 (BatchN (None, 8, 54, 128)   512         add_260[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 8, 54, 128)   0           batch_normalization_741[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 8, 54, 128)   147584      activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_742 (BatchN (None, 8, 54, 128)   512         conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 8, 54, 128)   0           batch_normalization_742[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 8, 54, 128)   147584      activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_261 (Add)                   (None, 8, 54, 128)   0           add_260[0][0]                    \n",
      "                                                                 conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_743 (BatchN (None, 8, 54, 128)   512         add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 8, 54, 128)   0           batch_normalization_743[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 4, 27, 256)   295168      activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_744 (BatchN (None, 4, 27, 256)   1024        conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 4, 27, 256)   0           batch_normalization_744[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 4, 27, 256)   33024       add_261[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 4, 27, 256)   590080      activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_262 (Add)                   (None, 4, 27, 256)   0           conv2d_775[0][0]                 \n",
      "                                                                 conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_745 (BatchN (None, 4, 27, 256)   1024        add_262[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 4, 27, 256)   0           batch_normalization_745[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 4, 27, 256)   590080      activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_746 (BatchN (None, 4, 27, 256)   1024        conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 4, 27, 256)   0           batch_normalization_746[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 4, 27, 256)   590080      activation_735[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_263 (Add)                   (None, 4, 27, 256)   0           add_262[0][0]                    \n",
      "                                                                 conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_747 (BatchN (None, 4, 27, 256)   1024        add_263[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_736 (Activation)     (None, 4, 27, 256)   0           batch_normalization_747[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 4, 27, 256)   590080      activation_736[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_748 (BatchN (None, 4, 27, 256)   1024        conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_737 (Activation)     (None, 4, 27, 256)   0           batch_normalization_748[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 4, 27, 256)   590080      activation_737[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_264 (Add)                   (None, 4, 27, 256)   0           add_263[0][0]                    \n",
      "                                                                 conv2d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_749 (BatchN (None, 4, 27, 256)   1024        add_264[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_738 (Activation)     (None, 4, 27, 256)   0           batch_normalization_749[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 4, 27, 256)   590080      activation_738[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_750 (BatchN (None, 4, 27, 256)   1024        conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_739 (Activation)     (None, 4, 27, 256)   0           batch_normalization_750[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 4, 27, 256)   590080      activation_739[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_265 (Add)                   (None, 4, 27, 256)   0           add_264[0][0]                    \n",
      "                                                                 conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_751 (BatchN (None, 4, 27, 256)   1024        add_265[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_740 (Activation)     (None, 4, 27, 256)   0           batch_normalization_751[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 4, 27, 256)   590080      activation_740[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_752 (BatchN (None, 4, 27, 256)   1024        conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_741 (Activation)     (None, 4, 27, 256)   0           batch_normalization_752[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 4, 27, 256)   590080      activation_741[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_266 (Add)                   (None, 4, 27, 256)   0           add_265[0][0]                    \n",
      "                                                                 conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_753 (BatchN (None, 4, 27, 256)   1024        add_266[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_742 (Activation)     (None, 4, 27, 256)   0           batch_normalization_753[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 4, 27, 256)   590080      activation_742[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_754 (BatchN (None, 4, 27, 256)   1024        conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_743 (Activation)     (None, 4, 27, 256)   0           batch_normalization_754[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 4, 27, 256)   590080      activation_743[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_267 (Add)                   (None, 4, 27, 256)   0           add_266[0][0]                    \n",
      "                                                                 conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_755 (BatchN (None, 4, 27, 256)   1024        add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_744 (Activation)     (None, 4, 27, 256)   0           batch_normalization_755[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 2, 14, 512)   1180160     activation_744[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_756 (BatchN (None, 2, 14, 512)   2048        conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_745 (Activation)     (None, 2, 14, 512)   0           batch_normalization_756[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 2, 14, 512)   131584      add_267[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 2, 14, 512)   2359808     activation_745[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_268 (Add)                   (None, 2, 14, 512)   0           conv2d_788[0][0]                 \n",
      "                                                                 conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_757 (BatchN (None, 2, 14, 512)   2048        add_268[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_746 (Activation)     (None, 2, 14, 512)   0           batch_normalization_757[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 2, 14, 512)   2359808     activation_746[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_758 (BatchN (None, 2, 14, 512)   2048        conv2d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_747 (Activation)     (None, 2, 14, 512)   0           batch_normalization_758[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 2, 14, 512)   2359808     activation_747[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_269 (Add)                   (None, 2, 14, 512)   0           add_268[0][0]                    \n",
      "                                                                 conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_759 (BatchN (None, 2, 14, 512)   2048        add_269[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_748 (Activation)     (None, 2, 14, 512)   0           batch_normalization_759[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 2, 14, 512)   2359808     activation_748[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_760 (BatchN (None, 2, 14, 512)   2048        conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_749 (Activation)     (None, 2, 14, 512)   0           batch_normalization_760[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 2, 14, 512)   2359808     activation_749[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_270 (Add)                   (None, 2, 14, 512)   0           add_269[0][0]                    \n",
      "                                                                 conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_761 (BatchN (None, 2, 14, 512)   2048        add_270[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_750 (Activation)     (None, 2, 14, 512)   0           batch_normalization_761[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 1, 1, 512)    0           activation_750[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 512)          0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 58)           29754       flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_762 (BatchN (None, 58)           232         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 58)           0           batch_normalization_762[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 41)           2419        dropout_24[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,332,757\n",
      "Trainable params: 21,317,409\n",
      "Non-trainable params: 15,348\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "97/97 [==============================] - 29s 298ms/step - loss: 2.2723 - acc: 0.6447 - val_loss: 1.0168 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81941, saving model to model/mfcc6/LGD_fold4_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.1365 - acc: 0.6910 - val_loss: 1.5002 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.81941\n",
      "Epoch 3/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0771 - acc: 0.7085 - val_loss: 1.1282 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.81941\n",
      "Epoch 4/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0688 - acc: 0.7053 - val_loss: 1.2514 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81941\n",
      "Epoch 5/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.1284 - acc: 0.7041 - val_loss: 1.0698 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81941\n",
      "Epoch 6/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0259 - acc: 0.7264 - val_loss: 1.0445 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81941\n",
      "Epoch 7/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0454 - acc: 0.7245 - val_loss: 1.3435 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81941\n",
      "Epoch 8/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0416 - acc: 0.7257 - val_loss: 1.1452 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81941\n",
      "Epoch 9/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0063 - acc: 0.7365 - val_loss: 1.3434 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81941\n",
      "Epoch 10/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.1803 - acc: 0.6952 - val_loss: 2.1399 - val_acc: 0.7332\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81941\n",
      "Epoch 11/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0820 - acc: 0.7268 - val_loss: 1.0363 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81941 to 0.82210, saving model to model/mfcc6/LGD_fold4_co_resnet.h5\n",
      "Epoch 12/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0935 - acc: 0.7183 - val_loss: 1.8764 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82210\n",
      "Epoch 13/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0991 - acc: 0.7156 - val_loss: 1.2118 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82210\n",
      "Epoch 14/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0996 - acc: 0.7146 - val_loss: 1.3374 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82210\n",
      "Epoch 15/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0960 - acc: 0.7206 - val_loss: 1.1062 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82210\n",
      "Epoch 16/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0372 - acc: 0.7303 - val_loss: 1.3613 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.82210\n",
      "Epoch 17/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0999 - acc: 0.7241 - val_loss: 1.0999 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.82210\n",
      "Epoch 18/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0249 - acc: 0.7427 - val_loss: 0.9930 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.82210\n",
      "Epoch 19/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 2.0075 - acc: 0.7510 - val_loss: 1.0019 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.82210\n",
      "Epoch 20/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0288 - acc: 0.7365 - val_loss: 1.6144 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82210\n",
      "Epoch 21/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 2.0193 - acc: 0.7416 - val_loss: 1.8251 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.82210\n",
      "Epoch 22/3000\n",
      "97/97 [==============================] - 23s 236ms/step - loss: 1.9915 - acc: 0.7466 - val_loss: 1.7974 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82210\n",
      "Epoch 23/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9765 - acc: 0.7482 - val_loss: 1.6918 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.82210\n",
      "Epoch 24/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9509 - acc: 0.7448 - val_loss: 1.5141 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.82210\n",
      "Epoch 25/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9390 - acc: 0.7572 - val_loss: 1.6826 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.82210\n",
      "Epoch 26/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9065 - acc: 0.7568 - val_loss: 1.4162 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.82210\n",
      "Epoch 27/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9023 - acc: 0.7606 - val_loss: 1.3482 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82210\n",
      "Epoch 28/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9000 - acc: 0.7635 - val_loss: 1.2256 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82210\n",
      "Epoch 29/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9242 - acc: 0.7551 - val_loss: 1.1220 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82210\n",
      "Epoch 30/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9020 - acc: 0.7651 - val_loss: 1.1317 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82210\n",
      "Epoch 31/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8990 - acc: 0.7626 - val_loss: 1.1677 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82210\n",
      "Epoch 32/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9336 - acc: 0.7504 - val_loss: 1.0943 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82210\n",
      "Epoch 33/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8872 - acc: 0.7647 - val_loss: 1.2519 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82210\n",
      "Epoch 34/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8905 - acc: 0.7614 - val_loss: 1.2821 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82210\n",
      "Epoch 35/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9383 - acc: 0.7566 - val_loss: 1.3324 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82210\n",
      "Epoch 36/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.9243 - acc: 0.7607 - val_loss: 1.4185 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82210\n",
      "Epoch 37/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8730 - acc: 0.7609 - val_loss: 1.3149 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82210\n",
      "Epoch 38/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8516 - acc: 0.7738 - val_loss: 1.2701 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82210\n",
      "Epoch 39/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8624 - acc: 0.7763 - val_loss: 1.3520 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.82210\n",
      "Epoch 40/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8594 - acc: 0.7679 - val_loss: 1.5098 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82210\n",
      "Epoch 41/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8805 - acc: 0.7669 - val_loss: 1.5651 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82210\n",
      "Epoch 42/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8723 - acc: 0.7736 - val_loss: 2.2891 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82210\n",
      "Epoch 43/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8998 - acc: 0.7631 - val_loss: 1.8922 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82210\n",
      "Epoch 44/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8513 - acc: 0.7732 - val_loss: 1.5156 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82210\n",
      "Epoch 45/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8313 - acc: 0.7767 - val_loss: 1.3560 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.82210\n",
      "Epoch 46/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8303 - acc: 0.7809 - val_loss: 1.2838 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.82210\n",
      "Epoch 47/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8217 - acc: 0.7833 - val_loss: 1.7174 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.82210\n",
      "Epoch 48/3000\n",
      "97/97 [==============================] - 23s 237ms/step - loss: 1.8501 - acc: 0.7686 - val_loss: 1.9234 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.82210\n",
      "Epoch 00048: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_3===\n",
      "semi loading: model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 32, 216, 64)  3200        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 32, 216, 64)  256         conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 32, 216, 64)  0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_368[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 16, 108, 64)  36928       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 16, 108, 64)  256         conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 16, 108, 64)  0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 16, 108, 64)  36928       activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 16, 108, 64)  0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 16, 108, 64)  256         add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 16, 108, 64)  0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 16, 108, 64)  36928       activation_370[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 16, 108, 64)  256         conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 16, 108, 64)  0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 16, 108, 64)  36928       activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 16, 108, 64)  0           add_132[0][0]                    \n",
      "                                                                 conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 16, 108, 64)  256         add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 16, 108, 64)  0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 16, 108, 64)  36928       activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 16, 108, 64)  256         conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 16, 108, 64)  0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 16, 108, 64)  36928       activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 16, 108, 64)  0           add_133[0][0]                    \n",
      "                                                                 conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 16, 108, 64)  256         add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 16, 108, 64)  0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 8, 54, 128)   73856       activation_374[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 8, 54, 128)   512         conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 8, 54, 128)   0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 8, 54, 128)   8320        add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 8, 54, 128)   147584      activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 8, 54, 128)   0           conv2d_398[0][0]                 \n",
      "                                                                 conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 8, 54, 128)   512         add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 8, 54, 128)   0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 8, 54, 128)   147584      activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 8, 54, 128)   512         conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 8, 54, 128)   0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 8, 54, 128)   147584      activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 8, 54, 128)   0           add_135[0][0]                    \n",
      "                                                                 conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 8, 54, 128)   512         add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 8, 54, 128)   0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 8, 54, 128)   147584      activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 8, 54, 128)   512         conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 8, 54, 128)   0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 8, 54, 128)   147584      activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 8, 54, 128)   0           add_136[0][0]                    \n",
      "                                                                 conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 8, 54, 128)   512         add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 8, 54, 128)   0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 8, 54, 128)   147584      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_387 (BatchN (None, 8, 54, 128)   512         conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 8, 54, 128)   0           batch_normalization_387[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 8, 54, 128)   147584      activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 8, 54, 128)   0           add_137[0][0]                    \n",
      "                                                                 conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_388 (BatchN (None, 8, 54, 128)   512         add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 8, 54, 128)   0           batch_normalization_388[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 4, 27, 256)   295168      activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_389 (BatchN (None, 4, 27, 256)   1024        conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 4, 27, 256)   0           batch_normalization_389[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 4, 27, 256)   33024       add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 4, 27, 256)   590080      activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 4, 27, 256)   0           conv2d_407[0][0]                 \n",
      "                                                                 conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 4, 27, 256)   1024        add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 4, 27, 256)   0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 4, 27, 256)   590080      activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 4, 27, 256)   1024        conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 4, 27, 256)   0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 4, 27, 256)   590080      activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 4, 27, 256)   0           add_139[0][0]                    \n",
      "                                                                 conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 4, 27, 256)   1024        add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 4, 27, 256)   0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 4, 27, 256)   590080      activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 4, 27, 256)   1024        conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 4, 27, 256)   0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 4, 27, 256)   590080      activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 4, 27, 256)   0           add_140[0][0]                    \n",
      "                                                                 conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 4, 27, 256)   1024        add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 4, 27, 256)   0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 4, 27, 256)   590080      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 4, 27, 256)   1024        conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 4, 27, 256)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 4, 27, 256)   590080      activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 4, 27, 256)   0           add_141[0][0]                    \n",
      "                                                                 conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 4, 27, 256)   1024        add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 4, 27, 256)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 4, 27, 256)   590080      activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 4, 27, 256)   1024        conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 4, 27, 256)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 4, 27, 256)   590080      activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 4, 27, 256)   0           add_142[0][0]                    \n",
      "                                                                 conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 4, 27, 256)   1024        add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 4, 27, 256)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 4, 27, 256)   590080      activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 4, 27, 256)   1024        conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 4, 27, 256)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 4, 27, 256)   590080      activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 4, 27, 256)   0           add_143[0][0]                    \n",
      "                                                                 conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 4, 27, 256)   1024        add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 4, 27, 256)   0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 2, 14, 512)   1180160     activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 2, 14, 512)   2048        conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 2, 14, 512)   0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 2, 14, 512)   131584      add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 2, 14, 512)   2359808     activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 2, 14, 512)   0           conv2d_420[0][0]                 \n",
      "                                                                 conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 2, 14, 512)   2048        add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 2, 14, 512)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 2, 14, 512)   2359808     activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 2, 14, 512)   2048        conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 2, 14, 512)   0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 2, 14, 512)   2359808     activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 2, 14, 512)   0           add_145[0][0]                    \n",
      "                                                                 conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 2, 14, 512)   2048        add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 2, 14, 512)   0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 2, 14, 512)   2359808     activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 2, 14, 512)   2048        conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 2, 14, 512)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 2, 14, 512)   2359808     activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_147 (Add)                   (None, 2, 14, 512)   0           add_146[0][0]                    \n",
      "                                                                 conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 2, 14, 512)   2048        add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 2, 14, 512)   0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 1, 1, 512)    0           activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1, 1, 512)    0           average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 512)          0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 49)           25137       flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 49)           196         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 49)           0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 41)           2050        dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,327,735\n",
      "Trainable params: 21,312,405\n",
      "Non-trainable params: 15,330\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "390/390 [==============================] - 42s 108ms/step - loss: 2.0031 - acc: 0.7454 - val_loss: 0.9261 - val_acc: 0.7978\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79784, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 2/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 2.1345 - acc: 0.7183 - val_loss: 5.5527 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79784\n",
      "Epoch 3/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 2.0774 - acc: 0.6997 - val_loss: 1.1356 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79784\n",
      "Epoch 4/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.7720 - acc: 0.7300 - val_loss: 1.0821 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79784\n",
      "Epoch 5/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.7905 - acc: 0.7333 - val_loss: 1.3461 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79784\n",
      "Epoch 6/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.8427 - acc: 0.7072 - val_loss: 1.0537 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79784\n",
      "Epoch 7/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6715 - acc: 0.7567 - val_loss: 1.0276 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79784\n",
      "Epoch 8/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6250 - acc: 0.7686 - val_loss: 0.9414 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.79784 to 0.80593, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 9/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6379 - acc: 0.7673 - val_loss: 1.0006 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80593\n",
      "Epoch 10/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6670 - acc: 0.7665 - val_loss: 1.0503 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80593\n",
      "Epoch 11/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6114 - acc: 0.7747 - val_loss: 0.9794 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80593\n",
      "Epoch 12/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6408 - acc: 0.7647 - val_loss: 1.0924 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80593\n",
      "Epoch 13/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5904 - acc: 0.7762 - val_loss: 0.9016 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80593\n",
      "Epoch 14/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5996 - acc: 0.7782 - val_loss: 1.5326 - val_acc: 0.6927\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80593\n",
      "Epoch 15/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 2.0257 - acc: 0.7064 - val_loss: 0.8980 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80593\n",
      "Epoch 16/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6810 - acc: 0.7561 - val_loss: 1.0901 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80593\n",
      "Epoch 17/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.8378 - acc: 0.7519 - val_loss: 0.9990 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80593\n",
      "Epoch 18/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 2.0048 - acc: 0.7461 - val_loss: 0.9081 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.80593 to 0.80863, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 19/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.6894 - acc: 0.7715 - val_loss: 0.9016 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.80863 to 0.81132, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 20/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5906 - acc: 0.7837 - val_loss: 0.9507 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.81132\n",
      "Epoch 21/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5460 - acc: 0.7910 - val_loss: 0.9246 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.81132\n",
      "Epoch 22/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5326 - acc: 0.8013 - val_loss: 0.9672 - val_acc: 0.7951\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.81132\n",
      "Epoch 23/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5134 - acc: 0.7986 - val_loss: 0.9972 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81132\n",
      "Epoch 24/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5488 - acc: 0.7990 - val_loss: 0.9113 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.81132 to 0.83019, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 25/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5425 - acc: 0.7913 - val_loss: 0.8566 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83019\n",
      "Epoch 26/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4959 - acc: 0.8063 - val_loss: 0.9184 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83019\n",
      "Epoch 27/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5277 - acc: 0.8020 - val_loss: 0.9506 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83019\n",
      "Epoch 28/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5859 - acc: 0.7802 - val_loss: 0.8955 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83019\n",
      "Epoch 29/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5641 - acc: 0.7896 - val_loss: 0.8926 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83019\n",
      "Epoch 30/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5550 - acc: 0.7962 - val_loss: 0.9579 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83019\n",
      "Epoch 31/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5454 - acc: 0.7975 - val_loss: 0.8965 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83019\n",
      "Epoch 32/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5776 - acc: 0.7958 - val_loss: 0.9392 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83019\n",
      "Epoch 33/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.9135 - acc: 0.7659 - val_loss: 1.5286 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83019\n",
      "Epoch 34/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.9390 - acc: 0.7698 - val_loss: 1.5728 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83019\n",
      "Epoch 35/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.8680 - acc: 0.7820 - val_loss: 1.5708 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83019\n",
      "Epoch 36/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.8989 - acc: 0.7732 - val_loss: 1.3407 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83019\n",
      "Epoch 37/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5871 - acc: 0.7960 - val_loss: 0.8894 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83019\n",
      "Epoch 38/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.5210 - acc: 0.8003 - val_loss: 0.9384 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83019\n",
      "Epoch 39/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4939 - acc: 0.8111 - val_loss: 0.8711 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83019\n",
      "Epoch 40/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4990 - acc: 0.8058 - val_loss: 0.8624 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83019\n",
      "Epoch 41/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4731 - acc: 0.8167 - val_loss: 0.9056 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83019\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4871 - acc: 0.8073 - val_loss: 0.8813 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83019\n",
      "Epoch 43/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4715 - acc: 0.8088 - val_loss: 0.8737 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83019\n",
      "Epoch 44/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4627 - acc: 0.8171 - val_loss: 0.8846 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83019\n",
      "Epoch 45/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4705 - acc: 0.8126 - val_loss: 0.8670 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83019\n",
      "Epoch 46/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4750 - acc: 0.8109 - val_loss: 0.8575 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.83019 to 0.83288, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 47/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4536 - acc: 0.8179 - val_loss: 0.8668 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83288\n",
      "Epoch 48/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4467 - acc: 0.8192 - val_loss: 0.8630 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83288\n",
      "Epoch 49/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4578 - acc: 0.8200 - val_loss: 0.8483 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83288\n",
      "Epoch 50/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4369 - acc: 0.8201 - val_loss: 0.8637 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83288\n",
      "Epoch 51/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4517 - acc: 0.8204 - val_loss: 0.8672 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83288\n",
      "Epoch 52/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4532 - acc: 0.8232 - val_loss: 0.8738 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83288\n",
      "Epoch 53/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4326 - acc: 0.8233 - val_loss: 0.8355 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.83288 to 0.83827, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 54/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4627 - acc: 0.8171 - val_loss: 0.8695 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83827\n",
      "Epoch 55/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4354 - acc: 0.8246 - val_loss: 0.8606 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83827\n",
      "Epoch 56/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4012 - acc: 0.8340 - val_loss: 0.8580 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83827\n",
      "Epoch 57/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4202 - acc: 0.8270 - val_loss: 0.8449 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.83827 to 0.84367, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 58/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4290 - acc: 0.8238 - val_loss: 0.9061 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84367\n",
      "Epoch 59/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4215 - acc: 0.8211 - val_loss: 0.8876 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84367\n",
      "Epoch 60/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4135 - acc: 0.8195 - val_loss: 0.8433 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84367\n",
      "Epoch 61/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4060 - acc: 0.8255 - val_loss: 0.8697 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84367\n",
      "Epoch 62/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4323 - acc: 0.8159 - val_loss: 0.8661 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84367\n",
      "Epoch 63/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4294 - acc: 0.8230 - val_loss: 0.8392 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.84367 to 0.85175, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 64/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4277 - acc: 0.8189 - val_loss: 0.8131 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.85175\n",
      "Epoch 65/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4177 - acc: 0.8252 - val_loss: 0.8503 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.85175\n",
      "Epoch 66/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4533 - acc: 0.8218 - val_loss: 0.8618 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.85175\n",
      "Epoch 67/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4658 - acc: 0.8202 - val_loss: 0.8395 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.85175\n",
      "Epoch 68/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4179 - acc: 0.8272 - val_loss: 0.8502 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.85175\n",
      "Epoch 69/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4412 - acc: 0.8276 - val_loss: 0.8211 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00069: val_acc improved from 0.85175 to 0.85445, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 70/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4135 - acc: 0.8291 - val_loss: 0.8342 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.85445\n",
      "Epoch 71/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4261 - acc: 0.8283 - val_loss: 0.8339 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.85445\n",
      "Epoch 72/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3986 - acc: 0.8321 - val_loss: 0.8209 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.85445\n",
      "Epoch 73/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3951 - acc: 0.8314 - val_loss: 0.8201 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.85445\n",
      "Epoch 74/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3904 - acc: 0.8346 - val_loss: 0.8321 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.85445\n",
      "Epoch 75/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3900 - acc: 0.8290 - val_loss: 0.8282 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85445\n",
      "Epoch 76/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3935 - acc: 0.8304 - val_loss: 0.8291 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85445\n",
      "Epoch 77/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3902 - acc: 0.8342 - val_loss: 0.8237 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.85445 to 0.85445, saving model to model/mfcc6/LGD_fold3_co_resnet.h5\n",
      "Epoch 78/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4104 - acc: 0.8265 - val_loss: 0.8325 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85445\n",
      "Epoch 79/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3822 - acc: 0.8360 - val_loss: 0.8339 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85445\n",
      "Epoch 80/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4055 - acc: 0.8318 - val_loss: 0.8421 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.85445\n",
      "Epoch 81/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3860 - acc: 0.8344 - val_loss: 0.8335 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85445\n",
      "Epoch 82/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3729 - acc: 0.8342 - val_loss: 0.8212 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85445\n",
      "Epoch 83/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3839 - acc: 0.8341 - val_loss: 0.8270 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85445\n",
      "Epoch 84/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3952 - acc: 0.8300 - val_loss: 0.8255 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85445\n",
      "Epoch 85/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4005 - acc: 0.8264 - val_loss: 0.8381 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85445\n",
      "Epoch 86/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3965 - acc: 0.8236 - val_loss: 0.8263 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85445\n",
      "Epoch 87/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4006 - acc: 0.8304 - val_loss: 0.8248 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85445\n",
      "Epoch 88/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.4031 - acc: 0.8323 - val_loss: 0.8314 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85445\n",
      "Epoch 89/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3781 - acc: 0.8365 - val_loss: 0.8372 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 4e-06.\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85445\n",
      "Epoch 90/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3734 - acc: 0.8370 - val_loss: 0.8279 - val_acc: 0.8518\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85445\n",
      "Epoch 91/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3784 - acc: 0.8297 - val_loss: 0.8294 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85445\n",
      "Epoch 92/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3658 - acc: 0.8352 - val_loss: 0.8274 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85445\n",
      "Epoch 93/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3860 - acc: 0.8357 - val_loss: 0.8258 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85445\n",
      "Epoch 94/3000\n",
      "390/390 [==============================] - 37s 94ms/step - loss: 1.3910 - acc: 0.8372 - val_loss: 0.8295 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85445\n",
      "Epoch 00094: early stopping\n",
      "(3339, 64, 431, 1) (3339, 41)\n",
      "(6250, 64, 431, 1) (6250, 41)\n",
      "===train semi_2===\n",
      "semi loading: model/mfcc6/LGD_fold2_co_resnet.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 64, 431, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 32, 216, 64)  3200        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 32, 216, 64)  256         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 32, 216, 64)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 108, 64)  0           activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 16, 108, 64)  4160        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 108, 64)  256         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 108, 64)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 16, 108, 64)  36928       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 108, 64)  256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 108, 64)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 16, 108, 256) 16640       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 16, 108, 256) 16640       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 16, 108, 256) 0           conv2d_114[0][0]                 \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 108, 256) 1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 108, 256) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 108, 64)  16448       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 16, 108, 64)  256         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 108, 64)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 108, 64)  36928       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 16, 108, 64)  256         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 108, 64)  0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 108, 256) 16640       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 16, 108, 256) 0           add_41[0][0]                     \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 16, 108, 256) 1024        add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 108, 256) 0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 108, 64)  16448       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 16, 108, 64)  256         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 16, 108, 64)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 16, 108, 64)  36928       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 108, 64)  256         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 16, 108, 64)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 16, 108, 256) 16640       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 16, 108, 256) 0           add_42[0][0]                     \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 16, 108, 256) 1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 16, 108, 256) 0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 8, 54, 128)   32896       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 8, 54, 128)   512         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 8, 54, 128)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 8, 54, 128)   147584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 8, 54, 128)   512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 8, 54, 128)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 8, 54, 512)   131584      add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 8, 54, 512)   66048       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 8, 54, 512)   0           conv2d_124[0][0]                 \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 8, 54, 512)   2048        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 8, 54, 512)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 8, 54, 128)   65664       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 8, 54, 128)   512         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 8, 54, 128)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 8, 54, 128)   147584      activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 8, 54, 128)   512         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 8, 54, 128)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 8, 54, 512)   66048       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 8, 54, 512)   0           add_44[0][0]                     \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 8, 54, 512)   2048        add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 8, 54, 512)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 8, 54, 128)   65664       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 8, 54, 128)   512         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 8, 54, 128)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 8, 54, 128)   147584      activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 8, 54, 128)   512         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 8, 54, 128)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 8, 54, 512)   66048       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 8, 54, 512)   0           add_45[0][0]                     \n",
      "                                                                 conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 8, 54, 512)   2048        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 8, 54, 512)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 8, 54, 128)   65664       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 54, 128)   512         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 8, 54, 128)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 8, 54, 128)   147584      activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 54, 128)   512         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 8, 54, 128)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 8, 54, 512)   66048       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 8, 54, 512)   0           add_46[0][0]                     \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 54, 512)   2048        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 8, 54, 512)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 4, 27, 256)   131328      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 4, 27, 256)   1024        conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 4, 27, 256)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 4, 27, 256)   590080      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 4, 27, 256)   1024        conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 4, 27, 256)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 4, 27, 1024)  525312      add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 4, 27, 1024)  263168      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 4, 27, 1024)  0           conv2d_137[0][0]                 \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 4, 27, 1024)  4096        add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 4, 27, 256)   262400      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 4, 27, 256)   1024        conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 4, 27, 256)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 4, 27, 256)   590080      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 4, 27, 256)   1024        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 4, 27, 256)   0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 4, 27, 1024)  263168      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 4, 27, 1024)  0           add_48[0][0]                     \n",
      "                                                                 conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 4, 27, 1024)  4096        add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 4, 27, 256)   262400      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 4, 27, 256)   1024        conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 4, 27, 256)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 4, 27, 256)   590080      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 4, 27, 256)   1024        conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 4, 27, 256)   0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 4, 27, 1024)  263168      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 4, 27, 1024)  0           add_49[0][0]                     \n",
      "                                                                 conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 4, 27, 1024)  4096        add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 4, 27, 256)   262400      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 4, 27, 256)   1024        conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 4, 27, 256)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 4, 27, 256)   590080      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 4, 27, 256)   1024        conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 4, 27, 256)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 4, 27, 1024)  263168      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 4, 27, 1024)  0           add_50[0][0]                     \n",
      "                                                                 conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 4, 27, 1024)  4096        add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 4, 27, 256)   262400      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 4, 27, 256)   1024        conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 4, 27, 256)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 4, 27, 256)   590080      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 4, 27, 256)   1024        conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 4, 27, 256)   0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 4, 27, 1024)  263168      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 4, 27, 1024)  0           add_51[0][0]                     \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 4, 27, 1024)  4096        add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 4, 27, 256)   262400      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 4, 27, 256)   1024        conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 4, 27, 256)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 4, 27, 256)   590080      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 4, 27, 256)   1024        conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 4, 27, 256)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 4, 27, 1024)  263168      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 4, 27, 1024)  0           add_52[0][0]                     \n",
      "                                                                 conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 4, 27, 1024)  4096        add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 4, 27, 256)   262400      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 4, 27, 256)   1024        conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 4, 27, 256)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 4, 27, 256)   590080      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 4, 27, 256)   1024        conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 4, 27, 256)   0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 4, 27, 1024)  263168      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 4, 27, 1024)  0           add_53[0][0]                     \n",
      "                                                                 conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 4, 27, 1024)  4096        add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 4, 27, 256)   262400      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 4, 27, 256)   1024        conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 4, 27, 256)   0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 4, 27, 256)   590080      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 4, 27, 256)   1024        conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 4, 27, 256)   0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 4, 27, 1024)  263168      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 4, 27, 1024)  0           add_54[0][0]                     \n",
      "                                                                 conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 4, 27, 1024)  4096        add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 4, 27, 256)   262400      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 4, 27, 256)   1024        conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 4, 27, 256)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 4, 27, 256)   590080      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 4, 27, 256)   1024        conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 4, 27, 256)   0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 4, 27, 1024)  263168      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 4, 27, 1024)  0           add_55[0][0]                     \n",
      "                                                                 conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 4, 27, 1024)  4096        add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 4, 27, 256)   262400      activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 4, 27, 256)   1024        conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 4, 27, 256)   0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 4, 27, 256)   590080      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 4, 27, 256)   1024        conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 4, 27, 256)   0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 4, 27, 1024)  263168      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 4, 27, 1024)  0           add_56[0][0]                     \n",
      "                                                                 conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 27, 1024)  4096        add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 27, 256)   262400      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 27, 256)   1024        conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 4, 27, 256)   0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 27, 256)   590080      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 27, 256)   1024        conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 4, 27, 256)   0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 27, 1024)  263168      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 4, 27, 1024)  0           add_57[0][0]                     \n",
      "                                                                 conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 27, 1024)  4096        add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 27, 256)   262400      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 27, 256)   1024        conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 4, 27, 256)   0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 27, 256)   590080      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 27, 256)   1024        conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 4, 27, 256)   0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 4, 27, 1024)  263168      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 4, 27, 1024)  0           add_58[0][0]                     \n",
      "                                                                 conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 27, 1024)  4096        add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 4, 27, 256)   262400      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 27, 256)   1024        conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 4, 27, 256)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 4, 27, 256)   590080      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 27, 256)   1024        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 4, 27, 256)   0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 4, 27, 1024)  263168      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 4, 27, 1024)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 27, 1024)  4096        add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 4, 27, 256)   262400      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 4, 27, 256)   1024        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 4, 27, 256)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 4, 27, 256)   590080      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 4, 27, 256)   1024        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 4, 27, 256)   0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 4, 27, 1024)  263168      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 4, 27, 1024)  0           add_60[0][0]                     \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 4, 27, 1024)  4096        add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 4, 27, 256)   262400      activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 4, 27, 256)   1024        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 4, 27, 256)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 4, 27, 256)   590080      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 4, 27, 256)   1024        conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 4, 27, 256)   0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 4, 27, 1024)  263168      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 4, 27, 1024)  0           add_61[0][0]                     \n",
      "                                                                 conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 4, 27, 1024)  4096        add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 4, 27, 256)   262400      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 4, 27, 256)   1024        conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 4, 27, 256)   0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 4, 27, 256)   590080      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 4, 27, 256)   1024        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 4, 27, 256)   0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 4, 27, 1024)  263168      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 4, 27, 1024)  0           add_62[0][0]                     \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 4, 27, 1024)  4096        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 4, 27, 256)   262400      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 4, 27, 256)   1024        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 4, 27, 256)   0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 4, 27, 256)   590080      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 4, 27, 256)   1024        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 4, 27, 256)   0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 4, 27, 1024)  263168      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 4, 27, 1024)  0           add_63[0][0]                     \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 4, 27, 1024)  4096        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 4, 27, 256)   262400      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 4, 27, 256)   1024        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 4, 27, 256)   0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 4, 27, 256)   590080      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 4, 27, 256)   1024        conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 4, 27, 256)   0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 4, 27, 1024)  263168      activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 4, 27, 1024)  0           add_64[0][0]                     \n",
      "                                                                 conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 4, 27, 1024)  4096        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 4, 27, 256)   262400      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 4, 27, 256)   1024        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 4, 27, 256)   0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 4, 27, 256)   590080      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 4, 27, 256)   1024        conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 4, 27, 256)   0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 4, 27, 1024)  263168      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 4, 27, 1024)  0           add_65[0][0]                     \n",
      "                                                                 conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 4, 27, 1024)  4096        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 4, 27, 256)   262400      activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 4, 27, 256)   1024        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 4, 27, 256)   0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 4, 27, 256)   590080      activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 4, 27, 256)   1024        conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 4, 27, 256)   0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 4, 27, 1024)  263168      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 4, 27, 1024)  0           add_66[0][0]                     \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 4, 27, 1024)  4096        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 4, 27, 256)   262400      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 4, 27, 256)   1024        conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 4, 27, 256)   0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 4, 27, 256)   590080      activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 4, 27, 256)   1024        conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 4, 27, 256)   0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 4, 27, 1024)  263168      activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 4, 27, 1024)  0           add_67[0][0]                     \n",
      "                                                                 conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 4, 27, 1024)  4096        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 4, 27, 256)   262400      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 4, 27, 256)   1024        conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 4, 27, 256)   0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 4, 27, 256)   590080      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 4, 27, 256)   1024        conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 4, 27, 256)   0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 4, 27, 1024)  263168      activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 4, 27, 1024)  0           add_68[0][0]                     \n",
      "                                                                 conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 4, 27, 1024)  4096        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 4, 27, 256)   262400      activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 4, 27, 256)   1024        conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 4, 27, 256)   0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 4, 27, 256)   590080      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 4, 27, 256)   1024        conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 4, 27, 256)   0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 4, 27, 1024)  263168      activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 4, 27, 1024)  0           add_69[0][0]                     \n",
      "                                                                 conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 4, 27, 1024)  4096        add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 4, 27, 1024)  0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 2, 14, 512)   524800      activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 2, 14, 512)   2048        conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 2, 14, 512)   0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 2, 14, 512)   2359808     activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 2, 14, 512)   2048        conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 2, 14, 512)   0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 2, 14, 2048)  2099200     add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 2, 14, 2048)  0           conv2d_207[0][0]                 \n",
      "                                                                 conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 2, 14, 2048)  8192        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 2, 14, 512)   1049088     activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 2, 14, 512)   2048        conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 2, 14, 512)   0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 2, 14, 512)   2359808     activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 2, 14, 512)   2048        conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 2, 14, 512)   0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 2, 14, 2048)  0           add_71[0][0]                     \n",
      "                                                                 conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 2, 14, 2048)  8192        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 2, 14, 512)   1049088     activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 2, 14, 512)   2048        conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 2, 14, 512)   0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 2, 14, 512)   2359808     activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 2, 14, 512)   2048        conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 2, 14, 512)   0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 2, 14, 2048)  1050624     activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 2, 14, 2048)  0           add_72[0][0]                     \n",
      "                                                                 conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 2, 14, 2048)  8192        add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 2, 14, 2048)  0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 2048)   0           activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 1, 2048)   0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2048)         0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 54)           110646      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 54)           216         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 54)           0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 41)           2255        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 42,749,661\n",
      "Trainable params: 42,651,889\n",
      "Non-trainable params: 97,772\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,512,8,54] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_15/Adam/gradients/AddN_359-1-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_15/Adam/gradients/batch_normalization_115/cond/FusedBatchNorm/Switch_grad/cond_grad, PermConstNHWCToNCHW-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: batch_normalization_143/Shape/_117841 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3353_batch_normalization_143/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f335426f1ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_semi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY_semi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===train semi_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel_semi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_unverified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_semi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_semi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# 0=>0.88140\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 1 => 0.85714\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b45acce6126a>\u001b[0m in \u001b[0;36mtrain_unverified\u001b[0;34m(X_semi, Y_semi, fold)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#                         use_multiprocessing = True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#                         batch_size=batchSize,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#                         initial_epoch = int(patien/20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                        )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,512,8,54] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_15/Adam/gradients/AddN_359-1-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_15/Adam/gradients/batch_normalization_115/cond/FusedBatchNorm/Switch_grad/cond_grad, PermConstNHWCToNCHW-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: batch_normalization_143/Shape/_117841 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3353_batch_normalization_143/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "for fold in val_set_num:\n",
    "    X, y = getTrainData()\n",
    "    # X = np.swapaxes(X,2,3)\n",
    "    X_train, Y_train, X_valid, Y_valid = split_data(X, y, fold) #fold\n",
    "    # X_train, X_valid = normalize(X_train, X_valid)\n",
    "    print(X_train.shape, Y_train.shape)\n",
    "\n",
    "    # X_train = np.swapaxes(X_train,1,3)\n",
    "    # X_valid = np.swapaxes(X_valid,1,3)\n",
    "#     print(\"===train verified_fold\"+str(fold)+'_'+feature_type+'===')\n",
    "#     model,model_num = train_valid(X_train,Y_train,X_valid,Y_valid,fold)\n",
    "    X_semi , Y_semi = get_semi_data(X_train,Y_train)\n",
    "    print('===train semi_'+str(fold)+'===')\n",
    "    model_semi = train_unverified(X_semi,Y_semi,fold)\n",
    "# MFCC6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC7\n",
    "#0=>0.84367(resnet3_semi)/0.83288(not semi)\n",
    "#1=>0.84367(resnet3_semi)/0.81132(not semi)\n",
    "#2=>0.84097(resnet2_semi)/0.81671(not semi)\n",
    "#3=>0.88410(resnet1_semi)/0.84367\n",
    "#4=>0.82210(resnet4_semi)/0.74663(not semi)\n",
    "#5 => 0.82749(resnet4_semi)/0.77628\n",
    "#6 => 0.85445(resnet2_semi)/0.78437\n",
    "#7 => 0.82749(resnet3_semi)/0.76280\n",
    "#8 => 0.83288(resnet3_semi)/0.78706\n",
    "# 9=> 0.90566(resnet1_semi)/0.87332\n",
    "\n",
    "# 0=>0.88140 (co)\n",
    "# 1 => 0.85714 (co)\n",
    "# 2=>0.87062 (co)\n",
    "# 3=>0.88949 (co)\n",
    "# 4=>0.82749 (co)\n",
    "# 5=>0.84367 (co)\n",
    "# 6=>0.84906(X) (semi)\n",
    "# 7=>0.84906 (co)\n",
    "# 8=>0.83019(X) (smi)\n",
    "# 9=>0.90566 (co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC6\n",
    "# 0=> 0.81941 (not semi)\n",
    "# 1=>0.83019 (semi)\n",
    "# 2=>0.81941 (semi)\n",
    "# 3=>0.85984 (resnet1_not semi)、0.78437\n",
    "# 4=>0.84367 (renet1_not semi)、0.81132\n",
    "# 5=> 0.85175(resnet3_semi)、0.82749(not semi)\n",
    "#6 => 0.85904(resnet4_semi)、0.79784(not semi)\n",
    "# 7=>0.88949(resnet2_semi)、0.83558(not semi)\n",
    "# 8=>0.83558(resnet1 not semi)\n",
    "# 9=>0.86792(resnet2_semi)、0.8112(not semi)\n",
    "\n",
    "# 0 => 0.81941\n",
    "#1=>0.88410\n",
    "#2=>0.83558\n",
    "# 3=>0.84367 (X)\n",
    "# 4+>0.86523 (X)\n",
    "# 5=> (X) (前五個fold街從頭train)\n",
    "\n",
    "# 3=>0.85445 (x)\n",
    "# 4=>0.82110 (x)\n",
    "# 5=>0.84636 (X)\n",
    "# 6=>0.86792\n",
    "# 7=>0.88949\n",
    "# 8=>0.88410\n",
    "# 9=>0.87332"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
