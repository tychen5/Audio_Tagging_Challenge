{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "# from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import pandas as pd # data frame\n",
    "import numpy as np # matrix math\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "import matplotlib.pyplot as plt # to view graphs\n",
    "import wave\n",
    "from math import log, floor\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.cluster import KMeans\n",
    "import sys, os\n",
    "import random,math\n",
    "from tqdm import tqdm ##\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils import shuffle # shuffling of data\n",
    "from random import sample # random selection\n",
    "from tqdm import tqdm # progress bar\n",
    "# audio processing\n",
    "from scipy import signal # audio processing\n",
    "from scipy.fftpack import dct\n",
    "import librosa # library for audio processing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "from keras.utils import *\n",
    "from sklearn.ensemble import *\n",
    "import pickle\n",
    "from bayes_opt import BayesianOptimization\n",
    "from logHandler import Logger\n",
    "from utils import readCSV, getPath, writePickle,readPickle\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import History ,ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = np.load('feature/fbank/mean.npy')\n",
    "# std = np.load('feature/fbank/std.npy')\n",
    "autoencoder = load_model('model/lgd_dense_AE3.h5')\n",
    "X_un_dense = np.load('feature/fbank2/semi/X_un_dense.npy')\n",
    "Y_un_dense = np.load('feature/fbank2/semi/Y_un_dense.npy')\n",
    "X_test_dense = np.load('feature/fbank2/semi/X_test_dense.npy')\n",
    "Y_test_dense = np.load('feature/fbank2/semi/Y_test_dense.npy')\n",
    "\n",
    "base_path = 'feature/fbank2/'#'/tmp2/b03902110/newphase1'\n",
    "base_data_path = 'feature/fbank2/'#os.path.join(base_path, 'data')\n",
    "num_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def getTrainData():\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_fold):\n",
    "        fileX = os.path.join(base_data_path, 'X/X' + str(i+1) + '.npy')\n",
    "        fileY = os.path.join(base_data_path, 'y/y' + str(i+1) + '.npy')\n",
    "        \n",
    "        X.append(np.load(fileX))\n",
    "        y.append(np.load(fileY))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, idx):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(num_fold):\n",
    "        if i == idx:\n",
    "            X_val = X[i]\n",
    "            y_val = y[i]\n",
    "            continue\n",
    "        if X_train == []:\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "# def normalize(X_train, X_val):\n",
    "#     X_train = (X_train - mean)/std\n",
    "#     X_val = (X_val - mean)/std\n",
    "\n",
    "#     return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:32: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 63, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 63, 64)       1856      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 63, 32)       57376     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 63, 1)        897       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8064)              65036160  \n",
      "=================================================================\n",
      "Total params: 65,096,289\n",
      "Trainable params: 65,096,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3339, 8064) (371, 8064) (3339, 41) (371, 41)\n",
      "(15163, 8064) (15163, 41)\n"
     ]
    }
   ],
   "source": [
    "val_set_num = [x+1 for x in range(num_fold)]#str(sys.argv[1])\n",
    "X, y = getTrainData()\n",
    "X_train, Y_train, X_val, Y_valid = split_data(X, y, val_set_num[0])\n",
    "# X_train, X_val = normalize(X_train, X_val)\n",
    "\n",
    "model = Model(input = autoencoder.layers[0].input, output = autoencoder.layers[5].output)\n",
    "model.summary()\n",
    "X_train = model.predict(X_train)\n",
    "X_valid = model.predict(X_val)\n",
    "print(X_train.shape,X_valid.shape,Y_train.shape,Y_valid.shape)\n",
    "X_semi = np.concatenate((X_un_dense,X_test_dense))\n",
    "Y_semi = np.concatenate((Y_un_dense,Y_test_dense))\n",
    "X_semi , Y_semi = _shuffle(X_semi,Y_semi)\n",
    "Y_semi = to_categorical(Y_semi,num_classes=41)\n",
    "print(X_semi.shape,Y_semi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.47078 73.536835\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train,X_semi))\n",
    "X = np.concatenate((X,X_valid))\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "print(mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3339, 8064) (15163, 8064) (371, 8064) (5763, 8064) (9400, 8064)\n"
     ]
    }
   ],
   "source": [
    "X_train = (X_train - mean) / std\n",
    "X_valid = (X_valid - mean) / std\n",
    "X_semi = (X_semi - mean) / std\n",
    "print(X_train.shape , X_semi.shape , X_valid.shape , X_un_dense.shape , X_test_dense.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8064)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8064)         32256       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4096)         33034240    batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12160)        0           input_1[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12160)        48640       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4096)         49811456    batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 12160)        0           input_1[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 12160)        48640       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4096)         49811456    batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 4096)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 12160)        0           input_1[0][0]                    \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 12160)        48640       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4096)         49811456    batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 4096)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 12160)        0           input_1[0][0]                    \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 12160)        48640       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2048)         24905728    batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2048)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 10112)        0           input_1[0][0]                    \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 10112)        40448       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2048)         20711424    batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2048)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2048)         8192        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 41)           84009       batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 228,445,225\n",
      "Trainable params: 228,307,497\n",
      "Non-trainable params: 137,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ = Input(shape=(8064,))\n",
    "\n",
    "bn = BatchNormalization()(input_)\n",
    "dense = Dense(4096,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "dropout = Dropout(0.05)(dense)\n",
    "\n",
    "# con = Concatenate()([dropout,input_])\n",
    "con = Concatenate()([input_,dropout])\n",
    "bn = BatchNormalization()(con)\n",
    "dense = Dense(4096,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "dropout = Dropout(0.05)(dense)\n",
    "\n",
    "\n",
    "con = Concatenate()([input_,dropout])\n",
    "# con = Concatenate()([dropout,input_])\n",
    "# con = K.concatenate([input_,dense])\n",
    "bn = BatchNormalization()(con)\n",
    "dense = Dense(4096,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000004))(bn)\n",
    "dropout = Dropout(0.055)(dense)\n",
    "\n",
    "con = Concatenate()([input_,dropout])\n",
    "# con = Concatenate()([dropout,input_])\n",
    "bn = BatchNormalization()(con)\n",
    "dense = Dense(4096,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000004))(bn)\n",
    "dropout = Dropout(0.05)(dense)\n",
    "\n",
    "con = Concatenate()([input_,dropout])\n",
    "# con = Concatenate()([dropout,input_])\n",
    "# con = K.concatenate([input_,dense])\n",
    "bn = BatchNormalization()(con)\n",
    "dense = Dense(2048,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000002))(bn)\n",
    "dropout = Dropout(0.06)(dense)\n",
    "\n",
    "con = Concatenate()([input_,dropout])\n",
    "# con = Concatenate()([dropout,input_])\n",
    "bn = BatchNormalization()(con)\n",
    "dense = Dense(2048,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000002))(bn)\n",
    "dropout = Dropout(0.06)(dense)\n",
    "\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# con = Concatenate()([input_,dropout])\n",
    "# # con = K.concatenate([input_,dense])\n",
    "# bn = BatchNormalization()(con)\n",
    "# dense = Dense(1024,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000001))(bn)\n",
    "# dropout = Dropout(0.065)(dense)\n",
    "\n",
    "# con = Concatenate()([input_,dropout])\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# bn = BatchNormalization()(con)\n",
    "# dense = Dense(1024,activation='selu',kernel_initializer='lecun_normal',kernel_regularizer=l2(0.000001))(bn)\n",
    "# dropout = Dropout(0.065)(dense)\n",
    "\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# con = Concatenate()([input_,dropout])\n",
    "# bn = BatchNormalization()(con)\n",
    "# dense = Dense(512,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# dropout = Dropout(0.07)(dense)\n",
    "\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# con = Concatenate()([input_,dropout])\n",
    "# bn = BatchNormalization()(con)\n",
    "# dense = Dense(512,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# dropout = Dropout(0.07)(dense)\n",
    "\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# # con = K.concatenate([input_,dense])\n",
    "# bn = BatchNormalization()(dropout)\n",
    "# dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# dropout = Dropout(0.2)(dense)\n",
    "\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# # con = Concatenate()([dropout,input_])\n",
    "# bn = BatchNormalization()(dropout)\n",
    "# dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# dropout = Dropout(0.2)(dense)\n",
    "\n",
    "# con = Concatenate()([input_,dropout])\n",
    "# con = Concatenate()([dropout,input_])\n",
    "bn = BatchNormalization()(dropout)\n",
    "dense = Dense(41,activation='softmax',kernel_initializer='lecun_normal')(bn)\n",
    "# dense = Dense(41,activation='softmax',kernel_initializer='uniform',kernel_regularizer=l2(0.000008))(dropout)\n",
    "# dense = LeakyReLU()(dense)\n",
    "model = Model(inputs=input_, outputs=dense)\n",
    "# model = Sequential()\n",
    "# i\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(41,activation='linear',input_shape=(41,)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,X_semi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.concatenate((Y_train,Y_semi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18502 samples, validate on 371 samples\n",
      "Epoch 1/10000\n",
      "18502/18502 [==============================] - 40s 2ms/step - loss: 3.4166 - acc: 0.1304 - val_loss: 5.9909 - val_acc: 0.0809\n",
      "Epoch 2/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 3.2276 - acc: 0.1842 - val_loss: 4.3838 - val_acc: 0.0216\n",
      "Epoch 3/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 3.1274 - acc: 0.2023 - val_loss: 4.4900 - val_acc: 0.0485\n",
      "Epoch 4/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 3.0394 - acc: 0.2258 - val_loss: 5.9567 - val_acc: 0.0485\n",
      "Epoch 5/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.9844 - acc: 0.2429 - val_loss: 4.3588 - val_acc: 0.0377\n",
      "Epoch 6/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.9310 - acc: 0.2570 - val_loss: 4.8975 - val_acc: 0.0162\n",
      "Epoch 7/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.9341 - acc: 0.2638 - val_loss: 4.9709 - val_acc: 0.0377\n",
      "Epoch 8/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.8990 - acc: 0.2667 - val_loss: 4.2877 - val_acc: 0.0243\n",
      "Epoch 9/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.8611 - acc: 0.2760 - val_loss: 5.7761 - val_acc: 0.0189\n",
      "Epoch 10/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.8655 - acc: 0.2810 - val_loss: 4.4059 - val_acc: 0.0270\n",
      "Epoch 11/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.8338 - acc: 0.2906 - val_loss: 4.3696 - val_acc: 0.0135\n",
      "Epoch 12/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.8076 - acc: 0.2896 - val_loss: 4.2721 - val_acc: 0.0458\n",
      "Epoch 13/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7825 - acc: 0.2993 - val_loss: 4.5686 - val_acc: 0.0809\n",
      "Epoch 14/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7636 - acc: 0.3043 - val_loss: 4.5509 - val_acc: 0.0485\n",
      "Epoch 15/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7582 - acc: 0.3048 - val_loss: 4.4616 - val_acc: 0.0296\n",
      "Epoch 16/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7543 - acc: 0.3099 - val_loss: 4.3168 - val_acc: 0.0189\n",
      "Epoch 17/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7250 - acc: 0.3149 - val_loss: 4.3223 - val_acc: 0.0809\n",
      "Epoch 18/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7253 - acc: 0.3152 - val_loss: 4.3300 - val_acc: 0.0162\n",
      "Epoch 19/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.7145 - acc: 0.3169 - val_loss: 4.4974 - val_acc: 0.0162\n",
      "Epoch 20/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.6801 - acc: 0.3260 - val_loss: 4.2079 - val_acc: 0.0809\n",
      "Epoch 21/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.6474 - acc: 0.3312 - val_loss: 4.2010 - val_acc: 0.0162\n",
      "Epoch 22/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.6237 - acc: 0.3376 - val_loss: 4.3635 - val_acc: 0.0162\n",
      "Epoch 23/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.6209 - acc: 0.3360 - val_loss: 4.2357 - val_acc: 0.0809\n",
      "Epoch 24/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.6255 - acc: 0.3345 - val_loss: 4.2705 - val_acc: 0.0485\n",
      "Epoch 25/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5976 - acc: 0.3396 - val_loss: 4.2983 - val_acc: 0.0377\n",
      "Epoch 26/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5811 - acc: 0.3467 - val_loss: 4.0480 - val_acc: 0.0216\n",
      "Epoch 27/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5765 - acc: 0.3464 - val_loss: 5.2407 - val_acc: 0.0485\n",
      "Epoch 28/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5578 - acc: 0.3526 - val_loss: 4.5111 - val_acc: 0.0162\n",
      "Epoch 29/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5488 - acc: 0.3603 - val_loss: 4.1100 - val_acc: 0.0377\n",
      "Epoch 30/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5244 - acc: 0.3602 - val_loss: 4.1437 - val_acc: 0.0485\n",
      "Epoch 31/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5108 - acc: 0.3635 - val_loss: 4.3955 - val_acc: 0.0485\n",
      "Epoch 32/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.5119 - acc: 0.3602 - val_loss: 4.0622 - val_acc: 0.0162\n",
      "Epoch 33/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4786 - acc: 0.3701 - val_loss: 4.4417 - val_acc: 0.0350\n",
      "Epoch 34/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4688 - acc: 0.3740 - val_loss: 4.3906 - val_acc: 0.0296\n",
      "Epoch 35/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4798 - acc: 0.3739 - val_loss: 4.1063 - val_acc: 0.0270\n",
      "Epoch 36/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4476 - acc: 0.3766 - val_loss: 4.1870 - val_acc: 0.0296\n",
      "Epoch 37/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4122 - acc: 0.3839 - val_loss: 4.0945 - val_acc: 0.0809\n",
      "Epoch 38/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.4071 - acc: 0.3795 - val_loss: 4.0644 - val_acc: 0.0135\n",
      "Epoch 39/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3752 - acc: 0.3899 - val_loss: 4.1217 - val_acc: 0.0162\n",
      "Epoch 40/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3698 - acc: 0.3873 - val_loss: 4.3280 - val_acc: 0.0485\n",
      "Epoch 41/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3602 - acc: 0.3964 - val_loss: 4.0769 - val_acc: 0.0135\n",
      "Epoch 42/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3431 - acc: 0.3961 - val_loss: 4.0597 - val_acc: 0.0189\n",
      "Epoch 43/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3301 - acc: 0.4009 - val_loss: 4.2679 - val_acc: 0.0377\n",
      "Epoch 44/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3172 - acc: 0.4014 - val_loss: 3.9616 - val_acc: 0.0809\n",
      "Epoch 45/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.3027 - acc: 0.4051 - val_loss: 3.9503 - val_acc: 0.0296\n",
      "Epoch 46/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2924 - acc: 0.4049 - val_loss: 4.0269 - val_acc: 0.0485\n",
      "Epoch 47/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2774 - acc: 0.4143 - val_loss: 4.0284 - val_acc: 0.0809\n",
      "Epoch 48/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2766 - acc: 0.4120 - val_loss: 3.9685 - val_acc: 0.0377\n",
      "Epoch 49/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2633 - acc: 0.4163 - val_loss: 4.0588 - val_acc: 0.0377\n",
      "Epoch 50/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2564 - acc: 0.4151 - val_loss: 3.9468 - val_acc: 0.0485\n",
      "Epoch 51/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2320 - acc: 0.4161 - val_loss: 4.3513 - val_acc: 0.0485\n",
      "Epoch 52/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2226 - acc: 0.4213 - val_loss: 3.9343 - val_acc: 0.0809\n",
      "Epoch 53/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2144 - acc: 0.4247 - val_loss: 4.0288 - val_acc: 0.0377\n",
      "Epoch 54/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2065 - acc: 0.4290 - val_loss: 3.9159 - val_acc: 0.0809\n",
      "Epoch 55/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.2059 - acc: 0.4300 - val_loss: 3.9460 - val_acc: 0.0296\n",
      "Epoch 56/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1902 - acc: 0.4329 - val_loss: 3.9628 - val_acc: 0.0270\n",
      "Epoch 57/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1698 - acc: 0.4340 - val_loss: 3.9934 - val_acc: 0.0296\n",
      "Epoch 58/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1419 - acc: 0.4387 - val_loss: 4.1298 - val_acc: 0.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1500 - acc: 0.4416 - val_loss: 4.2910 - val_acc: 0.0485\n",
      "Epoch 60/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1470 - acc: 0.4369 - val_loss: 3.9845 - val_acc: 0.0485\n",
      "Epoch 61/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1302 - acc: 0.4452 - val_loss: 4.0324 - val_acc: 0.0485\n",
      "Epoch 62/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1227 - acc: 0.4435 - val_loss: 4.2259 - val_acc: 0.0296\n",
      "Epoch 63/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1267 - acc: 0.4450 - val_loss: 4.0906 - val_acc: 0.0350\n",
      "Epoch 64/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1219 - acc: 0.4492 - val_loss: 4.3099 - val_acc: 0.0485\n",
      "Epoch 65/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1209 - acc: 0.4466 - val_loss: 4.2482 - val_acc: 0.0296\n",
      "Epoch 66/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.1068 - acc: 0.4485 - val_loss: 3.9490 - val_acc: 0.0485\n",
      "Epoch 67/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0994 - acc: 0.4530 - val_loss: 3.9535 - val_acc: 0.0189\n",
      "Epoch 68/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0929 - acc: 0.4565 - val_loss: 4.0926 - val_acc: 0.0270\n",
      "Epoch 69/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0720 - acc: 0.4595 - val_loss: 4.1687 - val_acc: 0.0485\n",
      "Epoch 70/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0666 - acc: 0.4602 - val_loss: 4.3792 - val_acc: 0.0377\n",
      "Epoch 71/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0757 - acc: 0.4556 - val_loss: 3.9246 - val_acc: 0.0809\n",
      "Epoch 72/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0732 - acc: 0.4588 - val_loss: 4.0548 - val_acc: 0.0485\n",
      "Epoch 73/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0675 - acc: 0.4609 - val_loss: 3.9391 - val_acc: 0.0377\n",
      "Epoch 74/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0493 - acc: 0.4643 - val_loss: 3.9626 - val_acc: 0.0809\n",
      "Epoch 75/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0466 - acc: 0.4687 - val_loss: 3.9446 - val_acc: 0.0485\n",
      "Epoch 76/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0376 - acc: 0.4667 - val_loss: 4.3238 - val_acc: 0.0350\n",
      "Epoch 77/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0700 - acc: 0.4596 - val_loss: 3.9163 - val_acc: 0.0485\n",
      "Epoch 78/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0210 - acc: 0.4738 - val_loss: 3.9517 - val_acc: 0.0485\n",
      "Epoch 79/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0419 - acc: 0.4667 - val_loss: 3.9817 - val_acc: 0.0135\n",
      "Epoch 80/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0240 - acc: 0.4684 - val_loss: 4.0286 - val_acc: 0.0485\n",
      "Epoch 81/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0194 - acc: 0.4712 - val_loss: 3.9316 - val_acc: 0.0809\n",
      "Epoch 82/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0184 - acc: 0.4733 - val_loss: 4.6437 - val_acc: 0.0270\n",
      "Epoch 83/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0072 - acc: 0.4767 - val_loss: 3.9798 - val_acc: 0.0216\n",
      "Epoch 84/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0117 - acc: 0.4732 - val_loss: 4.0339 - val_acc: 0.0243\n",
      "Epoch 85/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0042 - acc: 0.4747 - val_loss: 4.3757 - val_acc: 0.0485\n",
      "Epoch 86/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 2.0018 - acc: 0.4738 - val_loss: 4.2009 - val_acc: 0.0809\n",
      "Epoch 87/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9910 - acc: 0.4782 - val_loss: 4.3430 - val_acc: 0.0485\n",
      "Epoch 88/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9901 - acc: 0.4792 - val_loss: 3.9489 - val_acc: 0.0296\n",
      "Epoch 89/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9746 - acc: 0.4798 - val_loss: 3.9029 - val_acc: 0.0809\n",
      "Epoch 90/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9696 - acc: 0.4844 - val_loss: 4.1684 - val_acc: 0.0485\n",
      "Epoch 91/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9750 - acc: 0.4833 - val_loss: 3.9688 - val_acc: 0.0809\n",
      "Epoch 92/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9684 - acc: 0.4864 - val_loss: 4.0747 - val_acc: 0.0485\n",
      "Epoch 93/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9746 - acc: 0.4836 - val_loss: 3.8722 - val_acc: 0.0296\n",
      "Epoch 94/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9617 - acc: 0.4828 - val_loss: 4.1450 - val_acc: 0.0296\n",
      "Epoch 95/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9694 - acc: 0.4859 - val_loss: 4.0442 - val_acc: 0.0485\n",
      "Epoch 96/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9615 - acc: 0.4863 - val_loss: 3.9530 - val_acc: 0.0485\n",
      "Epoch 97/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9644 - acc: 0.4871 - val_loss: 3.8990 - val_acc: 0.0809\n",
      "Epoch 98/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9451 - acc: 0.4935 - val_loss: 3.9149 - val_acc: 0.0809\n",
      "Epoch 99/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9384 - acc: 0.4956 - val_loss: 3.8867 - val_acc: 0.0485\n",
      "Epoch 100/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9292 - acc: 0.4918 - val_loss: 3.8755 - val_acc: 0.0809\n",
      "Epoch 101/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9455 - acc: 0.4894 - val_loss: 3.8592 - val_acc: 0.0809\n",
      "Epoch 102/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9281 - acc: 0.4990 - val_loss: 3.9924 - val_acc: 0.0485\n",
      "Epoch 103/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9190 - acc: 0.4970 - val_loss: 3.9941 - val_acc: 0.0485\n",
      "Epoch 104/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9249 - acc: 0.4975 - val_loss: 4.6771 - val_acc: 0.0243\n",
      "Epoch 105/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9309 - acc: 0.4941 - val_loss: 3.9614 - val_acc: 0.0296\n",
      "Epoch 106/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9105 - acc: 0.5006 - val_loss: 5.0018 - val_acc: 0.0323\n",
      "Epoch 107/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9193 - acc: 0.4985 - val_loss: 4.0328 - val_acc: 0.0296\n",
      "Epoch 108/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.9223 - acc: 0.4996 - val_loss: 3.9827 - val_acc: 0.0377\n",
      "Epoch 109/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8983 - acc: 0.5022 - val_loss: 3.9128 - val_acc: 0.0485\n",
      "Epoch 110/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8999 - acc: 0.5026 - val_loss: 4.4086 - val_acc: 0.0296\n",
      "Epoch 111/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8996 - acc: 0.5043 - val_loss: 4.0998 - val_acc: 0.0377\n",
      "Epoch 112/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8920 - acc: 0.5062 - val_loss: 3.9531 - val_acc: 0.0485\n",
      "Epoch 113/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8863 - acc: 0.5099 - val_loss: 4.1327 - val_acc: 0.0162\n",
      "Epoch 114/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8863 - acc: 0.5048 - val_loss: 3.9199 - val_acc: 0.0809\n",
      "Epoch 115/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8719 - acc: 0.5099 - val_loss: 3.9399 - val_acc: 0.0485\n",
      "Epoch 116/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8813 - acc: 0.5087 - val_loss: 3.9760 - val_acc: 0.0377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8766 - acc: 0.5114 - val_loss: 4.0044 - val_acc: 0.0270\n",
      "Epoch 118/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8702 - acc: 0.5095 - val_loss: 3.9760 - val_acc: 0.0485\n",
      "Epoch 119/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8875 - acc: 0.5092 - val_loss: 4.1208 - val_acc: 0.0162\n",
      "Epoch 120/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8754 - acc: 0.5111 - val_loss: 3.9506 - val_acc: 0.0485\n",
      "Epoch 121/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8740 - acc: 0.5098 - val_loss: 3.9304 - val_acc: 0.0485\n",
      "Epoch 122/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8524 - acc: 0.5150 - val_loss: 4.0556 - val_acc: 0.0485\n",
      "Epoch 123/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8592 - acc: 0.5117 - val_loss: 3.9874 - val_acc: 0.0485\n",
      "Epoch 124/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8724 - acc: 0.5081 - val_loss: 3.9492 - val_acc: 0.0216\n",
      "Epoch 125/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8510 - acc: 0.5172 - val_loss: 3.9955 - val_acc: 0.0270\n",
      "Epoch 126/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8674 - acc: 0.5120 - val_loss: 4.1193 - val_acc: 0.0377\n",
      "Epoch 127/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8549 - acc: 0.5128 - val_loss: 3.8467 - val_acc: 0.0485\n",
      "Epoch 128/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8506 - acc: 0.5162 - val_loss: 3.9323 - val_acc: 0.0162\n",
      "Epoch 129/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8427 - acc: 0.5146 - val_loss: 4.2110 - val_acc: 0.0809\n",
      "Epoch 130/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8591 - acc: 0.5157 - val_loss: 3.8353 - val_acc: 0.0809\n",
      "Epoch 131/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8665 - acc: 0.5156 - val_loss: 3.8741 - val_acc: 0.0377\n",
      "Epoch 132/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8707 - acc: 0.5116 - val_loss: 3.8361 - val_acc: 0.0809\n",
      "Epoch 133/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8384 - acc: 0.5233 - val_loss: 4.2368 - val_acc: 0.0377\n",
      "Epoch 134/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.8426 - acc: 0.5215 - val_loss: 4.0218 - val_acc: 0.0162\n",
      "Epoch 135/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8364 - acc: 0.5204 - val_loss: 3.9381 - val_acc: 0.0296\n",
      "Epoch 136/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8234 - acc: 0.5251 - val_loss: 4.1593 - val_acc: 0.0377\n",
      "Epoch 137/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8293 - acc: 0.5210 - val_loss: 3.8338 - val_acc: 0.0458\n",
      "Epoch 138/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8274 - acc: 0.5223 - val_loss: 3.9123 - val_acc: 0.0485\n",
      "Epoch 139/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8181 - acc: 0.5261 - val_loss: 3.8637 - val_acc: 0.0485\n",
      "Epoch 140/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8123 - acc: 0.5237 - val_loss: 4.0116 - val_acc: 0.0485\n",
      "Epoch 141/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8084 - acc: 0.5273 - val_loss: 4.5463 - val_acc: 0.0485\n",
      "Epoch 142/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8042 - acc: 0.5274 - val_loss: 3.8691 - val_acc: 0.0485\n",
      "Epoch 143/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8034 - acc: 0.5304 - val_loss: 3.8562 - val_acc: 0.0485\n",
      "Epoch 144/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8002 - acc: 0.5292 - val_loss: 4.0069 - val_acc: 0.0108\n",
      "Epoch 145/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8017 - acc: 0.5292 - val_loss: 3.9364 - val_acc: 0.0809\n",
      "Epoch 146/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8113 - acc: 0.5283 - val_loss: 3.9111 - val_acc: 0.0377\n",
      "Epoch 147/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8097 - acc: 0.5263 - val_loss: 3.9218 - val_acc: 0.0485\n",
      "Epoch 148/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8074 - acc: 0.5318 - val_loss: 3.8689 - val_acc: 0.0485\n",
      "Epoch 149/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8057 - acc: 0.5315 - val_loss: 3.9515 - val_acc: 0.0485\n",
      "Epoch 150/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8032 - acc: 0.5293 - val_loss: 3.9329 - val_acc: 0.0485\n",
      "Epoch 151/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.7947 - acc: 0.5339 - val_loss: 3.9279 - val_acc: 0.0485\n",
      "Epoch 152/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8147 - acc: 0.5318 - val_loss: 4.2041 - val_acc: 0.0809\n",
      "Epoch 153/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8101 - acc: 0.5329 - val_loss: 4.0031 - val_acc: 0.0270\n",
      "Epoch 154/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8012 - acc: 0.5312 - val_loss: 3.9395 - val_acc: 0.0162\n",
      "Epoch 155/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.8169 - acc: 0.5312 - val_loss: 4.2539 - val_acc: 0.0485\n",
      "Epoch 156/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.7866 - acc: 0.5352 - val_loss: 3.9108 - val_acc: 0.0350\n",
      "Epoch 157/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7987 - acc: 0.5361 - val_loss: 3.9713 - val_acc: 0.0296\n",
      "Epoch 158/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7794 - acc: 0.5365 - val_loss: 3.8882 - val_acc: 0.0809\n",
      "Epoch 159/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7960 - acc: 0.5329 - val_loss: 3.8810 - val_acc: 0.0485\n",
      "Epoch 160/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7842 - acc: 0.5363 - val_loss: 4.0554 - val_acc: 0.0485\n",
      "Epoch 161/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7714 - acc: 0.5340 - val_loss: 4.4404 - val_acc: 0.0162\n",
      "Epoch 162/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7969 - acc: 0.5317 - val_loss: 3.9691 - val_acc: 0.0485\n",
      "Epoch 163/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7693 - acc: 0.5402 - val_loss: 3.8331 - val_acc: 0.0809\n",
      "Epoch 164/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7614 - acc: 0.5413 - val_loss: 3.8767 - val_acc: 0.0377\n",
      "Epoch 165/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7711 - acc: 0.5406 - val_loss: 3.9312 - val_acc: 0.0485\n",
      "Epoch 166/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7683 - acc: 0.5372 - val_loss: 3.8655 - val_acc: 0.0809\n",
      "Epoch 167/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7588 - acc: 0.5423 - val_loss: 4.3491 - val_acc: 0.0243\n",
      "Epoch 168/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7818 - acc: 0.5406 - val_loss: 3.8957 - val_acc: 0.0485\n",
      "Epoch 169/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7860 - acc: 0.5361 - val_loss: 3.9326 - val_acc: 0.0809\n",
      "Epoch 170/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7643 - acc: 0.5432 - val_loss: 3.8737 - val_acc: 0.0809\n",
      "Epoch 171/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7840 - acc: 0.5377 - val_loss: 3.9268 - val_acc: 0.0135\n",
      "Epoch 172/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7681 - acc: 0.5420 - val_loss: 4.3153 - val_acc: 0.0377\n",
      "Epoch 173/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7572 - acc: 0.5440 - val_loss: 3.8549 - val_acc: 0.0809\n",
      "Epoch 174/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7518 - acc: 0.5425 - val_loss: 3.8243 - val_acc: 0.0809\n",
      "Epoch 175/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7443 - acc: 0.5457 - val_loss: 3.8999 - val_acc: 0.0485\n",
      "Epoch 176/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7390 - acc: 0.5482 - val_loss: 3.8398 - val_acc: 0.0809\n",
      "Epoch 177/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7404 - acc: 0.5485 - val_loss: 3.8481 - val_acc: 0.0485\n",
      "Epoch 178/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7521 - acc: 0.5401 - val_loss: 3.8503 - val_acc: 0.0809\n",
      "Epoch 179/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7358 - acc: 0.5464 - val_loss: 4.1593 - val_acc: 0.0081\n",
      "Epoch 180/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7563 - acc: 0.5465 - val_loss: 4.2803 - val_acc: 0.0809\n",
      "Epoch 181/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7623 - acc: 0.5417 - val_loss: 3.8735 - val_acc: 0.0809\n",
      "Epoch 182/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7288 - acc: 0.5503 - val_loss: 3.8823 - val_acc: 0.0296\n",
      "Epoch 183/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7418 - acc: 0.5450 - val_loss: 3.8696 - val_acc: 0.0809\n",
      "Epoch 184/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7376 - acc: 0.5498 - val_loss: 4.0969 - val_acc: 0.0809\n",
      "Epoch 185/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7384 - acc: 0.5509 - val_loss: 3.8874 - val_acc: 0.0809\n",
      "Epoch 186/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7403 - acc: 0.5462 - val_loss: 3.8857 - val_acc: 0.0485\n",
      "Epoch 187/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7383 - acc: 0.5473 - val_loss: 4.0252 - val_acc: 0.0485\n",
      "Epoch 188/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7338 - acc: 0.5500 - val_loss: 3.8614 - val_acc: 0.0809\n",
      "Epoch 189/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7570 - acc: 0.5462 - val_loss: 3.8817 - val_acc: 0.0809\n",
      "Epoch 190/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7307 - acc: 0.5490 - val_loss: 3.9308 - val_acc: 0.0809\n",
      "Epoch 191/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7402 - acc: 0.5472 - val_loss: 3.9421 - val_acc: 0.0485\n",
      "Epoch 192/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7460 - acc: 0.5453 - val_loss: 4.0909 - val_acc: 0.0162\n",
      "Epoch 193/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7254 - acc: 0.5530 - val_loss: 3.8297 - val_acc: 0.0485\n",
      "Epoch 194/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7103 - acc: 0.5554 - val_loss: 3.8568 - val_acc: 0.0809\n",
      "Epoch 195/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7330 - acc: 0.5502 - val_loss: 3.8388 - val_acc: 0.0809\n",
      "Epoch 196/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7271 - acc: 0.5498 - val_loss: 4.0423 - val_acc: 0.0809\n",
      "Epoch 197/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7298 - acc: 0.5552 - val_loss: 3.9007 - val_acc: 0.0485\n",
      "Epoch 198/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7411 - acc: 0.5497 - val_loss: 3.8780 - val_acc: 0.0485\n",
      "Epoch 199/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7279 - acc: 0.5538 - val_loss: 3.8476 - val_acc: 0.0485\n",
      "Epoch 200/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7097 - acc: 0.5572 - val_loss: 3.9328 - val_acc: 0.0458\n",
      "Epoch 201/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7145 - acc: 0.5562 - val_loss: 3.8415 - val_acc: 0.0377\n",
      "Epoch 202/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7208 - acc: 0.5521 - val_loss: 3.8331 - val_acc: 0.0485\n",
      "Epoch 203/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7231 - acc: 0.5519 - val_loss: 3.8655 - val_acc: 0.0485\n",
      "Epoch 204/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7312 - acc: 0.5520 - val_loss: 3.9918 - val_acc: 0.0485\n",
      "Epoch 205/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7347 - acc: 0.5535 - val_loss: 3.9084 - val_acc: 0.0485\n",
      "Epoch 206/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7330 - acc: 0.5525 - val_loss: 3.9658 - val_acc: 0.0809\n",
      "Epoch 207/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7155 - acc: 0.5543 - val_loss: 3.8927 - val_acc: 0.0809\n",
      "Epoch 208/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7167 - acc: 0.5549 - val_loss: 4.0249 - val_acc: 0.0485\n",
      "Epoch 209/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7088 - acc: 0.5577 - val_loss: 3.9838 - val_acc: 0.0162\n",
      "Epoch 210/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7227 - acc: 0.5566 - val_loss: 3.9052 - val_acc: 0.0809\n",
      "Epoch 211/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6966 - acc: 0.5611 - val_loss: 3.8626 - val_acc: 0.0485\n",
      "Epoch 212/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7005 - acc: 0.5609 - val_loss: 4.1928 - val_acc: 0.0809\n",
      "Epoch 213/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6987 - acc: 0.5598 - val_loss: 4.0086 - val_acc: 0.0809\n",
      "Epoch 214/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7161 - acc: 0.5563 - val_loss: 3.8640 - val_acc: 0.0485\n",
      "Epoch 215/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6987 - acc: 0.5630 - val_loss: 3.8620 - val_acc: 0.0485\n",
      "Epoch 216/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6809 - acc: 0.5671 - val_loss: 4.5332 - val_acc: 0.0485\n",
      "Epoch 217/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7224 - acc: 0.5544 - val_loss: 3.8704 - val_acc: 0.0485\n",
      "Epoch 218/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7106 - acc: 0.5551 - val_loss: 3.8297 - val_acc: 0.0485\n",
      "Epoch 219/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7011 - acc: 0.5602 - val_loss: 3.8267 - val_acc: 0.0809\n",
      "Epoch 220/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7214 - acc: 0.5562 - val_loss: 3.8734 - val_acc: 0.0485\n",
      "Epoch 221/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7112 - acc: 0.5588 - val_loss: 3.8645 - val_acc: 0.0809\n",
      "Epoch 222/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6964 - acc: 0.5623 - val_loss: 3.9110 - val_acc: 0.0809\n",
      "Epoch 223/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7241 - acc: 0.5537 - val_loss: 4.1999 - val_acc: 0.0809\n",
      "Epoch 224/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7013 - acc: 0.5602 - val_loss: 3.8469 - val_acc: 0.0809\n",
      "Epoch 225/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6934 - acc: 0.5624 - val_loss: 3.9597 - val_acc: 0.0809\n",
      "Epoch 226/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6752 - acc: 0.5667 - val_loss: 3.8874 - val_acc: 0.0485\n",
      "Epoch 227/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6916 - acc: 0.5623 - val_loss: 3.8017 - val_acc: 0.0485\n",
      "Epoch 228/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7242 - acc: 0.5569 - val_loss: 4.0098 - val_acc: 0.0809\n",
      "Epoch 229/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6964 - acc: 0.5644 - val_loss: 3.8803 - val_acc: 0.0485\n",
      "Epoch 230/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6941 - acc: 0.5607 - val_loss: 4.0713 - val_acc: 0.0296\n",
      "Epoch 231/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6983 - acc: 0.5626 - val_loss: 3.9200 - val_acc: 0.0809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7099 - acc: 0.5596 - val_loss: 3.8367 - val_acc: 0.0485\n",
      "Epoch 233/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6991 - acc: 0.5597 - val_loss: 3.8246 - val_acc: 0.0270\n",
      "Epoch 234/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6915 - acc: 0.5636 - val_loss: 3.8238 - val_acc: 0.0485\n",
      "Epoch 235/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6857 - acc: 0.5675 - val_loss: 3.9840 - val_acc: 0.0485\n",
      "Epoch 236/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6932 - acc: 0.5645 - val_loss: 4.0187 - val_acc: 0.0189\n",
      "Epoch 237/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6956 - acc: 0.5620 - val_loss: 3.8635 - val_acc: 0.0485\n",
      "Epoch 238/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7000 - acc: 0.5637 - val_loss: 3.8808 - val_acc: 0.0809\n",
      "Epoch 239/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6873 - acc: 0.5678 - val_loss: 4.4612 - val_acc: 0.0377\n",
      "Epoch 240/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6853 - acc: 0.5623 - val_loss: 4.1393 - val_acc: 0.0809\n",
      "Epoch 241/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6587 - acc: 0.5752 - val_loss: 3.9440 - val_acc: 0.0485\n",
      "Epoch 242/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6701 - acc: 0.5709 - val_loss: 3.9659 - val_acc: 0.0270\n",
      "Epoch 243/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6865 - acc: 0.5663 - val_loss: 3.8651 - val_acc: 0.0809\n",
      "Epoch 244/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6904 - acc: 0.5659 - val_loss: 3.8646 - val_acc: 0.0809\n",
      "Epoch 245/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6773 - acc: 0.5669 - val_loss: 3.9129 - val_acc: 0.0809\n",
      "Epoch 246/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6810 - acc: 0.5669 - val_loss: 4.0256 - val_acc: 0.0809\n",
      "Epoch 247/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6815 - acc: 0.5680 - val_loss: 4.0823 - val_acc: 0.0485\n",
      "Epoch 248/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6815 - acc: 0.5684 - val_loss: 3.9955 - val_acc: 0.0809\n",
      "Epoch 249/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6799 - acc: 0.5660 - val_loss: 3.8994 - val_acc: 0.0243\n",
      "Epoch 250/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6769 - acc: 0.5670 - val_loss: 4.0797 - val_acc: 0.0485\n",
      "Epoch 251/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6768 - acc: 0.5700 - val_loss: 4.2537 - val_acc: 0.0296\n",
      "Epoch 252/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6925 - acc: 0.5635 - val_loss: 3.9103 - val_acc: 0.0377\n",
      "Epoch 253/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.7194 - acc: 0.5603 - val_loss: 3.9108 - val_acc: 0.0296\n",
      "Epoch 254/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6757 - acc: 0.5710 - val_loss: 3.9501 - val_acc: 0.0485\n",
      "Epoch 255/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6836 - acc: 0.5678 - val_loss: 4.0313 - val_acc: 0.0458\n",
      "Epoch 256/10000\n",
      "18502/18502 [==============================] - 34s 2ms/step - loss: 1.6692 - acc: 0.5711 - val_loss: 3.8286 - val_acc: 0.0485\n",
      "Epoch 257/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6747 - acc: 0.5702 - val_loss: 4.1016 - val_acc: 0.0458\n",
      "Epoch 258/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6610 - acc: 0.5737 - val_loss: 3.9582 - val_acc: 0.0485\n",
      "Epoch 259/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6606 - acc: 0.5729 - val_loss: 3.9088 - val_acc: 0.0809\n",
      "Epoch 260/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6606 - acc: 0.5733 - val_loss: 3.9005 - val_acc: 0.0485\n",
      "Epoch 261/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6696 - acc: 0.5699 - val_loss: 3.9289 - val_acc: 0.0270\n",
      "Epoch 262/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6631 - acc: 0.5731 - val_loss: 3.8652 - val_acc: 0.0809\n",
      "Epoch 263/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6719 - acc: 0.5715 - val_loss: 3.8436 - val_acc: 0.0809\n",
      "Epoch 264/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6806 - acc: 0.5715 - val_loss: 3.8607 - val_acc: 0.0485\n",
      "Epoch 265/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6684 - acc: 0.5700 - val_loss: 3.9246 - val_acc: 0.0809\n",
      "Epoch 266/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6730 - acc: 0.5687 - val_loss: 3.8472 - val_acc: 0.0296\n",
      "Epoch 267/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6569 - acc: 0.5743 - val_loss: 3.8347 - val_acc: 0.0809\n",
      "Epoch 268/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6490 - acc: 0.5777 - val_loss: 3.8583 - val_acc: 0.0809\n",
      "Epoch 269/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6549 - acc: 0.5753 - val_loss: 4.0489 - val_acc: 0.0162\n",
      "Epoch 270/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6625 - acc: 0.5745 - val_loss: 3.9620 - val_acc: 0.0485\n",
      "Epoch 271/10000\n",
      "18502/18502 [==============================] - 36s 2ms/step - loss: 1.6511 - acc: 0.5774 - val_loss: 4.1699 - val_acc: 0.0081\n",
      "Epoch 272/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6667 - acc: 0.5722 - val_loss: 3.8495 - val_acc: 0.0809\n",
      "Epoch 273/10000\n",
      "18502/18502 [==============================] - 35s 2ms/step - loss: 1.6701 - acc: 0.5728 - val_loss: 3.8940 - val_acc: 0.0296\n",
      "Epoch 274/10000\n",
      " 8064/18502 [============>.................] - ETA: 19s - loss: 1.6690 - acc: 0.5745"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[12160,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/Adam/gradients/dense_5/weight_regularizer/Square_grad/Mul_1 = Mul[T=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/AddN_16\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_5/kernel/read, training_2/Adam/gradients/dense_5/weight_regularizer/Square_grad/Mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_3/add_3/_1125 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4152_loss_3/add_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8dce4acd59aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#callbacks=[reduce_lr])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12160,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_2/Adam/gradients/dense_5/weight_regularizer/Square_grad/Mul_1 = Mul[T=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/AddN_16\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dense_5/kernel/read, training_2/Adam/gradients/dense_5/weight_regularizer/Square_grad/Mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_3/add_3/_1125 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4152_loss_3/add_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.5, patience=10, min_lr=1e-4)\n",
    "# ton = TerminateOnNaN()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs=10000,validation_data=(X_valid,Y_valid),batch_size=64)#callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_epochs):\n",
    "    x, y = generate_data()\n",
    "    history = model.fit(x, y, epochs=1, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
