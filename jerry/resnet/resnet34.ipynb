{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from keras.utils import to_categorical ,Sequence\n",
    "from keras import losses, models, optimizers\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten,\n",
    "                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n",
    "from keras import backend as K                      \n",
    "\n",
    "# calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_dict = pk.load(open('data/map.pkl' , 'rb'))\n",
    "reverse_dict = pk.load(open('data/map_reverse.pkl' , 'rb'))\n",
    "\n",
    "name = pd.read_csv('data/sample_submission.csv')\n",
    "name = name['fname'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'model_full_resnet34'\n",
    "predict_path = 'model_full_resnet34_csv'\n",
    "\n",
    "unverified_path = 'jerry_resnet34_mfcc3_unverified_{}.csv'\n",
    "test_path = 'jerry_resnet34_mfcc3_test_{}.csv'\n",
    "\n",
    "cotrain_path = join((predict_path),'jerry_mfcc3_resnet34_mixup_cotrain_Y.csv')\n",
    "\n",
    "weight = [0.86253 ,0.85445 ,0.84636 ,0.87601 ,0.8814 ,0.86523 ,0.88949 ,0.85714 ,0.87871 ,0.85714]\n",
    "\n",
    "if not os.path.exists(predict_path):\n",
    "    os.mkdir(predict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all validation  for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "for i in range(1,11):\n",
    "    print('fold {}'.format(i))\n",
    "    print('#'*50)\n",
    "    \n",
    "    model = load_model(join(model_path,'best_{}.h5'.format(i)))\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for j in range(1,11):\n",
    "        val_name = np.load(\"data/ten_fold_data/valid_fname_{}.npy\".format(j))\n",
    "        val_predict = model.predict(np.load('data/ten_fold_data/X_valid_{}.npy'.format(j)) ,  verbose=1)\n",
    "        df = pd.DataFrame(val_predict)\n",
    "        df.insert(0,'fname' , val_name)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df = pd.concat(df_list)\n",
    "    df.to_csv(join(predict_path, 'fold_{}.csv'.format(i)),index=False)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Unverified data && Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "audio_martix = np.load('data/train/mfcc3/X_train.npy')\n",
    "\n",
    "df = pd.read_csv('data/train_label.csv')  \n",
    "df_unmanu = df[df['manually_verified'] == 0]\n",
    "df_unmanu['trans'] = df_unmanu['label'].map(map_dict)\n",
    "df_unmanu['onehot'] = df_unmanu['trans'].apply(lambda x: to_categorical(x,num_classes=41))\n",
    "unmanu_veri_idx = df_unmanu.index.values\n",
    "un_fnames = df_unmanu['fname'].values\n",
    "\n",
    "X = audio_martix[unmanu_veri_idx]\n",
    "Y = df_unmanu['onehot'].tolist()\n",
    "Y = np.array(Y)\n",
    "Y = Y.reshape(-1 ,41)\n",
    "\n",
    "test_X = np.load(\"data/test/mfcc3/X_test.npy\")\n",
    "test_fnames = pd.read_csv('data/sample_submission.csv')['fname'].values\n",
    "\n",
    "\n",
    "# predict unverified data and testing data\n",
    "for i in range(1,11):\n",
    "    model = load_model(join(model_path,'best_{}.h5'.format(i)))\n",
    "    \n",
    "    unverified_predict = model.predict(X ,  verbose=1 , batch_size = 32)\n",
    "    unver_df = pd.DataFrame(unverified_predict)\n",
    "    unver_df.insert(0,'fname' , un_fnames)\n",
    "    unver_df.to_csv(join(predict_path, unverified_path.format(i)),index=False) \n",
    "    \n",
    "    test_predict = model.predict(test_X ,  verbose=1 , batch_size = 32)\n",
    "    test_df = pd.DataFrame(test_predict)\n",
    "    test_df.insert(0,'fname' , test_fnames)\n",
    "    test_df.to_csv(join(predict_path, test_path.format(i)),index=False) \n",
    "\n",
    "    #concat_df = pd.concat([unver_df,test_df])\n",
    "    #concat_df.to_csv(join(predict_path, 'concat_{}.csv'.format(i)),index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble 10 Fold  with weignt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = 0.0\n",
    "fname = []\n",
    "\n",
    "for i in range(1,11): \n",
    "    \n",
    "    test_df = pd.read_csv(join(predict_path, test_path.format(i)))\n",
    "    uverified_df = pd.read_csv(join(predict_path,unverified_path.format(i)))\n",
    "    \n",
    "    concat_df = pd.concat([uverified_df,test_df])\n",
    "    fname = concat_df['fname'].values\n",
    "    \n",
    "    col_name = [str(x)for x in list(range(0,41))]\n",
    "    predict = concat_df[col_name].values\n",
    "    \n",
    "    result += predict*weight[i-1] / sum(weight)\n",
    "\n",
    "print('weight ensemble calculate done')\n",
    "max_confidence = np.amax(result, axis=1) \n",
    "\n",
    "mean_confidence = np.mean(max_confidence)\n",
    "std_confidence = np.std(max_confidence)\n",
    "\n",
    "threshold = mean_confidence + std_confidence\n",
    "\n",
    "semi_idx = []\n",
    "for idx,v in enumerate(result):\n",
    "    if np.max(v) > threshold:\n",
    "        semi_idx.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data chich probabilistic > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_result = result[semi_idx]\n",
    "semi_result = [np.argmax(x) for x in semi_result]\n",
    "semi_fname = fname[semi_idx]\n",
    "\n",
    "# print(semi_result.shape)\n",
    "# print(semi_fname.shape)\n",
    "\n",
    "cotrain_df = pd.DataFrame({'fname':semi_fname , 'label_verified':semi_result})\n",
    "cotrain_df.to_csv(cotrain_path,index=False) \n",
    "\n",
    "semi_result[0:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
