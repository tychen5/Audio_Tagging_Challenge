{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical ,Sequence\n",
    "from keras import losses, models, optimizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
    "from keras.layers import Activation, LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from random_eraser import get_random_eraser\n",
    "from keras.optimizers import Adam\n",
    "from os.path import join\n",
    "import resnet\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "map_dict = pk.load(open('data/map.pkl' , 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semi = pd.read_csv('data/cotrain/Y_selftrain_ens_verified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semi_map = {}\n",
    "\n",
    "semi_name = semi['fname'].values\n",
    "semi_label_verified = semi['label_verified'].values\n",
    "\n",
    "for idx ,d in enumerate( semi_name):\n",
    "    semi_map[d] = semi_label_verified[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unverified_df = pd.read_csv('data/train_label.csv')\n",
    "test_df = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "unverified_df = unverified_df[unverified_df['fname'].isin(semi_name)]\n",
    "unverified_df = unverified_df.drop(columns=['manually_verified'])\n",
    "unverified_df['label_verified'] = unverified_df['fname'].map(semi_map)\n",
    "\n",
    "test_df = test_df[test_df['fname'].isin(semi_name)]\n",
    "test_df['label_verified'] = test_df['fname'].map(semi_map)\n",
    "\n",
    "unverified_idx = unverified_df.index.values\n",
    "test_idx = test_df.index.values\n",
    "\n",
    "df = pd.concat([unverified_df , test_df])\n",
    "df = df.drop(columns=['label'])\n",
    "df['trans'] = df['label_verified'].map(map_dict)\n",
    "df['onehot'] = df['trans'].apply(lambda x: to_categorical(x,num_classes=41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unverified = np.load('data/train/mfcc3/X_train.npy')[unverified_idx]\n",
    "X_test = np.load('data/test/mfcc3/X_test.npy')[test_idx]\n",
    "\n",
    "X_semi = np.append(X_unverified,X_test , axis=0)\n",
    "Y_semi = np.array(df['onehot'].tolist()).reshape(-1,41)\n",
    "\n",
    "print(X_semi.shape)\n",
    "print(Y_semi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # data generator ====================================================================================\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Semi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = 'model_full_resnet152'\n",
    "refine_path = 'model_full_resnet152_refine_co'\n",
    "\n",
    "all_x = np.concatenate( (np.load('data/train/mfcc3/X_train.npy') , np.load('data/test/mfcc3/X_test.npy')))\n",
    "\n",
    "if not os.path.exists(refine_path):\n",
    "    os.mkdir(refine_path)\n",
    "\n",
    "for i in range(1,11):\n",
    "    X_train = np.load('data/ten_fold_data/X_train_{}.npy'.format(i)) \n",
    "    Y_train = np.load('data/ten_fold_data/Y_train_{}.npy'.format(i)) \n",
    "    X_test = np.load('data/ten_fold_data/X_valid_{}.npy'.format(i))\n",
    "    Y_test = np.load('data/ten_fold_data/Y_valid_{}.npy'.format(i))\n",
    "    \n",
    "    X_train = np.append(X_train,X_semi , axis=0)\n",
    "    Y_train = np.append(Y_train,Y_semi , axis=0)\n",
    "    \n",
    "    X_train , Y_train = shuffle(X_train, Y_train, random_state=5)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    model = load_model(join(model_path,'best_{}.h5'.format(i)))\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(join(refine_path , 'semi_self_%d_{val_acc:.5f}.h5'%i), monitor='val_acc', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=30)\n",
    "    callbacks_list = [checkpoint, early]\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=get_random_eraser(v_l=np.min(all_x), v_h=np.max(all_x)) # Trainset's boundaries.\n",
    "    )\n",
    "    \n",
    "    mygenerator = MixupGenerator(X_train, Y_train, alpha=1.0, batch_size=128, datagen=datagen)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(lr=0.0001),\n",
    "             metrics=['accuracy'])\n",
    "    # mixup\n",
    "    history = model.fit_generator(mygenerator(),\n",
    "                    steps_per_epoch= X_train.shape[0] // 128,\n",
    "                    epochs=10000,\n",
    "                    validation_data=(X_test,Y_test), callbacks=callbacks_list)\n",
    "    # normalize\n",
    "#     history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), callbacks=callbacks_list,\n",
    "#                         batch_size=32, epochs=10000)\n",
    "\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
